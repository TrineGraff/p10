\chapter{Asymptotiske egenskaber} \label{ch:asymptotics}
\textit{I dette kapitel vil vi betragte nogle asymptotiske egenskaber af lasso estimatoren og introducere nogle såkaldte orakelegenskaber.
Kapitlet er baseret på \cite{adaptive_lasso} og \cite{adaptive_lasso_knight}}. \\[4mm]
%
\input{main/ch/sub/orakelegenskaber}

Vi betragter en lineær regressionsmodel 
\begin{align*}
y_i= \mathbf{x}_i \tbeta^* + \epsilon_i, \quad i = 1, \ldots, n,
\end{align*}
hvor \(\epsilon_i \sim \text{iid} \del{0, \sigma^2}\), og vi antager, at \(\frac{1}{n} \X^T \X \rightarrow \textbf{C}\), hvor
\begin{align*}
\textbf{C} = 
\begin{bmatrix}
\textbf{C}_{11}& \textbf{C}_{12}\\
\textbf{C}_{21}& \textbf{C}_{22}
\end{bmatrix},
\end{align*}
er en positiv definit matrix, og $\textbf{C}_{11}$ er en $p_0 \times p_0$ matrix. 

Under denne antagelse er den asymptotiske fordeling af OLS estimatoren givet ved
\begin{align*}
\sqrt{n}(\widehat{\boldsymbol{\beta}}^{\text{OLS}}-\boldsymbol{\beta}^*) \overset{d}{\rightarrow} N \del{\mathbf{0},\sigma^2 \mathbf{C}^{-1}}.
\end{align*}
Heraf ses det, at OLS estimatoren er rod-\(n\)-konsistent.

\section{Lasso}
Lad os betragte lasso estimatoren
\begin{align*}
\widehat{\tbeta}^\text{lasso} = \argmin_{\tbeta \in \R^p} \cbr{ \Vert \y - \sum_{j=1}^p \x_j \beta_j \Vert_2^2 + \lambda_n \sum_{j=1}^p \abs{ \beta_j }},
\end{align*}
hvor \(\lambda_n\) varierer med \(n\).
Lad \(\A_n^\text{lasso} = \cbr{j : \widehat{\beta}_j^\text{lasso} \neq 0}\) betegne den aktive mængde for lasso estimatoren.

\begin{lem}\label{lem:lasso_consistency1}
Hvis $\frac{\lambda_n}{n} \rightarrow \lambda_0 \geq 0$, da vil
\begin{align*}
\widehat{\tbeta}^\text{lasso} \overset{p}{\rightarrow} \argmin_{\tbeta \in \R^p} \cbr{ \del{\mathbf{u} - \tbeta^*}^T \mathbf{C} \del{\mathbf{u} - \tbeta^*} + \lambda_0 \sum_{j=1}^p \abs{ u_j }}.
\end{align*}
\end{lem}
\begin{proof}
Beviset undlades, men der henvises til s. 1358 i \citep{adaptive_lasso_knight}.
\end{proof}
Hvis $\lambda_0=0$, da gælder ifølge lemma \ref{lem:lasso_consistency1}, at $\widehat{\boldsymbol{\beta}}^\text{lasso} \overset{p}{\rightarrow} \boldsymbol{\beta}^{*}$, da strafleddet forsvinder og $\mathbf{C}$ er en positiv definit matrix, og dermed er $\widehat{\boldsymbol{\beta}}^\text{lasso}$ svagt konsistent. 

Selvom \(\lambda_n = o \del{n}\) er tilstrækkeligt til konsistens, skal \(\lambda_n\) vokse langsommere for rod-\(n\)-konsistens.
Men hvis \(\lambda_n\) vokser for langsomt, da vil den asymptotiske fordeling af \(\sqrt{n} \del{\widehat{\tbeta}^\text{lasso} - \tbeta^*} \) være den samme som den for \(\sqrt{n} \del{\widehat{\tbeta}^\text{OLS} - \tbeta^*} \).
Sætning \ref{thm:asymp_lasso} indikerer, at \(\lambda_n = O \del{\sqrt{n}}\) kræves for rod-\(n\)-konsistens af lasso estimatoren.
%
\begin{thm}[Asymptotisk fordeling af lasso estimatoren] \label{thm:asymp_lasso}
Hvis $\frac{\lambda_n}{\sqrt{n}} \rightarrow \lambda_0 \geq 0$, da er den asymptotiske fordeling af \(\widehat{\tbeta}^\text{lasso}\) givet ved
\begin{align*}
\sqrt{n} \del{\widehat{\tbeta}^\text{lasso} - \tbeta^*} \overset{d}{\rightarrow} \argmin_{\tbeta \in \R^p} \cbr{ -2 \mathbf{u}^T \mathbf{W} + \mathbf{u}^T \mathbf{C} \mathbf{u} + \lambda_0 \sum_{j=1}^p \sbr{u_j \text{sign} \del{\beta^*_j} \mathbb{1} \del{\beta_j^* \neq 0} + \abs{u_j} \mathbb{1} \del{\beta_j^* = 0}}},
\end{align*}
hvor \(\mathbf{W} \sim N\del{\mathbf{0}, \sigma^2 \mathbf{C}}\).
\end{thm}
\begin{proof}
Beviset undlades, men der henvises til s. 1359 i \citep{adaptive_lasso_knight}.
\end{proof}
%
\begin{prop} \label{prop:asymp_lasso}
Hvis \(\frac{\lambda_n}{\sqrt{n}} \rightarrow \lambda_0 \geq 0\), da er \(\lim \sup_n \mathbb{P} \del{\A_n^\text{lasso} = \A} \leq c <1\), hvor \(c\) er en positiv konstant, der afhænger af den sande model.
\end{prop}
\begin{proof}
Beviset undlades, men vi refererer til s. 1425-1426 i \citep{adaptive_lasso}.
\end{proof}
Proposition \ref{prop:asymp_lasso} medfører, at når \(\lambda_n = O \del{\sqrt{n}}\), da kan \(\A_n^\text{lasso}\) ikke være \(\A\) med sandsynlighed 1. 
\cite{adaptive_lasso} beviste en nødvendig betingelse for at variabeludvælgelse for lasso estimatoren er konsistent. 
%
\begin{thm}[Nødvendig betingelse for at variabeludvælgelse for lasso estimatoren er konsistent]
Antag at \(\lim_{n \rightarrow \infty} \mathbb{P} \del{\A_n^\text{lasso} = \A}=1\), da eksisterer en fortegnsvektor \(\mathbf{s} = \del{s_1, \ldots, s_{p_0}}\), hvor \(s_j\) er lig \(1\) eller \(-1\), således at
\begin{align}
\left\vert \mathbf{C}_{21} \mathbf{C}_{11}^{-1} \mathbf{s} \right\vert \leq 1. \label{eq:betingelse_konsistent}
\end{align}
\end{thm}
%
\begin{proof}
Beviset undlades, men vi referer til s. 1426 i \citep{adaptive_lasso}.
\end{proof}
%
Hvis betingelse \eqref{eq:betingelse_konsistent} ikke er opfyldt, da er lasso variabeludvælgelsen ikke konsistent.
%Den nødvendige betingelse i \eqref{eq:betingelse_konsistent} er ikke triviel.
%
%\begin{cor}
%Antag \(p_0 = 2m+1 \geq 3\) og \(p=p_0+1\), således at én prædiktor er irrelevant.
%Lad \(\mathbf{C}_{11} = \del{1- \rho_1} \mathbf{I} + \rho_1 \mathbf{J}_1\), hvor \(\mathbf{J}_1\) er en matrix bestående af 1-taller og  \(\mathbf{C}_{12} =  \rho_2 \mathbf{1}\) og \(\mathbf{C}_{22}= 1\). Hvis \(-\frac{1}{p_0 - 1} < \rho_1 < -\frac{1}{p_0}\) og \(1 + \del{p_0 -1} \rho_1 < \vert \rho_2 \vert < \sqrt{\frac{1 + \del{p_0-1}\rho_1}{p_0}}\), da kan betingelse \eqref{eq:betingelse_konsistent} ikke være opfyldt.
%Derfor er lasso variabeludvælgelsen ikke konsistens.
%\end{cor}
%\begin{proof}
%Beviset undlades, men vi referer til s. 1426 i \citep{adaptive_lasso}.
%\end{proof}

Hvis \(\X\) er ortogonal, da er den nødvendige betingelse \eqref{eq:betingelse_konsistent} og dermed konsistent variabeludvælgelse for lasso estimatoren garanteret.
Hvis \(p=2\), er den nødvendige betingelse altid opfyldt, da \(\left\vert \mathbf{C}_{21} \mathbf{C}_{11}^{-1} \text{sign} \del{\beta^*_\A} \right\vert\) reduceres til \(\vert \rho \vert\), som er korrelationen imellem prædiktorerne.



\input{main/ch/sub/asymptotics_adaptivelasso}
\input{main/ch/sub/asymptotics_nonnegativegarrote}
\chapter{Faktor modellen} \label{ch:dfm}
\textit{I dette kapitel gives en kort introduktion af faktor modellen. Kapitlet er hovedsagligt baseret på \citep{stock_watson_2002a} og kapitel 9 i \citep{faktorbook}.} \\[2mm]
%
Det essentielle formål med faktor analyse er at beskrive kovariansen mellem et stort antal variable udfra få underliggende faktorer.
Faktor modellen er motiveret af følgende argument: antag variablerne kan opdeles i grupper udfra deres korrelation, således at variable i en gruppe har høj parvis korrelation, men variablerne har relativ lav korrelation med variabler i andre grupper.
Da vil hver gruppe af variable repræsenter én underliggende faktor.
Nedenfor introduceres den generelle definition af faktor modellen.
%
\begin{defn}[Faktor model] \label{def:faktor_model}
Lad \(\X_t\) være en stokastisk \(p \times 1\) vektor af observeret forklarende variable til tid \(t\) med middelværdi \(\mathbf{0}\) og kovariansmatrix \(\boldsymbol{\Sigma}_{\X \X}\).
Faktor modellen er givet ved
\begin{align}
\X_t = \tLambda \tF_t + \te_t, \label{eq:factor_model}
\end{align}
hvor \(\tF_t\) er en stokastisk \(r \times 1\) vektor af uobserveret faktorer, \(\tLambda\) er en \(p \times r\) matrix med faktor loadings og \(\te_t\) er en \(p \times 1\) vektor af fejlled.
Der antages, at
\begin{align*}
\text{cov} \sbr{\te_t, \tF_t} &= \mathbf{0}, \\
\E{\tF_t} &= \mathbf{0}, \quad \text{cov}\sbr{\tF_t} = \mathbf{I}_r, \\
\E{\te_t} &= \mathbf{0}, \quad \text{cov}\sbr{\te_t} = \boldsymbol{\Psi} = \text{diag} \del{\psi_{11}, \ldots, \psi_{pp}}.
\end{align*}
\end{defn}
%
I kapitlet betragter vi også følgende notation for faktor modellen
\begin{align*}
x_{jt} = \boldsymbol{\lambda}_j \tF_t + e_{jt} = \lambda_{jm}F_{jt} + e_{jt}, \quad j = 1, \ldots, p, \text{ og } m = 1, \ldots, r,
\end{align*} 
hvor \(x_{jt}\), \( e_{jt}\) og \(F_{jt}\) er \(j\)'te element af henholdsvis \(\X_t\), \(\te_t\) og \(\tF_t\), \(\boldsymbol{\lambda}_j \) er \(j\)'te række af \(\tLambda\), mens \(\lambda_{jm}\) betegner \(m\)'te element af \(\boldsymbol{\lambda}_j \).
Faktor modellen på matrix form opskrives ved
\begin{align*}
\X = \tF \tLambda^T + \mathbf{e},
\end{align*}
hvor \(\X\) er en \(T \times p\) matrix med \(t\)'te række \(\X_t^T\) og \(\tF\) er en \(T \times r\) matrix med \(t\)'te række \(\tF_t^T\).
For faktor modellen betragtes prædiktionsligningen \(h\)-step ahead for responsvariablen \(y_t\)
\begin{align}
y_{t+h} = \tbeta_{\tF}^T \tF_t + \tbeta_{\tw}^T \tw_t + \epsilon_{t+h}, \label{eq:factor_model_forecast}
\end{align}
hvor \(\tbeta_{\tF}\) og \(\tbeta_{\tw}\) er henholdsvis \(r \times 1\) og \(m \times 1\) vektorer med regressionskoefficienter, \(\tw_t\) er en \(m \times 1\) vektor af observeret lags af \(y_t\) og \(\epsilon_{t+h}\) betegner prædiktionsfejlen.
Lad \(y_t, \X_t, \tw_t\) være givet for \(t = 1, \ldots, T\), så ønsker vi at prædiktere \(y_{T+h}\).

\section{Estimation af faktorer}
Hvis antallet af variable er relativt lavt, vil man ofte anvende maksimum likelihood estimation til at estimere faktorerne, men i dette tilfælde hvor vi betragter et stort antal variable, skal mange parametre estimeres, hvilket kan være udfordring, derfor anvendes istedet \textit{principal component estimation}.
----
 der anvender en ortogonal transformation til at konvertere en mængde observationer af korrelerede variable til en mængde værdier af ukorrelerede variable, som kaldes principale komponenter.
Denne transformation defineres således at første komponent har den størst mulige varians, og hver efterfølgende komponent har igen den størst mulige varians givet de foregående komponenter er ortogonale.
%Vi antager, at data er stationært, og at variablerne er standardiseret samt responsvariablen centreret.

Lad os betragte minimeringsproblemet
\begin{align}
\argmin_{\tF, \tLambda} \cbr{V \del{\tF, \tLambda}}, \text{ hvor } V \del{\tF, \tLambda} = \del{pT}^{-1} \sum_{j=1}^p \sum_{t=1}^T \del{x_{jt} - \boldsymbol{\lambda}_j \tF_t}^2.  \label{eq:dfm11}
\end{align}
Objektfunktionen omskrives på matrixvektor form
\begin{align*}
V \del{\tF, \tLambda} = \del{pT}^{-1} \text{Trace} \sbr{ \del{\X - \tF \tLambda^T}^T \del{\X - \tF \tLambda^T}}.
\end{align*}
For at løse optimeringsproblemet \eqref{eq:dfm11} differentieres objektfunktionen mht \(\tF\), dette udtryk sættes lig 0 og vi isoleres for \(\tF\), hvoraf vi får, at
\begin{align*}
\frac{\partial}{\partial \tF} \del{\del{\X - \tF \tLambda^T}^T \del{\X - \tF \tLambda^T}} = -2 \X \tLambda + 2 \tF \tLambda^T \tLambda \quad \Longrightarrow \quad \widehat{\tF} = \X \tLambda \del{\tLambda^T \tLambda}^{-1} .
\end{align*}
Af antagelse \ref{ass:faktor}a) har vi  \(\widehat{\tF} = p^{-1} \X \tLambda\).
Hvis vi indsætter \(\widehat{\tF}\) i \eqref{eq:dfm11} fås
\begin{align*}
&\argmin_{\tLambda} \cbr{\text{Trace} \sbr{ \del{\X - p^{-1} \X \tLambda \tLambda^T}^T \del{\X - p^{-1} \X \tLambda \tLambda^T}}} \\
&\argmin_{\tLambda} \cbr{ \text{Trace} \sbr{\X^T \X - p^{-1} \tLambda \tLambda^T \X^T \X - p^{-1} \X^T \X \tLambda \tLambda^T + p^{-2} \tLambda \tLambda^T \X^T \X \tLambda \tLambda^T}},
\end{align*}
hvor der gælder at \(p^{-2} \tLambda \tLambda^T \X^T \X \tLambda \tLambda^T = p^{-1} \X^T \X \tLambda \tLambda^T\) og vi får da
\begin{align*}
\argmin_{\tLambda} \cbr{ \text{Trace} \sbr{\X^T \X - p^{-1} \tLambda \tLambda^T \X^T \X}}
\end{align*}
som er ækvivalent med
\begin{align*}
\argmax_{\tLambda} \cbr{ \text{Trace} \sbr{\tLambda^T \X^T \X \tLambda}}.
\end{align*}
Løsningen til dette problem findes ved at sætte \(\widehat{\tLambda}\) lig med de skalerede egenvektorer af \(\X^T \X\), svarende til dens \(r\) største egenværdier.
Dvs \(\hat{\tLambda} = \sqrt{n} \cdot \boldsymbol{\nu}_r\), hvor \(\boldsymbol{\nu}_r\) er egenvektorerne svarende til de \(r\) største egenværdier af \(\X^T \X\).
Hvorefter vi kan udregne \(\widehat{\tF} = p^{-1} \X \widehat{\tLambda}\), som altså kræver egenvektorerne af \(\X^T \X\).

%Hvis \(p > T\), kan løsningen udregnes simplere ved at koncentrere \(\tLambda\) ud istedet for \(\tF\), og da vil minimeringsproblemet \eqref{eq:dfm11} være ækvivalent med maksimeringsproblemet \\
%\(\argmax_{\tF} \cbr{ \text{Trace} \sbr{ \tF^T \X \X^T \tF}}\) underlagt at \(T^{-1} \tF^T \tF = \mathbf{I}_r\).
%Dette giver estimatoren \(\overset{\smile}{\tF}\), som er en matrix af egenvektorer svarende til de \(r\) største egenværdier af \(\X \X^T\).
%Da søjlerummene for \(\widehat{\tF}\) og \(\overset{\smile}{\tF}\) er ækvivalent, kan disse bruges i flæng, når der forecastes.
%
%De principale komponenter udregnes ved at første komponent beskriver så meget af variation i data som muligt, dvs at den har den største varians, anden komponent er da en lin kombination af variablerne som er ukorreleret med den første principal komponent og den har den største varians underlagt denne betingelse.
%Dvs hver komponent har den største muligt varians under betingelsen af den er ortgonal til de foregående komponenter.
%Dvs den nye tidsrækker er ukorreleret og den første principal komponenter i tidsrækkerne har det største variantion i den originale tidsrækker.

\subsection{Entydighed af estimatorer}
I dette underafsnit introduceres nogle antagelser, som sikrer at estimatorerne er entydige.
Der gælder, at faktorer og faktor loadings ikke er entydige, som vi først vil beskrive. 
Hertil udregnes kovariansmatricen af \(\X_t\), hvor vi betragter
\begin{align*}
\X_t \X_t^T &= \del{ \tLambda \tF_t + \te_t} \del{ \tLambda \tF_t + \te_t}^T \\
&= \tLambda \tF_t \del{\tLambda \tF_t }^T + \te_t \del{\tLambda \tF_t}^T + \tLambda \tF_t \te_t^T + \te_t \te_t^T,
\end{align*}
og finder, at
\begin{align*}
\boldsymbol{\Sigma}_{\X \X} &=  \E{\X_t \X_t^T} \\
&= \tLambda \E{\tF_t \tF_t^T} \tLambda^T + \E{\te_t \tF_t^T} \tLambda^T + \tLambda \E{\tF_t \te_t^T} + \E{\te_t \te_t^T} \\
&= \tLambda \tLambda^T + \boldsymbol{\Psi}.
\end{align*}
Lad \(\tR\) være en \(r \times r\) ortogonal matrix, således at \(\tR \tR^T = \tR^T \tR = \mathbf{I}_r\), da kan \eqref{eq:factor_model} skrives som
\begin{align*}
\X_t = \tLambda \tR \tR^T \tF_t + \te_t = \tLambda^* \tF_t^* + \te_t,
\end{align*}
hvor \(\tLambda^*=\tLambda \tR\) og \(\tF_t^*=\tR^T \tF_t\). 
Da \(\E{\tF_t^*} = \tR^T \E{\tF_t} = \mathbf{0}\) og \(\text{cov} \sbr{\tF^*_t} = \tR^T \text{cov} \sbr{\tF_t} \tR = \tR^T \tR = \mathbf{I}_r\) kan loadings \(\tLambda\) ikke adskilles fra loadings \(\tLambda^*\) udfra \(\X_t\).
Faktorerne \(\tF_t\) og \(\tF^*_t\) har samme statistiske egenskaber, og genererer begge kovariansmatricen \(\boldsymbol{\Sigma}\), som er givet ved
\begin{align*}
\boldsymbol{\Sigma}_{\X \X} = \tLambda \tR \tR^T \tLambda^T + \boldsymbol{\Psi} = \tLambda^* \tLambda^{*^T} + \boldsymbol{\Psi}.
\end{align*}
Derfor kan faktor loadings \(\tLambda\) kun bestemmes op til en ortogonal matrix \(\tR\), hvorfor der pålægges nogle betingelser således at \(\tLambda\) entydigt kan bestemmes.
%derfor skal der anvendes en normalisering for entydigt at definere faktorerne.
%Denne ikke-entydighed kan løses ved at roterer faktorerne således at faktor loadings opfylder følgende betingelser
Nedenfor introduceres nogle modelantagelser, som er nødvendige til at identificere entydige faktorer.
%
\begin{ass}[Faktorer og faktor loadings] \label{ass:faktor}
\begin{enumerate}[label=\alph*)]
\item \(\lim_{p \rightarrow \infty} p^{-1} \tLambda^T \tLambda = \mathbf{I}_r\). 
\item \(\E{\tF_t \tF_t^T} = \boldsymbol{\Sigma}_{\tF\tF}\), hvor \(\boldsymbol{\Sigma}_{\tF\tF}\) er en diagonalmatrix med indgange \(\sigma_{ii} > \sigma_{jj} > 0\) for \(i < j\).
\item \(\abs{\lambda_{jm}} \leq M < \infty\) for \(j = 1, \ldots, p\) og \(m = 1, \ldots, r\), hvor \(M\) er en konstant.
\item \(T^{-1} \sum_{t=1}^T \tF_t \tF_t^T \overset{p}{\rightarrow} \boldsymbol{\Sigma}_{\tF\tF}\).
\end{enumerate}
\end{ass}
%
Antagelse \ref{ass:faktor}.a) medfører, at \(\lim_{p \rightarrow \infty} p^{-1} \tLambda^{*^T} \tLambda^* = \mathbf{I}_r\) og dermed at \(\lim_{p \rightarrow \infty} p^{-1} \tR^T \tLambda{^T} \tLambda \tR = \tR^T \tR = \mathbf{I}_r\), som restringerer \(\tR\) til at være en ortonormal matrix.
Af antagelse \ref{ass:faktor}.b) har vi, at
\begin{align*}
\boldsymbol{\Sigma}_{\X \X} &=  \tLambda  \boldsymbol{\Sigma}_{\tF\tF} \tLambda^T + \boldsymbol{\Psi} \\
&= \tLambda^* \boldsymbol{\Sigma}_{\tF\tF} \tLambda^{*^T} + \boldsymbol{\Psi} \\
&= \tLambda \tR \boldsymbol{\Sigma}_{\tF\tF} \tR^T \tLambda^T +  \boldsymbol{\Psi},
\end{align*}
hvilket betyder, at \(\boldsymbol{\Sigma}_{\tF\tF} = R \boldsymbol{\Sigma}_{\tF\tF} R^T\) og dermed restringerer antagelse \ref{ass:faktor}.b) yderligere \(\tR\) til at være en diagonal matrix med diagonal elementerne \(\pm 1\).
Dette identificerer faktorerne op til en fortegnsfejl.
Antagelse \ref{ass:faktor}.b) tillader også faktorerne at være korreleret.

Lad os betragte \(\del{pT}^{-1} \sum_{t=1}^T \tLambda \tF_t \tF_t^T \tLambda^T\), som vi tager middelværdien af og lader \(T \rightarrow \infty\), da får vi, at
\(\lim_{T \rightarrow \infty} p^{-1} \tLambda \del{T^{-1} \sum_{t=1}^T \E{\tF_t \tF_t^T}} \tLambda^T = p^{-1} \tLambda \boldsymbol{\Sigma}_{\tF\tF} \tLambda^T\).
Spektral dekomposition sætningen giver, så at diagonal elementerne i \(\boldsymbol{\Sigma}_{\tF\tF}\) er egenværdierne for \(p^{-1} \tLambda \boldsymbol{\Sigma}_{\tF\tF} \tLambda^T\) og søjlerne i \(\tLambda\) er de tilsvarende egenvektorer.
%
%Antagelse \ref{ass:faktor} giver en normalisering asymptotisk ved at forbinde \(\tLambda\) med de ordnede ortonormal egenvektorer af \(\del{p T}^{-1} \sum_{t=1}^T \tLambda \tF_t \tF_t^T \tLambda^T\) og \(\tF_t\) med de principale komponenter af \(\tLambda \tF_t\) for \(j=1, \ldots, T\).
%Diagonal elementerne af \(\boldsymbol{\Sigma}_{\tF \tF}\) svarer til grænsende egenværdier af \(\del{p T}^{-1} \sum_{t=1}^T \tLambda \tF_t \tF_t^T \tLambda^T\).

I den klassiske faktor model \ref{def:faktor_model} antages fejlleddene at være iid, men for makroøkonomiske tidsrækker er dette urealistisk, da tidsrækkerne er autokorreleret og nogle forklarende variable kan være krydskorreleret. 
Derfor har vi følgende antagelser for fejlleddene.
%
\begin{ass}[Momenter af fejlene \(\te_t\)] \label{ass:momenter_fejl}
Lad \(e_{jt}\) betegne \(j\)'te element af \(\te_t\), da antages, at
\begin{enumerate}[label=\alph*)]
\item \(\lim_{p \rightarrow \infty} \sup_t \sum_{u = - \infty}^\infty \abs{\E{p^{-1} \te_t^T \te_{t+u}}} < \infty\).
\item \(\lim_{p \rightarrow \infty} \sup_t p^{-1} \sum_{j = 1}^p \abs{\E{e_{jt} e_{it}}} < \infty\).
\item \(\lim_{p \rightarrow \infty} \sup_{t,s} p^{-1} \sum_{j=1}^p \sum_{i=1}^p \abs{\text{cov}\del{e_{js} e_{jt}, e_{is} e_{it}}} < \infty\).
\end{enumerate}
\end{ass}
%
Antagelse \ref{ass:momenter_fejl}.a) tillader autokorrelation for fejlene.
Antagelse \ref{ass:faktor}.b) tillader fejlene at være svagt korreleret på tværs af tidsrækkerne.
Vi antager ikke normalitet, men antagelse \ref{ass:faktor}.c) begrænser størrelsen af fjerde momenter.

%\section{Prædiktion med faktor modellen}
%Først estimeres tidsrækkerne af faktorer udfra de forklarende variable, og herefter kan relationen mellem responsvariablen og faktorerne estimeres vha OLS.
%
For prædiktionsligningen \eqref{eq:factor_model_forecast} antages, at hvis \(\tF_t\) er observeret, da vil OLS give konsistente estimatorer af regressions koefficienterne.
Nedenfor gives de specifikke antagelser for prædiktionsligningen.
%
\begin{ass}[Prædiktionsligning] \label{ass:forecasting_ligning}
Lad \(\tz_t = \del{\tF_t^T \boldsymbol{\omega}_t^T}^T\) og \(\tbeta = \del{\tbeta_{\tF}^T \tbeta_{\boldsymbol{\omega}}^T}^T\) være \(\del{r+m} \times 1\) vektorer, da gælder følgende
\begin{enumerate}[label=\alph*)]
\item \(\E{\tz_t \tz_t^T} = \boldsymbol{\Sigma}_{\tz \tz} = \begin{pmatrix}
\boldsymbol{\Sigma}_{\tF \tF} & \boldsymbol{\Sigma}_{\tF \tw} \\
\boldsymbol{\Sigma}_{\tw \tF} & \boldsymbol{\Sigma}_{\tw \tw}
\end{pmatrix} \) er en positiv definit matrix.
\item \(T^{-1} \sum_{t=1}^T \tz_t \tz_t^T \overset{p}{\rightarrow} \boldsymbol{\Sigma}_{\tz \tz}\).
\item \(T^{-1} \sum_{t=1}^T \tz_t \epsilon_{t+h} \overset{p}{\rightarrow} \mathbf{0}\).
\item  \(T^{-1} \sum_{t=1}^T \epsilon_{t+h}^2 \overset{p}{\rightarrow} \sigma^2\).
\item \(\abs{\tbeta} < \infty\).
\end{enumerate}
\end{ass}
%
Antagelse \ref{ass:forecasting_ligning}.a)-\ref{ass:forecasting_ligning}.c) medfører, at regressionen af \(y_{t+h}\) på \(\tz_t\) giver konsistente OLS estimatorer givet ved \(\widehat{\tbeta} = \del{\sum_{t=1}^{T-h} \widehat{\tz}_t \widehat{\tz}_t^T}^{-1} \del{\sum_{t=1}^{T-h} \widehat{\tz}_t y_{t+h}}\).
De yderligere antagelser er nødvendige for at sikre konsistente OLS estimatorer af regressionen af \(y_{t+h}\) på \(\widehat{\tz}_t^T =\del{\widehat{\tF}_t^T \tw_t^T}\), hvor \(\tF_t\) altså ikke er observeret.
Inden prædiktionen skal vi estimere faktorerne og bestemme antallet af faktorer.



\subsection{Konsistens af estimatorer}
I dette underafsnit introduceres nogle sætninger, som sikrer, at estimatorerne i faktor modellen er konsistente.
Vi inkluderer kun de centrale sætninger hertil, og referer derfor til andre beviser i beviserne til disse sætninger.

Sætning \ref{thm:factorthm1} giver, at estimatorerne er punktvis konsistente og har en begrænset MSE, som konvergerer i sandsynlighed mod 0.
Af antagelse \ref{ass:faktor} kan vi blot estimere faktorerne op til en fortegnsfejl, derfor introduceres en variabel \(S_j\), som korrigerer for dette.
%
\begin{thm} \label{thm:factorthm1}
Lad \(S_j\) betegne en variabel med værdi \(\pm 1\), lad \(p, T \rightarrow \infty\) og antag at antagelse \ref{ass:faktor} og \ref{ass:momenter_fejl} er opfyldt.
Antag yderligere at \(k\) faktorer estimeres og \(r\) er det sande antal faktorer.
Da kan \(S_j\) vælges således at følgende gælder:
\begin{enumerate}[label=\alph*)]
\item \(T^{-1} \sum_{t=1}^T \del{S_j \widehat{F}_{jt} - F_{jt}}^2 \overset{p}{\rightarrow} 0\), for \(j=1, \ldots, r\).
\item \(S_j \widehat{F}_{jt} \overset{p}{\rightarrow} F_{jt}\), for \(j=1, \ldots, r\).
\item \(T^{-1} \sum_{t=1}^T \widehat{F}_{jt}^2 \overset{p}{\rightarrow} 0\), for \(j=r+1, \ldots, k\).
\end{enumerate}
\end{thm}
%
\begin{proof}
Beviset undlades, men vi refererer til (R14), (R15) og (R19) s. 1176 i \citep{stock_watson_2002a}.
\end{proof}
%
%Hvis \(\tLambda\) er kendt, da kan estimationen simplificeres, idet \(\tF_t\)  kan estimeres udfra en OLS regression af \(\cbr{x_{jt}}_{j=1}^p\) på \(\cbr{\lambda_j}_{j=1}^p\).
%For at bestemme om den resulterede estimator er konsistent, skal vi betragte \(\widehat{\tF}_t - \tF_t = \del{p^{-1} \tLambda^T \tLambda}^{-1} \del{p^{-1} \sum_{j=1}^p \lambda_j e_{jt}}\).
%For \(p \rightarrow \infty\) gælder, at \(p^{-1} \tLambda^T \tLambda \overset{p}{\rightarrow} \mathbf{I}_r\) af antagelse \ref{ass:faktor}.a) og \(p^{-1} \sum_{j=1}^p \lambda_j e_{jt} \overset{p}{\rightarrow} 0\) af antagelse \ref{ass:momenter_fejl}.a) og \ref{ass:faktor}.c), hvoraf konsistens af \(\widehat{\tF}_t\) følger direkte.
%Hvis istedet \(\tF\) er kendt, da kan \(\lambda_j\) estimeres udfra regression \(\cbr{x_{jt}}_{j=1}^p\) på \(\cbr{\tF_{t}}_{t=1}^T\) og da skulle vi istedet undersøge \(\widehat{\lambda}_j - \lambda_j= \del{T^{-1} \sum_{t=1}^T \tF_t \tF_t^T}^{-1} \del{T^{-1} \sum_{t=1}^T \tF_t e_{jt}}\) for \(T \rightarrow \infty\) tilsvarende.
%
%Da både \(\tF\) og \(\tLambda\) er ukendt, kræves at \(p, T \rightarrow \infty\), hvilket er betydeligt sværere at bevise.
%Strategien for beviset er, at vise at de \(r\) første egenvektorer af \(\del{pT}^{-1} \X^T \X\) opfører sig, som de \(r\) første egenvektorer af \(\del{pT}^{-1} \tLambda^T \tF^T \tF \tLambda\), og da vise at disse egenvektorer kan bruges til at konstruere en konsistent estimator af \(\tF\).
%
Hernæst introduceres et lemma, som anvendes ofte i beviset for sætning \ref{thm:factorthm2}.
%
\begin{lem} \label{lem:factorlem1}
Lad \(q_t\) betegne en følge af stokastiske variable, hvor \(T^{-1} \sum_{t=1}^T q_t^2 \overset{p}{\rightarrow} \sigma_q^2\) og \(T^{-1} \sum_{t=1}^T \tF_t q_t \overset{p}{\rightarrow} \Sigma_{\tF q}\).
Da gælder, at \(T^{-1} \sum_{t=1}^T \mathbf{S} \widehat{\tF}_t q_t \overset{p}{\rightarrow} \Sigma_{\tF q}\), hvor \(\mathbf{S} = \text{diag} \del{S_1, \ldots, S_r}\).
\end{lem}
\begin{proof}
Beviset undlades, men vi refererer til (R16) s. 1176 i \citep{stock_watson_2002a}.
\end{proof}
%
%\begin{proof}
%Vi har, at
%\begin{align}
%T^{-1} \sum_{t=1}^T \mathbf{S} \hat{\tF}_t q_t &=p^{-1} T^{-1} \sum_{t=1}^T \mathbf{S} \widehat{\tLambda}^T \X_t q_t \nonumber \\
%&= T^{-1} \sum_{t=1}^T \del{p^{-1} \mathbf{S} \widehat{\tLambda}^T \tLambda} \tF_t q_t + p^{-1} T^{-1} \sum_{t=1}^T \mathbf{S} \widehat{\tLambda}^T \te_t q_t, \label{eq:factorlem1}
%\end{align}
%hvor der gælder, at \(p^{-1} \mathbf{S} \widehat{\tLambda}^T \tLambda \overset{p}{\rightarrow} \mathbf{I}_r\) af (R12) s. 1175 i \citep{stock_watson_2002a}.
%Vi antog, at \(T^{-1} \sum_{t=1}^T \tF_t q_t \overset{p}{\rightarrow} \Sigma_{Fq}\), derfor får vi for det første led i \eqref{eq:factorlem1} at
%\begin{align*}
%T^{-1} \sum_{t=1}^T \del{N^{-1} \mathbf{S} \widehat{\tLambda}^T \tLambda} \tF_t q_t \overset{p}{\rightarrow} \Sigma_{Fq}.
%\end{align*}
%For det andet led i \eqref{eq:factorlem1} betragtes det \(j\)'te element som må opfylde
%\begin{align*}
%\abs{p^{-1} T^{-1} \sum_{t=1}^T S_i \underline{\widehat{\lambda}}_i^T \te_t q_t} &= \abs{T^{-1} \sum_{t=1}^T q_t \del{p^{-1} \sum_{j=1}^p \widehat{\lambda}_{ji} e_{jt}}} \\
%& \leq \sup_{\gamma \in \Gamma} \abs{T^{-1} \sum_{t=1}^T q_t \del{p^{-1} \sum_{j=1}^p \gamma_j e_{jt}}} \\
%& \overset{p}{\rightarrow} 0,
%\end{align*}
%hvor uligheden følger af at \(\widehat{\lambda}_j \in \Gamma\) hvor \(\Gamma = \cbr{\gamma \given p^{-1} \gamma^T \gamma = 1}\) og \(\gamma\) er en \(p \times 1\) vektor, og grænsen kommer af (R3) s. 1175 i \citep{stock_watson_2002a}.
%\end{proof}
%
Næste sætning viser, at prædiktionen, som konstrueres udfra de estimerede faktorer og estimerede parametre, er asymptotisk efficient.
Yderligere vises at regressionskoefficients estimatorerne er konsistente.

Resultatet antager, at prædiktionsligningen \eqref{eq:factor_model_forecast} er estimeret med det sande antal faktorer, dvs \(k=r\).
Dette er tab af generalitet, da flere metoder konsistent estimerer antallet af faktorer.
%
\begin{thm} \label{thm:factorthm2}
Lad antagelse \ref{ass:forecasting_ligning} og betingelserne i sætning \ref{thm:factorthm1} være opfyldt. 
Lad \(\widehat{\tbeta}_{\tF}\) og \(\widehat{\tbeta}_{\tw}\) betegne OLS estimaterne af \(\tbeta_{\tF}\) og \(\tbeta_{\tw}\) fra regressionen af \(y_{t+h}\) på \(\widehat{\tz}_t\) for \(t=1, \ldots, T-h\). Da gælder følgende
\begin{enumerate}[label=\alph*)]
\item \(\del{\widehat{\tbeta}_{\tF}^T \widehat{\tF}_T + \widehat{\tbeta}^T_{\tw} \tw_T} - \del{\tbeta_{\tF}^T \tF_T + \tbeta_{\tw}^T \tw_T} \overset{p}{\rightarrow} 0\).
\item \(\widehat{\tbeta}_{\tw} - \tbeta_{\tw} \overset{p}{\rightarrow} \mathbf{0}\) og \(S_j\) defineret i sætning \ref{thm:factorthm1} kan vælges således at \(S_j \widehat{\beta}_{j \tF} - \beta_{j \tF} \overset{p}{\rightarrow} 0\) for \(j = 1, \ldots, r\).
\end{enumerate}
\end{thm}
%
\begin{proof}
Først bevises b). 
Lad \(\widehat{\tbeta} = \del{\widehat{\tbeta}_{\tF}^T \widehat{\tbeta}_{\tw}^T}^T\), da skal vi vise, at \(\widehat{\tbeta}_{\tw} - \tbeta_{\tw} \overset{p}{\rightarrow} \mathbf{0}\) og \(S_j \widehat{\beta}_{j \tF} - \beta_{j \tF} \overset{p}{\rightarrow} 0\) for \(j = 1, \ldots, r\).
Vi har, at
\begin{align*}
\begin{pmatrix}
\mathbf{S} \widehat{\tbeta}_{\tF} \\ \widehat{\tbeta}_{\tw}
\end{pmatrix} &= \del{T^{-1} \sum_{t=1}^T \begin{pmatrix}
\widehat{\tF}_t^T \mathbf{S} \\ \tw_t^T
\end{pmatrix} \begin{pmatrix}
\widehat{\tF}_t^T \mathbf{S} & \tw_t^T
\end{pmatrix}}^{-1} \del{T^{-1} \sum_{t=1}^T \begin{pmatrix}
\widehat{\tF}_t^T \mathbf{S} \\ \tw_t^T
\end{pmatrix} \del{\tbeta_{\tF}^T \tF_t + \tbeta_{\tw}^T \tw_t + \epsilon_{t+h}}} \\
&= \begin{pmatrix}
T^{-1} \sum_{t=1}^T \widehat{\tF}_t \widehat{\tF}_t^T & T^{-1} \mathbf{S} \sum_{t=1}^T \widehat{\tF}_t \tw_t^T \\
T^{-1} \sum_{t=1}^T \tw_t \widehat{\tF}_t^T \mathbf{S} & T^{-1} \sum_{t=1}^T \tw_t \tw_t^T
\end{pmatrix}^{-1} 
\del{ \begin{pmatrix}
T^{-1} \sum_{t=1}^T \mathbf{S} \widehat{\tF}_t \del{\tbeta_{\tF}^T \tF_t + \tbeta_{\tw}^T \tw_t} \\  T^{-1} \sum_{t=1}^T \tw_t \del{\tbeta_{\tF}^T \tF_t + \tbeta_{\tw}^T \tw_t}
\end{pmatrix}
+ \begin{pmatrix}
T^{-1} \mathbf{S} \sum_{t=1}^T \widehat{\tF}_t \epsilon_{t+h} \\ T^{-1} \sum_{t=1}^T \tw_t \epsilon_{t+h}
\end{pmatrix} }\\
&= \begin{pmatrix}
T^{-1} \sum_{t=1}^T \widehat{\tF}_t \widehat{\tF}_t^T & T^{-1} \mathbf{S} \sum_{t=1}^T \widehat{\tF}_t \tw_t^T \\
T^{-1} \sum_{t=1}^T \tw_t \widehat{\tF}_t^T \mathbf{S} & T^{-1} \sum_{t=1}^T \tw_t \tw_t^T
\end{pmatrix}^{-1} \\
&\qquad
\del{ \begin{pmatrix}
T^{-1} \sum_{t=1}^T \mathbf{S} \widehat{\tF}_t \tF_t^T & T^{-1} \mathbf{S} \sum_{t=1}^T \widehat{\tF}_t \tw_t^T \\
T^{-1} \sum_{t=1}^T \tw_t \tF_t^T & T^{-1} \sum_{t=1}^T \tw_t \tw_t^T
\end{pmatrix} 
\begin{pmatrix}
\tbeta_{\tF} \\ \tbeta_{\tw}
\end{pmatrix}  + \begin{pmatrix}
T^{-1} \mathbf{S} \sum_{t=1}^T\widehat{\tF}_t \epsilon_{t+h} \\ T^{-1} \sum_{t=1}^T \tw_t \epsilon_{t+h},
\end{pmatrix} }
\end{align*}
som omskrives til
\begin{align*}
\begin{pmatrix}
\mathbf{S} \widehat{\tbeta}_{\tF} \\ \widehat{\tbeta}_{\tw}
\end{pmatrix} - \begin{pmatrix}
\tbeta_{\tF} \\ \tbeta_{\tw}
\end{pmatrix} &= \begin{pmatrix}
T^{-1} \sum_{t=1}^T \widehat{\tF}_t \widehat{\tF}_t^T & T^{-1} \mathbf{S} \sum_{t=1}^T \widehat{\tF}_t \tw_t^T \\
T^{-1} \sum_{t=1}^T \tw_t \widehat{\tF}_t^T \mathbf{S} & T^{-1} \sum_{t=1}^T \tw_t \tw_t^T
\end{pmatrix}^{-1} \begin{pmatrix}
T^{-1} \mathbf{S} \sum_{t=1}^T \widehat{\tF}_t \epsilon_{t+h} \\
T^{-1} \sum_{t=1}^T \tw_t \epsilon_{t+h}
\end{pmatrix} \\
&\overset{p}{\rightarrow} \begin{pmatrix}
\boldsymbol{\Sigma}_{\tF \tF} & \boldsymbol{\Sigma}_{\tF \tw} \\ \boldsymbol{\Sigma}_{\tw \tF} & \boldsymbol{\Sigma}_{\tw \tw}   
\end{pmatrix}^{-1} \begin{pmatrix}
\mathbf{0} \\ \mathbf{0}
\end{pmatrix} = \mathbf{0},
\end{align*}
hvor 
\(T^{-1} \sum_{t=1}^T \tw_t \tw_t^T \overset{p}{\rightarrow} \boldsymbol{\Sigma}_{\tw \tw}\) samt \(T^{-1} \sum_{t=1}^T \tw_t \epsilon_{t+h} \overset{p}{\rightarrow} \mathbf{0}\) følger direkte af henholdsvis antagelse \ref{ass:forecasting_ligning}.b) og \ref{ass:forecasting_ligning}.c).
Af lemma \ref{lem:factorlem1} har vi, at: \\
\(T^{-1} \sum_{t=1}^T \widehat{\tF}_t \widehat{\tF}_t^T \rightarrow \boldsymbol{\Sigma}_{\tF \tF}\) for \(q_t = S_j \widehat{F}_{jt}\), da \(T^{-1} \sum_{t=1}^T \widehat{F}_{jt}^2 \overset{p}{\rightarrow} \sigma_{jj}\), som følger af (R13) s. 1175-1176 i \citep{stock_watson_2002a}, og da \(T^{-1} \sum_{t=1}^T \mathbf{S} \widehat{\tF}_t \tF_t \overset{p}{\rightarrow} \boldsymbol{\Sigma}_{\tF \tF}\), som også følger af samme lemma for \(q_t =F_{jt}\) for \(j=1,\ldots, r\), da antagelse \ref{ass:faktor}.d) opfylder \(T^{-1} \sum_{t=1}^T \tF_t \tF_t^T \overset{p}{\rightarrow} \boldsymbol{\Sigma}_{\tF\tF}\). \\ 
Lemma \ref{lem:factorlem1} giver også, at 
\(T^{-1} \mathbf{S} \sum_{t=1}^T \widehat{\tF}_t \tw_t^T \overset{p}{\rightarrow} \boldsymbol{\Sigma}_{\tF \tw}\) for \(q_t = w_{jt}\), hvor antagelse \ref{ass:forecasting_ligning}.b) giver at \(T^{-1} \sum_{t=1}^T \tz_t \tz_t^T \overset{p}{\rightarrow} \boldsymbol{\Sigma}_{\tz \tz}\). \\
Igen af lemma \ref{lem:factorlem1} har vi, at \(T^{-1} \mathbf{S} \sum_{t=1}^T \widehat{\tF}_t \epsilon_{t+h} \overset{p}{\rightarrow} 0\) for \(q_t = \epsilon_{t+h}\), hvor antagelse \ref{ass:forecasting_ligning}.c) og \ref{ass:forecasting_ligning}.d) giver at \(T^{-1} \sum_{t=1}^T \tz_t \epsilon_{t+h}^2 \overset{p}{\rightarrow} \mathbf{0}\) og \(T^{-1} \sum_{t=1}^T \epsilon_{t+h}^2 \overset{p}{\rightarrow} \sigma^2\). \\
Af antagelse \ref{ass:forecasting_ligning}.a) er \(\boldsymbol{\Sigma}_{\tz \tz}\) invertibel og resultatet følger af Slutskys sætning \ref{thm:slutsky}.

Herefter bevises a), hvor vi skal vise, at \(\widehat{\tbeta}^T \widehat{\tz}_T - \tbeta^T \tz_T \overset{p}{\rightarrow} 0\).
Lad \(\tR = \begin{pmatrix}
\mathbf{S} & 0 \\ 0 & \mathbf{I}_{n_{\tw}}
\end{pmatrix}\), hvor \(n_{\tw}\) betegner antallet af elementer i \(\tw_t\), da fås
\begin{align*}
\widehat{\tbeta}^T \widehat{\tz}_T - \tbeta^T \tz_T &= \del{\tR \widehat{\tbeta}}^T \tR \widehat{\tz}_T - \tbeta^T \tz_T \\
&= \del{\tR \widehat{\tbeta} - \tbeta}^T \tz_T + \del{\tR \widehat{\tbeta}}^T \del{\tR \widehat{\tz}_T - \tz_T} \\
&\overset{p}{\rightarrow} 0.
\end{align*}
Af antagelse \ref{ass:forecasting_ligning}.a) har vi, at \(\E{\tz_T \tz_T^T} = \boldsymbol{\Sigma}_{\tz \tz}\), dvs \(\tz_T\) er \(O_p \del{1}\) og sætning \ref{thm:factorthm2}.b) giver at \(\del{\tR \widehat{\tbeta} - \tbeta}^T \overset{p}{\rightarrow} 0\), dermed forsvinder første led i sandsynlighed af slutskys sætning \ref{thm:slutsky}.
Tilsvarende da \(\tbeta\) er endelig af antagelse \ref{ass:forecasting_ligning}.e) og \(\del{\tR \widehat{\tz}_T - \tz_T} \overset{p}{\rightarrow} 0\) af (R15) s. 1176 i \citep{stock_watson_2002a}, forsvinder også andet led i sandsynlighed af slutskys sætning \ref{thm:slutsky}. 
\end{proof}

\section{Valg af antal faktorer}
%Antallet af faktorer kan bestemmes udfra et såkaldt \textit{scree plot}.
%Her er egenværdierne ordnede fra den største til den mindste.
%For at bestemme antallet af faktorer ser vi efter en bøjning i scree plottet.
%Antallet af komponenter er givet ved punktet hvori de resterende egenværdier er relativ lav og approksimativ samme størrelse.
%
Valg af antal faktorer kan baseres på informationskriterier.
Informationskriterierne betragter tradeoff mellem at inkludere en ekstra faktor, dvs en ekstra parameter i modellen, mod omkostningen af at øge variabiliteten, som kommer af at estimere en ekstra parameter.
AIC kan anvendes til at udvælge antallet af faktorer, men \citep{Bai_Ng} beviset at dette ikke giver et konsistent estimat.
Istedet foreslås at betragtet funktionen i \eqref{eq:dfm11}, som en funktion af \(\widehat{\tF}\) og \(k\), hvor \(0<k<k_\text{max}\) er antallet af faktorer, dvs
\begin{align*}
V \del{k, \widehat{\tF}} = \del{pT}^{-1} \sum_{j=1}^p \sum_{t=1}^T \del{x_{jt} -\boldsymbol{\lambda}_j \widehat{\tF}_t}^2
\end{align*}
som i fællesskab med en straffunktion \(g \del{p,T}\) giver informationskriteriet
\begin{align*}
\text{IC} \del{k} = \ln V \del{k, \widehat{\tF}} + k g \del{N,T}.
\end{align*}
\citep{Bai_Ng} foreslår følgende straffunktioner
\begin{align*}
g_1 \del{p,T} &= \frac{p + T}{p T} \ln \del{\frac{pT}{p + T}}, \\
g_2 \del{p,T} &= \frac{p + T}{p T} \ln \del{ \min \cbr{p, T}}, \\
g_3 \del{p,T} &= \frac{\ln \del{\min \cbr{p, T}}}{\min \cbr{p, T}},
\end{align*}
som resulterer i konsistente kriterierne, der betegnes henholdsvis \(\text{IC}_1 \del{k}\), \(\text{IC}_2 \del{k}\) samt \(\text{IC}_3 \del{k}\).

For \(p = T\) fås at \(g_2 \del{T,T} = 2T^{-1} \ln \del{T}\), som bekendt er \(2\) gange BIC straffaktoren.

%\citep{Bai_Ng} viste, at under betingelserne af approksimativ dfm, da er \(\hat{r}\) som minimere et af informationskriterierne med \(g \del{N,T}\) opfylder disse betingelser konsistent for den sande værdi af \(r\), under antagelse af at værdien af \(r\) er endelig og ikke stiger med \(\del{N, T}\).

\chapter{Empirisk analyse}
I dette kapitel anvender vi data fra FRED, som er hentet fra Federal Reserve Bank of St. Louis \url{https://research.stlouisfed.org/econ/mccracken/fred-databases/}.
Der anvendes nogle transformationer for at gøre tidsrækkerne stationære, som er noteret mere detaljeret i appendiks ... 
Disse transformationer er forslået af Michael McCracken fra Federal Reserve Bank of St. Louis.  
Der observeres at 5 af disse variabler har mere end 43 NA's inkluderet og derfor bliver de fjernet fra datasættet. 
Derudover ser vi at de resterende NA's opstår kun i de første eller/og sidste observationer, vi fjerner derfor 17 observationer i hver variable for at undgå NA's. 
Så vores datasæt inkluderer altså 123 tidsrækker, som indeholder $691$ månedlige observationer og går fra 1. januar 1960 til 1. juli 2017. 
Hernæst har vi standardiseret vores data, således at variablerne er centreret omkring 0 og har en varians 1, således vi undgår skæringen i vores regression. 

Datasættet repræsenterer en stor brede af makroøkonomiske variabler, som Michael McCracken har inddelt i 8 kategorier. 
\begin{enumerate}
\item \textbf{Output og indkomst:} Indeholder 16 tidsrækker
\item \textbf{Arbejdsmarked:}  Indeholder 31 tidsrækker
\item \textbf{Boliger:} Indeholder 10 tidsrækker
\item \textbf{Forbrug, ordrer og varebeholdninger:} Indeholder 7 tidsrækker
\item \textbf{Penge og kredit:} Indeholder 14 tidsrækker
\item\textbf{ Rente og valutakurs:} Indeholder 21 tidsrækker
\item \textbf{Priser:} Indeholder 20 tidsrækker
\item \textbf{Aktiemarked:} Indeholder 4 tidsrækker
\end{enumerate}

Når vi konstruerer vores model anvender vi 122 forklarende variabler og 1 respons variable og for at validerer vores model deler vi vores fulde datasæt i et træningssæt, som består af 552 observationer fra 1. januar 1960 til 1. december 2005 og et testsæt, som består af 139 observationer fra 1. januar 2006 til 1. Juli 2017. 
En betydningsfuld makroøkonomisk variable er arbejdsløshed, som blandt andet vil være vores responsvariable.  Arbejdsløshed er inkluderet i gruppen arbejdsmarked. 
\section{Benchmark}
Faktormodellen


\section{Shrinkage metoder}
I denne sektion vil vi finde den optimale $\lambda$ for de forskellige shrinking metoder. 
Vi har tidligere introduceret to algoritmer til at løse vores optimerings problemer, som vi vil anvende i denne sektion.
Vi deler derfor vores analyse op i to dele, hvor vi anvender hhv. coordinate descent og LARS algoritmen.

Metoderne er alle anvendt med forskellige værdier af $\lambda$, vi vælger derfor én 10-gang-krydsvalidering for at estimerer den optimale værdi af lambda, således vi kan finde den bedste mulige model. 
Krydsvalidering bliver målt i gennemsnitlig kvadrerede fejl. 
Vi er interesseret i den model med mindst gennemsnitlige kvadrerede fejl, men kompleksiteten vil også have en rolle.
Derfor ser vi ikke kun på den $\lambda$ der giver mindst mulige krydvaliderings fejl (der betegnes $\lambda_{\min}$), men også den største værdi således at fejlen er inde for en standard afvigelses af minimum, som vi betegner $\lambda_{\text{1sd}}$.  

\subsection{Coordinate descent}
Den her sektion er baseret på pakkerne \textit{glmnet}, \citep{glmnet} og \textit{gglasso}, \citep{gglasso}.

Figur \ref{tab:cv_plot} viser den gennemsnitlige krydsvaliderings fejl for hver værdi af $\log \lambda$.
Fejlene er målt i MSE.

Det skal lige bemærkes, at for Elastic net har vi to turning parameter $\alpha$ og $\lambda$. 
Så vi har anvendt en 10-gange krydsvalidering for hele 10 værdier af $\alpha$, hvor $\alpha \in (0,1)$. 
For hvert $\alpha$ har vi fundet $\lambda_{\min}$ og $\lambda_{1\text{sd}}$ og deres krydsvalideringsfejl. Vi finder hernæst den $\alpha$, som giver den mindste krydsvaliderings fejl for $\lambda_{\min}$, samt $\lambda_{1\text{sd}}$. 
Den mindste krydsvalideringsfejl er så når  $\alpha = 0.9$ både for $\lambda_{\min}$ og $\lambda_{1\text{sd}}$. 
Derudover skal der bemærket at for group lasso, skal vi indelle vores forklarende variabler i nogle grupper. Disse grupper er forslået af Michael McCracken og ses i appendiks.

\imgfigh{cv_plot.pdf}{1}{10-gange krudsvaliderings fejl plottede som en function af $ \log(\lambda)$. De stiplede indikerer minimum fejl, samt fejlen med en standard afvigelse af minimum for hver metode}{cv_plot}

For at få et bedre overblik viser tabel  \ref{tab:cv_tab} værdierne af vores $\lambda$ samt antallet af koefficienter. 
Vi ser for lasso og elastic net der sker en reducering af antallet af parameter for $\lambda_{1\text{sd}}$, men hvor MSE ikke er signifikant forskellig. 
Ud fra dette lader vi den optimale lambda for lasso og elastic net være $\lambda_{1\text{sd}}$ for lasso og Elastic net. 

For Ridge regression ved vi at der ingen reducering vil være for antallet af parameter, men en reducering af de estimerede koefficients værdier og derfor lader vi den optimale lambda være den med mindst krydvalideringsfejl og anvender $\lambda_{\min}$, som vores optimale værdi. 

Group lasso opfører sig anderledes end hvad vi ville have forventet. 
Den fejler i reducering af parameter. 
Det indikerer lidt på, at Group lasso ikke er en god model for vores data. 
Men vi  lader den optimale lambda være $\lambda_{\min}$, da den har mindst krydsvalideringsfejl. 

\input{fig/tab/cv_tab}

Tabel \ref{tab: lasso_ud} viser hvilken koefficienter elastic net og lasso udvælger, og de fleste variabler de to variabler udvælger tilhører samme gruppe, som vores respons variable altså arbejdsmarked. 
\input{fig/tab/lasso_ud}

\subsection{Lars}
Vi vil gerne se om en anden iterativ metode til at løse vores problem kan ændre vores resultater. 
I den her sektion anvender vi lars algoritmen 
Vi anvender pakken ... til at udfører variable selektion med lasso. 
\imgfigh{lars_lasso.pdf}{0.7}{}{lars_lasso}

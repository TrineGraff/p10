\chapter{Lasso i lineære modeller}
\textit{I dette kapital introduceres lasso estimatoren for lineær regression. 
Først beskrives standard lasso og dens relation til ridge regression, hvorefter vi skitser en simple udregning af lasso.
Herefter beskrives nogle generaliseringer af standard lasso, hvor iblandt vi kan nævne elastisk net, group lasso og adaptive lasso.} \\[4mm]
%
Betragt \(n\) observationer \(\{x_i, y_i\}_{i=1}^n \), hvor $x_i=(x_{i1}, \ldots, x_{ip})$ er en $p$ dimensional vektor af forklarende variable eller prædiktorer og $y_i \in \R$ er den tilhørende respons variabel.

Den velkendte estimator for mindste kvadraters metode for $(\beta_0, \beta)$ findes udfra
\begin{align}
\arg \min_{\beta_0, \beta} \cbr{\frac{1}{2n} \sum_{i=1}^n \del{y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j}^2} \label{eq:OLS}
\end{align}
Som bekendt er løsningen på matrix-vektor form af mindste kvadraters metoden givet ved
\begin{align*}
\hat{\beta}^{\text{OLS}} = (\X^T \X)^{-1} \X^T \y.
\end{align*}
Som bekendt er estimatoren unbiased, men har ofte høj varians. 
Prediktions præcisionen kan ofte forbedres hvis koefficienterne mindskes eller sættes direkte lig 0.
Dette vil give estimatoren lidt bias, men reducere variansen og dermed forbedre prediktions præcisionen.
Det anden årsag til at vi leder efter alternative til mindste kvadraters metode er, at med et højt antal prediktorer ønsker vi at bestemme en mindre delmængde af disse som siger mest om prediktiorerne, dvs forbedre fortolkning.\\[4mm]

Hvis \(p \gg n\) da vil prediktorerne være kollineære og dermed har \(\X\) ikke fuld rang.
Det betyder, at $\X^T \X$ er singulær og der findes derfor en ikke entydig estimator for mindste kvadraters metode.
flere variabel end obs \\[4mm]
%
Nedenfor introduceres \textit{lasso} som kombinerer objektfunktionen i \eqref{eq:OLS} med en $\ell_1$-norm betingelse eller øvre grænse for summen af de absolutte værdier af koefficienterne.
Denne betingelse mindsker koefficienterne og sætter endda nogle lig 0. 
Dermed udfører metoden model udvælgelse i lineær regression.
Det resulterende optimeringsproblem er konveks, og kan løses effektivt som beskrives nærmere i kapitel \ref{kap:optimeringsmetoder}.



%
%Vi betragter et sædvanligt lineær regression set-up. Lad $\textbf{y} = \del{y_1, \dots, y_n}^T$ være respons variablen og lad $\textbf{X} = \del{\textbf{x}_1, \dots, \textbf{x}_n}$ være en $n \times p$ matrix af prædiktorer, hvor $\textbf{x}_j = \del{x_{1,j}, \dots, x_{n,j}}^T$, $j = 1, \dots, p$ er vektorer af prædiktorer. 
%Vi antager en lineær funktion af prædiktorer, som har normal fordelte fejl. 
%\begin{align*}
%\textbf{y} = \textbf{X} \mathcal{\beta} + \epsilon, \quad \epsilon \sim N \del{0, \sigma^2 \text{\textbf{I}}_n},
%\end{align*}
%hvor $\mathcal{\beta} = \del{\beta_1, \dots, \beta_p}^T$ er en vektor af ukendte parameter og $\text{\textbf{I}}_n$ er $n \times n$ identitetsmatricen. 
%For nu antager vi, at $p<n$ og at første kolonne i $\X$ er alle elementer 1'taller. 
%Vi angiver $\widehat{\boldsymbol{\beta}}^{OLS}$ til at være den mindste kvadraters estimat;
%\begin{align*}
%\widehat{\boldsymbol{\beta}}^{OLS} = \underset{\boldsymbol\beta \in \R^p}{\argmin} \norm{ \textbf{y} - \textbf{X}\boldsymbol\beta}^2_2
%\end{align*}
%Den velkendte løsning er givet ved 
%\begin{align*}
%\widehat{\boldsymbol\beta}^{\text{OLS}} = \del{\X^T \X}^{-1} \X^T \y.
%\end{align*}
%For at inverterer $\X^T \X$ skal $\X$ have fuld rang ellers er $\widehat{\boldsymbol\beta}^{\text{OLS}}$ ikke unik. 
%Men når  \(p \gg n\) vil prædiktorerne være kollineære og dermed har \(\X\) ikke fuld rang.
%Det betyder, at $\X^T \X$ er singulær og der findes derfor en ikke entydig estimator for mindste kvadraters metode.



\input{main/ch/sub/lasso}
\input{main/ch/sub/generalizations_lasso}
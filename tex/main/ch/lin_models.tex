\chapter{Lasso i lineære modeller}
\textit{I dette kapitel introduceres lasso estimatoren for lineær regression. 
Først beskrives standard lasso og dens relation til ridge regression, hvorefter vi skitser en simple udregning af lasso.} \\[4mm]
%
Givet \(n\) observationer betragtes responsvariablen \(y_i\) og en \(p \times 1\) vektor \(\tx_i\) af prædiktorer, hvor \(j\)'te element betegnes \(x_{ij}\).
%Betragt \(n\) observationer \(\{\tx_i, y_i\}_{i=1}^n \), hvor $\tx_i=(x_{i1}, \ldots, x_{ip})$ er en $p$ dimensional vektor af forklarende variable eller prædiktorer og $y_i \in \R$ er den tilhørende responsvariabel.

Den velkendte estimator for mindste kvadraters metode (OLS) findes ved at minimere summen af kvadrerede residualer (SSR)
\begin{align}
\widehat{\tbeta}^\text{OLS} = \argmin_{\beta_0, \tbeta \in \R^p} \cbr{\sum_{i=1}^n \del{y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j}^2}. \label{eq:OLS}
\end{align}
%
Ofte standardiseres prædiktorerne, således at de er centreret \( \del{\frac{1}{n} \sum_{i=1}^n x_{ij} = 0}\) og har unit varians \( \del{\frac{1}{n} \sum_{i=1}^n x_{ij}^2=1}\).
Hvis ikke prædiktorerne standardiseres, da vil estimaterne afhænge af enhederne, som prædiktorerne er målt i.
For fuldstændighed centreres responsvariablen også \( \del{\frac{1}{n} \sum_{i=1}^n y_{i} = 0} \).
Hermed kan vi se bort fra skæringen $\beta_0$ i det givne optimeringsproblem.
Givet en optimal løsning \(\widehat{\tbeta}\) på det centreret data, kan vi finde løsningen for det ikke-centreret data: der gælder, at \(\widehat{\tbeta}\) er den samme og 
\(\widehat{\beta}_0 = \bar{y} - \sum_{j=1}^p \bar{x}_j \widehat{\beta}_j\), hvor \(\bar{y}\) og \(\bar{x}_j \) for \(j=1, \ldots, p\) er gennemsnittene for det ikke-centreret data.
Derfor ser vi bort fra skæringen i resten af dette kapitel samt kapitel \ref{ch:generalisering_lasso}, hvor generaliseringer af lasso estimatoren introduceres.

Lad \(\y\) være en \(n \times 1\) vektor med responsvariable og lad \(\X\) være en $n \times p$ matrix med  i'te række $\tx_i$, da kan \eqref{eq:OLS} omskrives til matrix-vektor form
\begin{align*}
\widehat{\tbeta}^\text{OLS} = \argmin_{ \tbeta \in \R^p} \cbr{ \Vert \y - \X \tbeta \Vert_2^2},
\end{align*}
hvor \(\Vert \cdot \Vert_2\) betegner den Euklidiske norm.
Som bekendt er løsningen hertil givet ved
\begin{align*}
\widehat{\tbeta}^{\text{OLS}} = (\X^T \X)^{-1} \X^T \y.
\end{align*}
OLS estimatoren er unbiased, men har ofte høj varians. 
Prædiktionen af responsvariablen kan ofte forbedres, hvis koefficienterne mindskes eller sættes direkte lig 0.
Dette vil give estimatoren lidt bias, men reducere variansen, hvilket forbedrer bias-variance tradeoff og dermed også prædiktionen.
En anden årsag til, at vi leder efter alternativer til OLS er, at med et højt antal prædiktorer ønsker vi, at bestemme en mindre delmængde af disse, som siger mest om responsvariablen, dvs forbedre fortolkningen.

Hvis \(p > n\), da kan \(\X\) ikke have fuld rang.
Det betyder, at $\X^T \X$ er singulær, og der findes derfor ikke en entydig estimator for OLS.

Nedenfor introduceres \textit{lasso estimatoren}, som kombinerer objektfunktionen i \eqref{eq:OLS} med en $\ell_1$-norm betingelse eller øvre grænse for summen af de absolutte værdier af koefficienterne.
Denne betingelse mindsker koefficienterne og sætter endda nogle lig 0. 
Dermed udfører metoden variabeludvælgelse i lineær regression.
Det resulterende optimeringsproblem er konveks og kan løses effektivt, som beskrives nærmere i kapitel \ref{ch:optimeringsmetoder}.
%
\input{main/ch/sub/lasso}
\input{main/ch/sub/uniqueness_lasso}
\input{main/ch/sub/nonnegative_garrote}
%
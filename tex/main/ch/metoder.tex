\chapter{Metoder til valg af model} \label{ch:metoder}
I dette kapitel vil vi redegøre for nogle metoder, som anvendes til at udvælge den ``bedste'' model. 
Vi er interesseret i en model, der er god til at prædiktere, og som ikke er for kompleks.

Datasættet opdeles i en træningsmængde og en testmængde. 
Observationerne i træningsmængden betragtes som in-sample observationer og anvendes primært til at identificere modellen og estimere parametrene. 
Observationerne i testmængden betragtes som out-of-sample observationer og anvendes til at vurdere modellen. 
 
\section{In-sample metoder}
For faktor modellen skal vi bestemme antallet af faktorer, mens vi for lasso og dens generaliseringer skal estimere tuning parameteren.

\subsection{Valg af antal faktorer} \label{sec:faktorer}
%Antallet af faktorer kan bestemmes udfra informationskriterier, som betragter et tradeoff mellem at inkludere en ekstra faktor, dvs en ekstra parameter i modellen, mod omkostningen af at øge variabiliteten, som kommer af at estimere en ekstra parameter.
%AIC kan anvendes til at udvælge antallet af faktorer, men \citep{Bai_Ng} beviset, at dette ikke giver et konsistent estimat.
Antallet af faktorer kan bestemmes udfra nogle informationskriterier foreslået af \citep{Bai_Ng}, som er givet ved
\begin{align}
\text{IC}_1 \del{k} &= \ln V \del{k, \widehat{\tF}} + k \frac{p + T}{p T} \ln \del{\frac{p T}{p + T}}, \label{eq:ic1} \\
\text{IC}_2 \del{k} &= \ln V \del{k, \widehat{\tF}} + k \frac{p + T}{p T} \ln \del{ \min \cbr{p, T}}, \label{eq:ic2} \\
\text{IC}_3 \del{k} &= \ln V \del{k, \widehat{\tF}} + k \frac{\ln \del{\min \cbr{p, T}}}{\min \cbr{p, T}}, \label{eq:ic3}
\end{align}
hvor \(V \del{k, \widehat{\tF}}\) er givet i \eqref{eq:dfm11}, men nu som en funktion af \(\widehat{\tF}\) og \(k\), hvor \(0<k<k_\text{max}\) er antallet af faktorer, dvs
\begin{align*}
V \del{k, \widehat{\tF}} = \del{p T}^{-1} \sum_{j=1}^p \sum_{t=1}^{T} \del{x_{jt} -\boldsymbol{\lambda}_j \widehat{\tF}_t}^2.
\end{align*}
Informationskriterierne giver konsistent estimation af antallet af faktorer i faktor modellen.
%For \(p = T\) fås at \(g_2 \del{T,T} = 2 T^{-1} \ln \del{T}\), som bekendt er \(2\) gange straffaktoren for BIC.

\subsection{Valg af tuning parameter}
I dette underafsnit introduceres to metoder til at estimere tuning parameteren i lasso modellen og den generaliseringer, nemlig krydsvalidering og BIC. 
Krydsvalidering finder modellen med mindst prædiktionsfejl, mens BIC vælger den model, som giver det bedste fit i forhold til modellens kompleksitet. 
%For elastisk net problemet \eqref{eq:4.2} betragtes to tuning parametre \(\lambda\) og \(\alpha\).

\subsubsection{Krydsvalidering}
\textit{Dette afsnit er baseret på s. 178-186 i \citep{james}.}
%\textit{En af de simpleste og oftest anvendte metoder til at estimere prædiktions fejlen er krydsvalidering. 
%Dette afsnit giver en kort introduktion af krydsvalidering og er baseret på s. 175-184 i \citep{james}.}

%Krydsvalidering er en metode hvor vi estimerer prædiktions fejl fra træningssættet ved at holde fast i en del mængde. Delmængden bliver så anvendt som valideringssæt. 
%Denne tilgang med at anvende et valideringssæt er en simple strategi for at estimerer prædiktions fejl på en mængde af observationer. 

%Tilgangen splitter tilfældige observationer ind  i to delmængder, et træningssæt og et valideringssæt. 
%Forskellige regressions modeller fittes på træningsdata og deres prædiktion af responsvariablen er evaluerede i valideringssættet. 
%Valideringssættets fejl er normalt målt i MSE. 
%Tilgangen med anvendelse af et valideringssæt er nem at implementere, men den har to ulemper. 
%\begin{itemize}
%\item Prædiktions fejlene kan være meget varierende, da fejlene afhænger af hvilken observationer der er inkluderet i træningssættet og valideringsættet. 
%\item Idet denne tilgang deler vores data op i to delmængder vil vi derfor have færre observationer til at fitte vores model på. Derudover performere statistiske metoder sig dårligere på et træningsdata med færre informationer. Derfor kan valideringsættets have en tendens til at overestimere prædiktions fejlene for modellen, som er fitted på hele datasættet. ??
%\end{itemize}
%For at undgå disse to problemstillinger introduceres k-fold krydsvalidering. 

%\subsubsection{k-fold krydsvalidering}
En k-fold krydsvalidering opdeler tilfældigt observationerne i \(k\) grupper, som er tilnærmelsesvis af samme størrelse.
Én af disse grupper anvendes som valideringsmængde, og de resterende \(k-1\) grupper betragtes som træningsmængde.
En regressionsmodel fittes på træningsmængden, hvorfra den gennemsnitlige kvadrerede fejl findes udfra valideringsmængden, hvilket giver \(\text{MSE}_1\).
Denne procedure gentages \(k\) gange, således at hver gruppe betragtes som valideringsmængde.
Dette resulterer i \(k\) estimater af prædiktionsfejlen $\text{MSE}_1, \text{MSE}_2, \dots , \text{MSE}_k$.
Estimatet for \(k\)-fold krydsvalidering er da givet ved
\begin{align}
\text{CV}_k = \frac{1}{k} \sum_{i=1}^k \text{MSE}_i. \label{eq:cv_k}
\end{align}
Oftest er $k=5$ eller $k = 10$. 
Figur \ref{fig:cv_teori} illustrerer en 5-fold krydsvalidering. 
%
\begin{figure}
\center
\scalebox{0.6}{\input{fig/img/cv.tikz}}
\caption{5-fold krydsvalidering. I dette tilfælde fittes modellen i første, anden, fjerde og femte gruppe af data og udregner prædiktionsfejlen af den fittede model for tredje gruppe.} \label{fig:cv_teori}
\end{figure} 
%
Et $k$-fold krydsvalidering har en lav varians, da \eqref{eq:cv_k} tager det gennemsnitlige output af $k$ fittede modeller, som har en lav korrelation, siden vi har et relativt lille overlap mellem træningsmængderne i hver model. 
Bias kunne være et problem i forhold til hvordan vi vælger vores træningsmængde. Hvis vi ikke har nok observationer i træningsmængden vil $k$-fold krydsvalidering have høj bias. Derfor er der også en bias-variance trade-off med valget af $k$. 

\subsubsection{BIC}
Tuning parameteren kan også vælges udfra Bayes informationskriteriet.
Vi betragter dens skalerede version.
%
\begin{defn}[Bayes informationskriteriet (BIC)] \label{def:bic}
\begin{align*}
\text{BIC} =  \frac{- 2 \widehat{\ell}}{T} + \frac{p \log T}{T}, 
\end{align*}
hvor \(\widehat{\ell}\) er den maksimerede log-likehood, \(p\) er antallet af parametre og \(T\) er antallet af observationer i træningsmængden.
\end{defn} 
%
%BIC vil være større des lavere log-likehood eller jo flere parametre der er i modellen.
Modellen med den laveste BIC vælges, da det indikerer, at modellen giver en god tilnærmelse af data i forhold til modellens kompleksitet. 

Lad fejlledene være uafhængige og normalfordelte, da er maksimum likelihood estimatoren for variansen defineret som
\begin{align*}
\widehat{\sigma}_p^2 = \frac{1}{T} \sum_{t=1}^{T} \del{y_t - \sum_{j=1}^p x_{tj}\beta_j}^2,
\end{align*}
og BIC kan omskrives til
\begin{align*}
\text{BIC} = \log \widehat{\sigma}^2_p + \frac{p \log T}{T}.
\end{align*}

\section{Out-of-sample metoder}
I den empiriske del betragter vi en one-step-ahead prædiktion, hvor estimeringsvinduet udvides med én observeret observation per prædiktion.
%Dermed prædikteres \(y_{t+1}\) udfra de observerede observation fra \(1\) til \(t\), mens  \(y_{t+2}\) prædiktion er baseret på de observerede observationer fra \(1\) til \(t+1\).
For at vurdere en models prædiktion betragtes den gennemsnitlige absolutte fejl (MAE) og den gennemsnitlige kvadrerede fejl (MSE)
\begin{align}
\text{MAE} & =  \frac{1}{T} \sum_{t=1}^{T} \abs{y_{t} - \widehat{y}_{t}} \label{eq:mae} , \\
\text{MSE} & =  \frac{1}{T} \sum_{t=1}^{T} \del{y_{t} - \widehat{y}_{t}}^2 \label{eq:mse},
\end{align} 
hvor $T$ er antallet af observationer i testmængden, $y_{t}$ er observationen til tid $t$ og $\widehat{y}_{t}$ er prædiktionen af $y_{t}$.
MAE og MSE kan betragtes som tabsfunktioner, da de måler en afvigelse fra de observerede værdier.
Tabsfunktionerne vil være lig 0 for en perfekt prædiktion, og ellers vil de have en positiv værdi, derfor foretrækkes modeller med lav MAE eller MSE.

Udfra tabsfunktionerne kan vi betragte et gennemsnitlig tabs ratio mellem en given model og benchmark modellen, som er givet ved
\begin{align}
R^{\text{MAE}} = \frac{\text{MAE}^{\text{Alternativ}}}{\text{MAE}^{\text{Benchmark}}} \qquad R^{\text{\text{MSE}}} = \frac{\text{MSE}^{\text{Alternativ}}}{\text{MSE}^{\text{Benchmark}}}. \label{eq:gennemsnitligtabsratio}
\end{align} 
Hvis et gennemsnitlig tabs ratio er mindre end 1, da er den alternative model bedre end benchmark modellen og omvendt hvis værdien er større end 1. 
Derudover vil vi betragte et rullende gennemsnitligt tabs ratio, som vi betegner $R_t^\text{MAE}$ og $R_{t}^\text{MSE}$, for at vurdere modellernes prædiktion fra observation til observation.
%
%For at vurdere modellernes prædiktion i forhold til benchmark modellen fra observation til observation, defineres et rullende gennemsnitlig tabs ratio, som er givet ved
%\begin{align*}
%R_{t}^\text{MSE} =\frac{\text{MSE}_\text{t}^\text{Alternative}}{\text{MSE}_\text{t}^\text{Benchmark}}, \qquad\frac{\text{MAE}_\text{t}^\text{Alternative}}{\text{MAE}_\text{t}^\text{Benchmark}},
%\end{align*}
%hvor $\text{MSE}_t = \frac{1}{T} \sum_{t =1}^{T} \del{y_t - \widehat{y}_t}^2$ og $\text{MAE}_t = \frac{1}{T} \sum_{t = 1}^{T} \abs{y_t - \widehat{y}_t}$.
%Hvis et rullende gennemsnitlig tabs ratio er mindre end 1, da prædikterer den alternative model gennemsnitlig bedre end benchmark modellen til tid \(t\) og omvendt hvis værdien er større end 1.

Foruden MAE og MSE betragtes \textit{Diebold-Mariano} testen samt \textit{model confidence set} (MCS) proceduren.
Diebold-Mariano testen kan anvendes til at teste om én model er signifikant bedre end en anden model, mens MCS proceduren identificerer en mængde af modeller, som er signifikant bedre end de øvrige modeller, men ikke signifikant bedre end de øvrige modeller i mængden.
Nedenfor introduceres kort Diebold-Mariano testen samt den generelle teori for MCS.

\input{main/ch/sub/diebold_mariano}
\input{main/ch/sub/mcs}

\section{LASSO for lineære modeller}

Betragt \(n\) observationer \(\{x_i, y_i\}_{i=1}^n \), hvor $x_i=(x_{i1}, \ldots, x_{ip})$ er en $p$ dimension vektor af forklarende variable eller prediktorer og $y_i \in \R$ er den tilhørende respons variabel.

Det velkendte estimat for mindste kvadraters metode for $(\beta_0, \beta)$ findes udfra
\begin{align*}
\min_{\beta_0, \beta} \cbr{\frac{1}{2n} \sum_{i=1}^n \del{y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j}^2}
\end{align*}
Dette estimat har ofte lav bias men høj varians, hvilket betyder



Lasso finder løsningen til optimerings problemet
\begin{align}
\min_{\beta_0, \beta} \cbr{\frac{1}{2n} \sum_{i=1}^n \del{y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j}^2}, \quad s.t. \sum_{j=1}^p \vert \beta_j \vert \leq t \label{eq:2.3}
\end{align}
Betingelsen $\sum_{j=1}^p \vert \beta_j \vert \leq t$ kan skrives mere kompakt ved $\Vert \beta \Vert_1 \leq t$.
Dette kan udtrykkes på matrix-vektor notation.
Lad \(\y=(y_1, \ldots, y_n)\) være en \(n\) dimensional vektor med responsvariable og \(\X\) være en $n \times p$ matrix med $x_i \in \R^p$ som den i'te række, da kan \eqref{eq:2.3} omskrives til
\begin{align*}
\min_{\beta_0, \beta} \cbr{\frac{1}{2n} \Vert \y - \beta_0 \mathbf{1} - \X \beta \Vert_2^2}, \quad s.t. \Vert \beta \Vert_1 \leq t,
\end{align*}
hvor \(\mathbf{1}\) er en \(n\) dimensionel vektor bestående af 1 og \(\Vert \cdot \Vert_2\) betegner den Euklidiske norm af vektorer.

Grænsen \(t\) begrænser summen af de absolutte værdier af parameter estimaterne.
Denne skal specificeres ved en ekstern procedure kaldet \textit{kryds validering}, som vil blive diskuteret i kap --.

Ofte standardiseres prediktorerne \(\X\) således at kolonnerne er centeret og har varians 1. Dvs \(\frac{1}{n} \sum_{i=1}^n x_{ij} = 0\) og \(\frac{1}{n} \sum_{i=1}^n x_{ij}^2=1\). Hvis ikke prediktorerne standardiseres da vil lasso estimaterne afhænge af enhederne.
Hvis prediktorerne er målt i samme enhed, da vil vi typisk ikke standardisere.
For fuldstændigheden, antager vi også at responsvariablen $y_i$ er centeret, dvs \(\frac{1}{n} \sum_{i=1}^n y_{i} = 0\).
Når data er centreret da kan vi se bort fra skæringen $\beta_0$ i lasso optimeringen.
Given en optimal lasso løsning \(\hat{\beta}\) på det centreret data, kan vi finde løsningen for det ikke-centreret data. Der gælder at
\begin{align*}
\hat{\beta}^{\text{ikke-centreret}} = \hat{\beta}^{\text{centreret}} \\
\hat{\beta}_0^{\text{ikke-centreret}} = \bar{y} - \sum_{j=1}^p \bar{x}_j \hat{\beta}_j
\end{align*}
Derfor ser vi bort fra skæringen resten af kapitlet.

Vi kan omskrive lasso problemet til Lagrange form
\begin{align}
\min_{\beta} \cbr{\frac{1}{2n} \Vert \y - \X \beta \Vert_2^2 + \lambda \Vert \beta \Vert_1} \label{eq:2.5}
\end{align}
for $\lambda \geq 0$.
Af Lagrange dualiteten er der en bijektion mellem \eqref{eq:2.3} og \eqref{eq:2.5}: for hver værdi af \(t\) hvor \(\Vert \beta \Vert_1 \leq t\) er opfyldt, da findes en tilhørende værdi af $\lambda$ som giver den samme løsning for \eqref{eq:2.5}.
Mens løsningen $\hat{\beta}_\lambda$ til \eqref{eq:2.5} løser grænse problemet med $t=\Vert \hat{\beta}_\lambda \Vert_1$

 
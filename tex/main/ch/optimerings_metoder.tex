\chapter{Optimeringsmetoder} \label{kap:optimeringsmetoder}
\textit{Lasso problemet er et quadratic programming problem med lineære uligheder. 
Flere algoritmer kan løse dette problemet grundet konveksiteten.  
I dette kapitel beskrives to af disse algoritmer kaldet coordinate descent og Least Angle Regression (LARS).} \\[4mm]
%
For en model matrix i general position findes der ikke en eksplicit løsning for lasso estimatoren.
Men som sagt er objektfunktionen af lasso problemet \eqref{eq:2.5} konveks.
For at vise dette kan objektfunktionen skrives som
\begin{align*}
f \del{\beta} = g \del{\beta} + h \del{\beta},
\end{align*}
hvor \(g \del{\beta} = \Vert \y - \X \beta \Vert_2^2\) og \(h \del{\beta} = \lambda \Vert \beta \Vert_1\).
For \(g \del{\beta}\) udregnes Hessematricen
\begin{align*}
\Delta^2 g \del{\beta} = \frac{\partial^2 g }{\partial \beta_i \partial \beta_j} = 2 \X^T \X.
\end{align*}
For enhver vektor \(\ell \in \R^p\) er \(\ell^T \X^T \X \ell > 0\), dvs \(\ell^T \X^T \X \ell \) er positiv semidefinit, hvilket medfører at \(g \del{\beta}\) er konveks.
For ethvert \(\alpha \in \del{0,1}\) og \(\beta\), \(\beta'\) har vi at
\begin{align*}
h \del{\alpha \beta + \del{1+\alpha}\beta'} &= \lambda \Vert \alpha \beta + \del{1+\alpha} \beta' \Vert_1 \\
&\leq \lambda \Vert \alpha \beta \Vert_1 + \lambda \Vert \del{1+\alpha} \beta' \Vert_1 \\
&= \lambda \alpha \Vert \beta \Vert_1 + \lambda \del{1+\alpha} \Vert \beta' \Vert_1 \\
&= \alpha h \del{\beta} + \del{1+\alpha} h \del{\beta'},
\end{align*}
hvilket medfører, at \(h \del{\beta}\) er konveks.
Da summen af to konvekse funktioner er konveks, medfører dette konveksiteten af \(f \del{\beta}\). 



\input{main/ch/sub/coordinate_descent}
\input{main/ch/sub/lars_theory}


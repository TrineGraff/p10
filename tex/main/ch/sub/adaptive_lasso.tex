\section{Adaptive lasso}
Adaptive lasso blev introduceret i \citep{adaptive_lasso} og er endnu en udvidelse af standard lasso.
Ideen bag adaptive lasso er at tildele prædiktorernes koefficienter individuelle straffe, istedet for at alle koefficienter straffes ligeligt, som er tilfældet for standard lasso.
Den vægtede lasso er givet ved
\begin{align*}
\argmin_{\tbeta \in \R^p} \cbr{\Vert \y - \X \tbeta \Vert_2^2 + \lambda \sum_{j=1}^p w_j \vert \beta_j \vert},
\end{align*}
hvor \(\mathbf{w}\) er en kendt \(p \times 1\) vektor og \(w_j \geq 0\).

Adaptive lasso er blot en vægtet lasso, hvor vægtene \(\mathbf{w}\) er bestemt, således at metoden opfylder orakelegenskaberne, som vi vil bevise i afsnit \ref{subsec:konsistentAL}.
\begin{defn}[Adaptive lasso]
Antag \(\tilde{\tbeta}\) er rod-n konsistent til \(\tbeta^*\) (se definition \ref{def:rodn}).
Vælg \(\gamma>0\) og definer \(\widehat{\mathbf{w}} = \frac{1}{\vert \tilde{\tbeta} \vert^\gamma}\), da er adaptive lasso estimaterne givet ved
\begin{align}
\widehat{\tbeta}^\text{AL} = \argmin_{\tbeta \in \R^p} \cbr{ \Vert \y - \X \tbeta \Vert_2^2 + \lambda \sum_{j=1}^p \frac{ \vert \beta_j \vert}{\vert \tilde{\beta}_j \vert }^\gamma}. \label{eq:4.76}
\end{align}
\end{defn}
%Af sætning --- Der gælder, at \(\widehat{\tbeta}^\text{OLS}\) og \(\widehat{\tbeta}^\text{lasso}\) er rod-n konsistent.

\subsection{Udregning af adaptive lasso}
Givet \(\tilde{\tbeta}\) er optimeringsproblemet \eqref{eq:4.76} konveks, og kan dermed løses med coordinate descent og LARS algoritmen.
\subsubsection{Coordinate descent}
Opdateringerne for adaptive lasso i coordinate descent algoritmen er blot en simpel udvidelse af opdateringerne for standard lasso i afsnit \ref{subsec:udregning_lasso}.
For standardiserede prædiktorer og en centeret responsvariabel er coordinate descent opdateringen for $j$'te koefficient givet ved
\begin{align}
\widehat{\beta}^\text{AL}_j \del{\lambda}= S_{\frac{\widehat{\mathbf{w}} \lambda}{2n}} \del{\tilde{\beta}_j}, \label{eq:AL_coordinate_update}
\end{align}
hvor \(\tilde{\beta}_j = \frac{1}{n} \sum_{i=1}^n r_{i}^{(j)} x_{ij}\) og \(r_{i}^{(j)} = y_i - \sum_{k \neq j} x_{ik} \widehat{\beta}^\text{AL}_k \del{\lambda}\) er de partielle residualer.
Vi gennemløber opdateringen \eqref{eq:AL_coordinate_update} indtil konvergens.

\begin{eks}
På figur \ref{fig:diabetes_lasso_adaptivelasso} illustreres koefficientstierne for lasso og adaptive lasso med OLS vægte og \(\gamma = 1\) for diabetes data.
\imgfigh{diabetes_lasso_adaptivelasso.pdf}{0.9}{Koefficientstierne for lasso og adaptive lasso med OLS vægte og \(\gamma = 1\) som funktion af $\log \del{\lambda}$ for diabetes data.}{diabetes_lasso_adaptivelasso}

\end{eks}

\subsubsection{LARS}
Adaptive lasso problemet kan løses med LARS algoritmen ud fra følgende simple trin
\begin{enumerate}
\item Definér \(\mathbf{x}_j^{*} = \frac{\mathbf{x}_j}{\widehat{w}_j}\) for \(j=1, \ldots, p\)
\item Løs lasso problemet for alle \(\lambda_n\): \(\widehat{\tbeta}^{*} = \argmin_{\tbeta \in \R^p} \left\Vert \y - \sum_{j=1}^p \mathbf{x}_j^{*} \beta_j \right\Vert^2 + \lambda_n \sum_{j=1}^p \vert \beta_j \vert\)
%\begin{align*}
%\widehat{\tbeta}^{*} = \argmin_{\tbeta} \left\Vert \y - \sum_{j=1}^p \mathbf{x}_j^{*} \beta_j \right\Vert^2 + \lambda_n \sum_{j=1}^p \vert \beta_j \vert
%\end{align*}
\item Adaptive lasso estimatoren er da givet ved \(\widehat{\beta}_j^{\text{AL}} = \frac{\widehat{\beta}_j^{*}}{\widehat{w}_j}\)
\end{enumerate}
%computermæssige omkostninger er af orden \(O\del{np^2}\) svarende til et enkelt OLS fit.
%
%Antag vi anvendes \(\hat{\beta}^\text{OLS}\) til at konstruere vægtene i adaptive lasso, da ønsker vi at finde et optimal par af \(\del{\gamma, \lambda_n}\).
%For en given \(\gamma\), kan vi udføre to dimensionel krydsvalidering sammen med LARS algoritmen til at søge efter en optimal \(\lambda_n\).
%I princippet kan \(\hat{\beta}^\text{OLS}\) med andre rod-n konsistente estimatoren.
%Vi kan behandle den som en tredje tunning parameter og udføre en tre-dimensionel krydsvalidering til at finde en optimal triple \(\del{\hat{\beta}, \gamma, \lambda_n}\).
%I \citep{adaptive_lasso} foreslås \(\hat{\beta}^\text{OLS}\) medmindre kollinaritet er en bekymring, i dette tilfælde kan vi forsøge med \(\hat{\beta}^\text{ridge}\), da den er mere stabil end \(\hat{\beta}^\text{OLS}\).
%
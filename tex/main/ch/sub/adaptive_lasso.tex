\section{Adaptive lasso}
Adaptive lasso blev introduceret i \citep{adaptive_lasso} og er endnu en udvidelse af standard lasso.
Ideen bag adaptive lasso er at tildele prædiktorernes koefficienter individuelle straffe, istedet for at alle koefficienter straffes ligeligt, som er tilfældet for standard lasso.
Den vægtede lasso er givet ved
\begin{align*}
\argmin_{\tbeta \in \R^p} \cbr{\Vert \y - \X \tbeta \Vert_2^2 + \lambda \sum_{j=1}^p w_j \vert \beta_j \vert},
\end{align*}
hvor \(\mathbf{w}\) er en kendt \(p \times 1\) vektor og \(w_j \geq 0\).

Adaptive lasso er blot en vægtet lasso, hvor vægtene \(\mathbf{w}\) er bestemt, således at metoden opfylder orakelegenskaberne.
\begin{defn}[Adaptive lasso]
Antag responsvariablen er centreret og prædiktorerne er standardiseret.
Antag yderligere at \(\tilde{\tbeta}\) er rod-n konsistent til \(\tbeta^*\).
Vælg \(\gamma>0\) og definer \(\widehat{\mathbf{w}} = \frac{1}{\vert \tilde{\tbeta} \vert^\gamma}\), da er adaptive lasso estimaterne givet ved
\begin{align}
\widehat{\tbeta}^\text{AL} = \argmin_{\tbeta \in \R^p} \cbr{ \Vert \y - \X \tbeta \Vert_2^2 + \lambda \sum_{j=1}^p \frac{ \vert \beta_j \vert}{\vert \tilde{\beta}_j \vert^\gamma}}. \label{eq:4.76}
\end{align}
\end{defn}
Af sætning --- Der gælder, at \(\widehat{\tbeta}^\text{OLS}\) og \(\widehat{\tbeta}^\text{lasso}\) er rod-n konsistent.

\subsection{Udregning af adaptive lasso}
Givet \(\tilde{\tbeta}\) er \eqref{eq:4.76} konveks, og kan dermed løses med coordinate descent og LARS algoritmen.
\subsubsection{Coordinate descent}


På figur \ref{fig:diabetes_lasso_adaptivelasso} illustreres koefficientstierne for lasso og adaptive lasso med OLS vægte for diabetes data.
\imgfigh{diabetes_lasso_adaptivelasso.pdf}{0.9}{Koefficientstierne for lasso og adaptive lasso med OLS vægte som funktion af $\log \del{\lambda}$ for diabetes data.}{diabetes_lasso_adaptivelasso}

\subsubsection{LARS}
Adaptive lasso problemet kan løses med LARS algoritmen \citep{efron}, udfra følgende simple trin
\begin{enumerate}
\item Definer \(\mathbf{x}_j^{*} = \frac{\mathbf{x}_j}{\widehat{w}_j}\) for \(j=1, \ldots, p\)
\item Løs lasso problemet for alle \(\lambda_n\): \(\widehat{\tbeta}^{*} = \argmin_{\tbeta} \left\Vert \y - \sum_{j=1}^p \mathbf{x}_j^{*} \beta_j \right\Vert^2 + \lambda_n \sum_{j=1}^p \vert \beta_j \vert\)
%\begin{align*}
%\widehat{\tbeta}^{*} = \argmin_{\tbeta} \left\Vert \y - \sum_{j=1}^p \mathbf{x}_j^{*} \beta_j \right\Vert^2 + \lambda_n \sum_{j=1}^p \vert \beta_j \vert
%\end{align*}
\item Adaptive lasso estimatoren er da givet ved \(\widehat{\beta}_j^{\text{AL}} = \frac{\widehat{\beta}_j^{*}}{\widehat{w}_j}\)
\end{enumerate}
%computermæssige omkostninger er af orden \(O\del{np^2}\) svarende til et enkelt OLS fit.
%
%Antag vi anvendes \(\hat{\beta}^\text{OLS}\) til at konstruere vægtene i adaptive lasso, da ønsker vi at finde et optimal par af \(\del{\gamma, \lambda_n}\).
%For en given \(\gamma\), kan vi udføre to dimensionel krydsvalidering sammen med LARS algoritmen til at søge efter en optimal \(\lambda_n\).
%I princippet kan \(\hat{\beta}^\text{OLS}\) med andre rod-n konsistente estimatoren.
%Vi kan behandle den som en tredje tunning parameter og udføre en tre-dimensionel krydsvalidering til at finde en optimal triple \(\del{\hat{\beta}, \gamma, \lambda_n}\).
%I \citep{adaptive_lasso} foreslås \(\hat{\beta}^\text{OLS}\) medmindre kollinaritet er en bekymring, i dette tilfælde kan vi forsøge med \(\hat{\beta}^\text{ridge}\), da den er mere stabil end \(\hat{\beta}^\text{OLS}\).
%
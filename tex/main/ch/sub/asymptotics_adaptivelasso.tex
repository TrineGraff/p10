\section{Adaptive lasso} \label{subsec:konsistentAL}
I dette afsnit vil vi bevise, at adaptive lasso estimaterne opfylder orakelegenskaberne for et passende valg af \(\lambda_n\). 
Lad os betragte adaptive lasso estimatoren
\begin{align*}
\widehat{\tbeta}^\text{AL} = \argmin_{\tbeta \in \R^p} \cbr{ \Vert \y - \sum_{j=1}^p \x_j \beta_j \Vert_2^2 + \lambda_n \sum_{j=1}^p \widehat{w}_j \vert \beta_j \vert},
\end{align*}
hvor \(\lambda_n\) varierer med \(n\).
Lad \(\mathcal{A}_n^\text{AL} = \cbr{j \ : \widehat{\beta}_j^{\text{AL}} \neq 0 }\) betegne den aktive mængde for adaptive lasso estimatoren.
%
\begin{thm}\label{thm:ALoracle}
Antag $\frac{\lambda_n}{\sqrt{n}} \rightarrow 0$ og $\lambda_n n^\frac{\gamma-1}{2} \rightarrow \infty$, da opfylder adaptive lasso estimaterne følgende:
\begin{itemize}
\item Konsistent variabeludvælgelsen: $\lim_{n \rightarrow \infty} P(\mathcal{A}_n^\text{AL}=\mathcal{A})=1$.
\item Asymptotisk normalitet: $\sqrt{n}\left( \widehat{\boldsymbol{\beta}}_\mathcal{A}^{\text{AL}}-\boldsymbol{\beta}_\mathcal{A}^* \right) \overset{d}{\rightarrow} N(\textbf{0},\sigma^2 \boldsymbol{C}_{11}^{-1}).$
\end{itemize} 
\end{thm}
%
\begin{proof}
Først bevises asymptotisk normalitet. Lad $\boldsymbol{\beta}=\boldsymbol{\beta}^{*} +\frac{\textbf{u}}{\sqrt{n}}$ og
\begin{align*}
\Psi_n(\textbf{u})=\left\Vert \mathbf{y}-\sum_{j=1}^p \textbf{x}_j \left( \beta_j^{*} +\frac{u_j}{\sqrt{n}} \right) \right\Vert_2^2 + \lambda_n \sum_{j=1}^p \widehat{w}_j \left\vert \beta_j^{*} + \frac{u_j}{\sqrt{n}} \right\vert.
\end{align*}
Lad $\widehat{\textbf{u}}^{(n)}=\argmin \Psi_n(\textbf{u})$, da er $\widehat{\boldsymbol{\beta}}^{{\text{AL}}}=\boldsymbol{\beta}^{*} + \frac{\widehat{\boldsymbol{u}}^{(n)}}{\sqrt{n}}$ eller $\widehat{\boldsymbol{u}}^{(n)}=\sqrt{n}\left(\widehat{\boldsymbol{\beta}}^{\text{AL}}-\boldsymbol{\beta}^{*}\right)$.
Lad $V(\mathbf{u})^{(n)}=\Psi_n(\textbf{u}) - \Psi_n(\textbf{0})$, da gælder, at
\begin{align*}
V(\mathbf{u})^{(n)}= \left\Vert \textbf{y} - \sum_{j=1}^p \textbf{x}_j \left( \beta_j^{*} + \frac{u_j}{\sqrt{n}} \right) \right\Vert_2^2 +
\lambda_n \sum_{j=1}^p \widehat{w}_j \left\vert \beta_j^{*} + \frac{u_j}{\sqrt{n}} \right\vert 
-
\left\Vert \textbf{y} - \sum_{j=1}^p \textbf{x}_j \beta_j^{*} \right\Vert_2^2 - \lambda_n \sum_{j=1}^p \widehat{w}_j \left\vert \beta_j^{*} \right\vert.
\end{align*}
Vi opdeler ovenstående ligning.
Vi betragter først leddene hvor strafparametrene indgår
\begin{align*}
\lambda_n \sum_{j=1}^p \widehat{w}_j \left\vert \beta_j^{*} + \frac{u_j}{\sqrt{n}} \right\vert- \lambda_n \sum_{j=1}^p \widehat{w}_j \left\vert \beta_j^{*} \right\vert 
= \lambda_n \sum_{j=1}^p \widehat{w}_j \left( \left\vert \beta_j^{*} + \frac{u_j}{\sqrt{n}} \right\vert - \left\vert \beta_j^{*} \right\vert
\right).
\end{align*}
Vi ser herefter på de to resterende led
\begin{align*}
\left\Vert \textbf{y} - \sum_{j=1}^p \textbf{x}_j \left( \beta_j^{*} + \frac{u_j}{\sqrt{n}} \right) \right\Vert_2^2 -\left\Vert \textbf{y} - \sum_{j=1}^p \textbf{x}_j \beta_j^{*} \right\Vert_2^2,
\end{align*}
som kan skrives på matrix-vektor form
\begin{align*}
\left\Vert \textbf{y}-\textbf{X}\boldsymbol{\beta}^{*} -\frac{\textbf{X}\textbf{u}}{\sqrt{n}} \right\Vert_2^2 - \left\Vert \textbf{y} - \textbf{X} \boldsymbol{\beta}^{*} \right\Vert_2^2  & =
\left\Vert \boldsymbol{\epsilon} - \frac{\textbf{X}\textbf{u}}{\sqrt{n}} \right\Vert_2^2 - \left\Vert \boldsymbol{\epsilon} \right\Vert_2^2  \\
&= \del{\boldsymbol{\epsilon} - \frac{\textbf{X}\textbf{u}}{\sqrt{n}}}^T \del{\boldsymbol{\epsilon} - \frac{\textbf{X}\textbf{u}}{\sqrt{n}}} - \boldsymbol{\epsilon}^T \boldsymbol{\epsilon} \\
& = \frac{\textbf{u}^T (\textbf{X}^T\textbf{X})  \textbf{u}}{n} - 2 \boldsymbol{\epsilon}^T \left( \frac{\mathbf{X}\mathbf{u}}{\sqrt{n}} \right) \\ 
&= \textbf{u}^T \left(\frac{1}{n}\textbf{X}^T\textbf{X}\right)  \textbf{u}- 2 \frac{\boldsymbol{\epsilon}^T \textbf{X}}{\sqrt{n}}\textbf{u}.
\end{align*}
Vi får så, at 
\begin{align}
V(\mathbf{u})^{(n)} & = \textbf{u}^T \left(\frac{1}{n}\textbf{X}^T\textbf{X}\right)  \textbf{u} - 2 \frac{\boldsymbol{\epsilon}^T \textbf{X}}{\sqrt{n}}\textbf{u} + \lambda_n \sum_{j=1}^p \widehat{w}_j \left( \left\vert \beta_j^{*} + \frac{u_j}{\sqrt{n}} \right\vert - \left\vert \beta_j^{*}\right\vert
\right) \nonumber \\
 & = \textbf{u}^T \left(\frac{1}{n}\textbf{X}^T\textbf{X}\right)  \textbf{u} - 2 \frac{\boldsymbol{\epsilon}^T \textbf{X}}{\sqrt{n}}\textbf{u} +\frac{\lambda_n}{\sqrt{n}} \sum_{j=1}^p \widehat{w}_j \sqrt{n} \left( \left\vert \beta_j^{*} + \frac{u_j}{\sqrt{n}} \right\vert - \left\vert \beta_j^{*} \right\vert
\right). \label{eq:V_4}
\end{align}
%
For første led i \eqref{eq:V_4} gælder der, at $\frac{1}{n} \mathbf{X}^T \mathbf{X} \overset{p}{\rightarrow} \mathbf{C}$ og for andet led har vi, at $\frac{\boldsymbol{\epsilon}^T \mathbf{X}}{\sqrt{n}} \overset{d}{\rightarrow} \textbf{W}=N(\textbf{0},\sigma^2 \boldsymbol{C})$. 
Lad os betragte sidste led i \eqref{eq:V_4}. \\
Hvis $\beta_j^{*} \neq 0$, da har vi, at $\widehat{w}_j \overset{p}{\rightarrow} \left\vert \beta_j^{*} \right\vert^{-\gamma}$. Yderligere har vi, at 
\begin{align*}
\lim_{n\rightarrow \infty}
\frac{\left\vert \beta_j^{*} +\frac{u_j}{\sqrt{n}} \right\vert - \left\vert \beta_j^{*} \right\vert}{\frac{u_j}{\sqrt{n}}} =\frac{\partial}{\partial \beta_j^{*}} \left\vert \beta_j^{*} \right\vert =\text{sign}\left(\beta_j^{*} \right),
\end{align*} 
hvoraf der gælder, at $\lim_{n\rightarrow \infty} \sqrt{n} \left( \left\vert \beta_j^{*} +\frac{u_j}{\sqrt{n}} \right\vert - \left\vert \beta_j^{*} \right\vert \right) = u_j \text{sign}\left(\beta_j^{*} \right)$.
Af Slutskys sætning \ref{thm:slutsky} har vi, at 
\begin{align*}
\frac{ \lambda_n}{\sqrt{n}} \widehat{w}_j \sqrt{n} \left(\left\vert \beta_j^{*} +\frac{u_j}{\sqrt{n}} \right\vert - \left\vert \beta_j^{*} \right\vert \right) \overset{p}{\rightarrow} 0.
\end{align*}
Hvis $\beta_j^{*} = 0$, da har vi $\sqrt{n} \left( \left\vert \beta_j^{*} +\frac{u_j}{\sqrt{n}} \right\vert - \left\vert \beta_j^{*} \right\vert \right) = \left\vert u_j \right\vert$.
Vægtene omskrives til
\begin{align*}
\widehat{w}_j= \left( \frac{1}{\left\vert \widehat{\beta}_j \right\vert} \right)^\gamma=\left( \frac{\sqrt{n}}{\sqrt{n} \left\vert \widehat{\beta}_j \right\vert} \right)^\gamma = \frac{n^{\gamma/2}}{ \left( \sqrt{n} \left\vert \widehat{\beta}_j \right\vert \right)^\gamma},
\end{align*} 
hvor $\gamma >0$ og $\widehat{\beta}_j$ er rod-$n$-konsistent. Heraf har vi, at $\frac{\lambda_n}{\sqrt{n}} \widehat{w}_j = \frac{\lambda_n}{\sqrt{n}} \frac{n^{\gamma/2}}{\vert \sqrt{n} \widehat{\beta}_j \vert^\gamma} = \lambda_n n^{\frac{\gamma -1}{2}} \frac{1}{\vert \sqrt{n} \widehat{\beta}_j \vert^\gamma} $, hvor $\sqrt{n} \widehat{\beta}_j = O_p(1)$ og vi ved at  $\lambda_n n^\frac{\gamma-1}{2} \rightarrow \infty$, da har vi, at $\frac{\lambda_n}{\sqrt{n}} \widehat{w}_j  \vert u_j \vert \rightarrow \infty$.
Af Slutsky sætning ser vi, at $V^{(n)} (\mathbf{u}) \overset{d}{\rightarrow} V(\textbf{u})$ for alle $\mathbf{u}$, hvor
\begin{align*}
V(\textbf{u}) = \begin{cases}
    \mathbf{u}_\mathcal{A}^T \mathbf{C}_{11} \mathbf{u}_\mathcal{A}-2\mathbf{u}^T_\mathcal{A} \mathbf{W}_\mathcal{A} & \text{hvis  $u_j=0, \ \forall j \notin \mathcal{A} $},\\
    \infty & \text{hvis } \exists u_j \neq 0, \ j \notin \mathcal{A} .
  \end{cases}
\end{align*}
Da funktionen $V^{(n)}$ er konveks, og $(\mathbf{C}_{11}^{-1} \mathbf{W_\mathcal{A}},0)^T$ er et entydig minimum af $V$, følger det af \citep{adaptive_lasso} at 
$\arg\min V^{(n)} \rightarrow \arg\min V$.
Derfor får vi
\begin{align}
\widehat{\mathbf{u}}_\mathcal{A}^{(n)} \overset{d}{\rightarrow} \mathbf{C}_{11}^{-1} \mathbf{W}_\mathcal{A} \quad \text{og} \quad \widehat{\mathbf{u}}_{\mathcal{A}^C}^{(n)} \overset{d}{\rightarrow} \mathbf{0}. \label{eq:minUA}
\end{align}
Vi observerer, at $\mathbf{W}_\mathcal{A}=N(\mathbf{0}, \sigma^2 \mathbf{C}_{11})$, og da har vi bevist asymptotisk normalitet. \\

Herefter vil vi bevise, at variabeludvælgelsen er konsistent. For alle $j \in \mathcal{A}$ giver den asymptotiske normalitet, at $\widehat{\beta}_j^{\text{AL}} \overset{p}{\rightarrow}\beta_j^{*}$, dvs $P \del{j \in \mathcal{A}_n^{\text{AL}}} \rightarrow 1$. Derfor er det tilstrækkeligt at vise, at for alle $j' \notin \mathcal{A}$, da vil $P \del{j' \in \mathcal{A}_n^{\text{AL}}} \rightarrow 0$. \\
Vi betragter $j' \in \mathcal{A}_n^{\text{AL}}$, således at $\widehat{\beta}_{j'}^{\text{AL}} \neq 0$. Af KKT betingelserne har vi, at 
\begin{align*}
2 \mathbf{x}_{j'}^T  \left( \mathbf{y}-\mathbf{X}\widehat{\boldsymbol{\beta}}^{\text{AL}} \right)=\lambda_n \widehat{w}_{j'} \left\vert \text{sign} \del{\widehat{\beta}_{j'}^\text{AL}} \right\vert,
\end{align*}
som er ækvivalent med
\begin{align*}
2 \frac{\mathbf{x}_{j'}^T \left( \mathbf{y}-\mathbf{X}\widehat{\boldsymbol{\beta}}^{{\text{AL}}}\right)}{\sqrt{n}}=\frac{\lambda_n}{\sqrt{n}} \widehat{w}_{j'}.
\end{align*}
Vi fandt, at $\frac{\lambda_n}{\sqrt{n}} \widehat{w}_{j'} \overset{p}{\rightarrow} \infty$. Vi har da, at 
\begin{align*}
2 \frac{\mathbf{x}_{j'}^T \left(\mathbf{y}-\mathbf{X}\widehat{\boldsymbol{\beta}}^{{\text{AL}}} \right)}{\sqrt{n}}
 &= 2 \frac{\mathbf{x}_{j'}^T \left(\mathbf{X}\boldsymbol{\beta}^*+\boldsymbol{\epsilon}-\mathbf{X}\widehat{\boldsymbol{\beta}}^{{\text{AL}}} \right) }{\sqrt{n}} \\
&= 2 \frac{\mathbf{x}_{j'}^T \mathbf{X} \left(\boldsymbol{\beta}^*-\widehat{\boldsymbol{\beta}}^{\text{AL}} \right)}{\sqrt{n}}+2\frac{\mathbf{x}_{j'}^T \boldsymbol{\epsilon}}{\sqrt{n}} \\
&= 2 \frac{\mathbf{x}_{j'}^T \mathbf{X} \sqrt{n} \left(\boldsymbol{\beta}^*-\widehat{\boldsymbol{\beta}}^{\text{AL}}\right)}{n}+2\frac{\mathbf{x}_{j'}^T \boldsymbol{\epsilon}}{\sqrt{n}}.
\end{align*}
Af \eqref{eq:minUA} og Slutskys sætning \ref{thm:slutsky}, ved vi at $ 2 \frac{\mathbf{x}_{j'}^T \mathbf{X} \sqrt{n} \left(\boldsymbol{\beta}^*-\widehat{\boldsymbol{\beta}}^{\text{AL}}\right)}{n}$ konvergerer i fordeling mod en normalfordeling og $2\frac{\mathbf{x}_{j'}^T \boldsymbol{\epsilon}}{\sqrt{n}} \overset{d}{\rightarrow} N \left(\mathbf{0}, 4 \Vert \mathbf{x}_{j'} \Vert^2 \sigma^2 \right)$. Dermed har vi, at
\begin{align*}
P\left(j' \in \mathcal{A}_n^{\text{AL}}\right) \leq P\left(2 \mathbf{x}_{j'}^T \left(\mathbf{y}-\mathbf{X} \widehat{\boldsymbol{\beta}}^{\text{AL}}\right)=\lambda_n \widehat{w}_{j'} \right) \rightarrow 0.
\end{align*}
\end{proof}
%
%Vægtene \(\widehat{\mathbf{w}}\), som afhænger af data, er nøglen bag sætning \ref{thm:ALoracle}.
%Når stikprøve størrelsen stiger, da øges vægtene for nul koefficient prædiktorerne til uendelig, mens vægtene for ikke-nul koefficient prædiktorerne konvergerer til en endelig konstant.

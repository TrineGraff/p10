\section{Konsistens af lasso estimatoren}
Vi betragter en lineær regression model 
\begin{align*}
y_i= \mathbf{x}_i \tbeta^* + \epsilon_i, \quad i = 1, \ldots, n,
\end{align*}
hvor \(\epsilon_i \sim \text{iid} \del{0, \sigma^2}\).
Følgende antagelser vil blive anvendt igennem afsnittet.
\begin{ass} \label{ass:konsistens}
\begin{enumerate}[label=\alph*)]
%\item Designmatricen $\textbf{X}$ har fuld rang.
\item Stabilitet af prædiktorer:
$\frac{1}{n} \textbf{X}^T \textbf{X} \overset{p}{\rightarrow} E[\textbf{X}^T \textbf{X}] = \textbf{C}$, hvor $\textbf{C}$ eksisterer og er en positiv definit matrix.
%\item Ortogonalitets betingelse:
%$\frac{1}{n} \textbf{X}^T \boldsymbol{\epsilon} \overset{p}{\rightarrow} E[\textbf{X}^T \boldsymbol{\epsilon}] = \textbf{0} $.
\item $\frac{1}{\sqrt{n}} \mathbf{X}^T \boldsymbol{\epsilon} \overset{d}{\rightarrow} \mathbf{W}=N(\mathbf{0},\sigma^2 \mathbf{C})$.
\end{enumerate}
\end{ass}
%Bemærk, at antagelse \ref{ass:konsistens}.e) er indeholdt i antagelse \ref{ass:konsistens}.f). \\
%Antagelse \ref{ass:konsistens}.d) og \ref{ass:konsistens}.e) er en anvendelse af store tals lov, som siger at den empiriske middelværdi konvergerer i sandsynlighed mod den sande middelværdi, når $n \rightarrow \infty$. Antagelse \ref{ass:konsistens}.f) er en anvendelse af den centrale grænseværdi sætning, som løst sagt siger, at en stokastisk variabel under givne betingelser konvergerer i fordeling mod en normalfordeling, når $n \rightarrow \infty$. 
\subsection{OLS estimatoren}
I dette afsnit introduceres den asymptotiske fordeling af OLS estimatoren.
Som bekendt er OLS estimatoren svagt konsistent.
\begin{thm}[Asymptotisk fordeling af OLS estimatoren] \label{thm:asymp_ols}
Under antagelse \ref{ass:konsistens}.a) og \ref{ass:konsistens}.b) er den asymptotiske fordeling af $\boldsymbol{\widehat{\beta}}^{\text{OLS}}$ givet ved
\begin{align*}
\sqrt{n}(\widehat{\boldsymbol{\beta}}^{\text{OLS}}-\boldsymbol{\beta}) \overset{d}{\rightarrow} N(\mathbf{0},\sigma^2 \mathbf{C}^{-1}).
\end{align*}
\end{thm}
\begin{proof}
Lad os omskrive OLS estimatoren
\begin{align*}
\widehat{\boldsymbol{\beta}}^{\text{OLS}}  &= (\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{y} \\
&= (\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T (\textbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}) \\
&= \boldsymbol{\beta} + (\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T \boldsymbol{\epsilon},
\end{align*}
som yderligere kan omskrives til
\begin{align}
\widehat{\boldsymbol{\beta}}^{\text{OLS}}  &=\boldsymbol{\beta} + \left( \frac{1}{n} \mathbf{X}^T \mathbf{X} \right)^{-1} \left( \frac{1}{n} \mathbf{X}^T \boldsymbol{\epsilon} \right) \nonumber \\
\sqrt{n}(\hat{\boldsymbol{\beta}}^{\text{OLS}} -\boldsymbol{\beta})&=\left( \frac{1}{n} \mathbf{X}^T \mathbf{X} \right)^{-1} \left( \frac{1}{\sqrt{n}} \mathbf{X}^T \boldsymbol{\epsilon} \right). \label{eq:OLSasymp}
\end{align}
Af antagelse \ref{ass:konsistens}.a) ved vi, at første del i \eqref{eq:OLSasymp} konvergerer i sandsynlighed mod $\mathbf{C}^{-1}$, mens antagelse \ref{ass:konsistens}.b) giver, at anden del konvergerer i fordeling mod $N(\mathbf{0},\sigma^2 \mathbf{C})$. Af Slutskys sætning \ref{thm:slutsky} finder vi, at den asymptotiske fordeling for OLS estimatoren er
\begin{align*}
\sqrt{n}(\hat{\boldsymbol{\beta}}^{\text{OLS}}-\boldsymbol{\beta}) \overset{d}{\rightarrow} N(\mathbf{0},\sigma^2 \mathbf{C}^{-1} \mathbf{C} \mathbf{C}^{-1})=N(\mathbf{0},\sigma^2 \mathbf{C}^{-1}).
\end{align*}
Heraf ses det at OLS estimatoren er \(\sqrt{n}\)-konsistent.
\end{proof}
%
\subsection{Lasso estimatoren}
I dette afsnit vil vi betragte de asymptotiske egenskaber for lasso estimatoren.
Afsnittet er baseret på \citep{adaptive_lasso}.

For en estimator, som udfører variabeludvælgelse, findes nogle såkaldte \textit{orakelegenskaber}.
En estimator, som opfylder disse egenskaber, er konsistent i variabeludvælgelse og de estimerede koefficienter følger asymptotisk en normalfordeling.
Nedenfor defineres orakelegenskaberne.

\subsubsection{Orakelegenskaberne} 
Lad $\mathcal{A} =\cbr{j:\beta_j^* \neq 0}$, hvor $\beta_j^*$ betegner koefficienten af $\mathbf{x}_j$ i den sande model og antag at $\vert \mathcal{A} \vert=p_0 <p$, således at den sande model afhænger af en delmængde af prædiktorerne. 
Så defineres orakelegenskaberne som følgende. 
\begin{defn}[Orakelegenskaber]
\begin{itemize}
\item Variabeludvælgelsen er konsistent, dvs for
\begin{align*}
\mathcal{A}_n=\lbrace j :\widehat{\beta}_j \neq 0 \rbrace \ \text{og} \ \mathcal{A} =\{j:\beta_j^* \neq 0\},
\end{align*}
gælder der, at $\lim_{n \rightarrow \infty }P(\mathcal{A}_n=\mathcal{A})=1$.
\item Estimatoren er asymptotisk normalfordelt, dvs
\begin{align*}
\sqrt{n}(\widehat{\boldsymbol{\beta}}_\mathcal{A}-\boldsymbol{\beta}^*_\mathcal{A}) \overset{d}{\rightarrow} N(\mathbf{0}, \boldsymbol{\Sigma}^*_I),
\end{align*}
hvor $\boldsymbol{\beta}^*_\mathcal{A}=\{ \beta_j^*, j \in \mathcal{A} \}$ og $\boldsymbol{\Sigma}^*_I$ er kovariansmatricen, hvor vi antager, at vi kender den sande model.
\end{itemize}
\end{defn}
En god procedure bør have disse orakelegenskaber.
Dog bør proceduren have nogle ekstra betingelser udover orakelegenskaberne for at være optimal, såsom kontinuert shrinkage.
Derfor er det vigtigt at understrege, at orakelegenskaber ikke alene resulterer i en optimal procedure.

Lad os betragte lasso estimatoren
\begin{align*}
\widehat{\tbeta}^\text{lasso} = \argmin_{\tbeta} \Vert \y - \sum_{j=1}^p \x_j \beta_j \Vert_2^2 + \lambda_n \sum_{j=1}^p \vert \beta_j \vert,
\end{align*}
hvor \(\lambda_n\) varierer med \(n\).
Lad \(\A_n^\text{lasso} = \cbr{j : \widehat{\beta}_j^\text{lasso} \neq 0}\), da er lasso variabeludvælgelse konsistent hvis og kun hvis \(\lim_{n \rightarrow \infty} P \del{\A_n^\text{lasso} - \A} = 1\).

\begin{lem}\label{lem:lasso_consistency1}
Hvis $\frac{\lambda_n}{\sqrt{n}} \rightarrow \lambda_0 \geq 0$, da vil
\begin{align*}
\widehat{\tbeta}^\text{lasso} \overset{p}{\rightarrow} \argmin \cbr{ \del{\mathbf{u} - \tbeta^*}^T \mathbf{C} \del{\mathbf{u} - \tbeta^*} + \lambda_0 \sum_{j=1}^p \vert u_j \vert}.
\end{align*}
\end{lem}
\begin{proof}
Beviset undlades, men der henvises til s. 1358 i \citep{adaptive_lasso_knight}.
\end{proof}
%
\begin{thm}[Asymptotisk fordeling af lasso estimatoren] \label{thm:asymp_lasso}
Hvis $\frac{\lambda_n}{\sqrt{n}} \rightarrow \lambda_0 \geq 0$, da er den asymptotiske fordeling af \(\widehat{\tbeta}^\text{lasso}\) givet ved
\begin{align*}
\sqrt{n} \del{\widehat{\tbeta}^\text{lasso} - \tbeta^*} \overset{d}{\rightarrow} \argmin \cbr{ -2 \mathbf{u}^T \mathbf{W} + \mathbf{u}^T \mathbf{C} \mathbf{u} + \lambda_0 \sum_{j=1}^p \sbr{u_j \text{sign} \del{\beta^*_j} \mathbb{1} \del{\beta_j^* \neq 0} + \vert u_j \vert \mathbb{1} \del{\beta_j^* = 0}}},
\end{align*}
hvor \(\mathbf{W} = N\del{\mathbf{0}, \sigma^2 \mathbf{C}}\).
\end{thm}
\begin{proof}
Beviset undlades, men der henvises til s. 1359 i \citep{adaptive_lasso_knight}.
\end{proof}
Dette beviser, at lasso estimatoren er \(\sqrt{n}\)-konsistent.

Hvis $\lambda_0=0$, da gælder ifølge lemma \ref{lem:lasso_consistency1}, at $\widehat{\boldsymbol{\beta}}^\text{lasso} \overset{p}{\rightarrow} \boldsymbol{\beta}^{*}$, da strafleddet forsvinder og $\mathbf{C}$ er en positiv definit matrix, og dermed er $\widehat{\boldsymbol{\beta}}^\text{lasso}$ svagt konsistent. Men da strafleddet forsvinder, medfører det også, at lasso asymptotisk ingen variabeludvælgelse har. Hvis $\lambda_0>0$ kan det ikke udledes fra lemma \ref{lem:lasso_consistency1}, om estimatoren er konsistent. Det angives i \citep{adaptive_lasso}, at dette kun vil være tilfældet, når en given betingelse er opfyldt. 

%\begin{thm}[Nødvendig betingelse]
%Antag at \(\lim_{n \rightarrow \infty} P \del{\A_n^\text{lasso} = \A}=1\), da eksisterer en fortegnsvektor \(\mathbf{s} = \del{s_1, \ldots, s_{p_0}}\), hvor \(s_j\) er lig \(1\) eller \(-1\), således at
%\begin{align}
%\left\vert \mathbf{C}_{21} \mathbf{C}_{11}^{-1} \mathbf{s} \right\vert \leq 1. \label{eq:betingelse_konsistent}
%\end{align}
%\end{thm}
%%
%\begin{proof}
%Beviset undlades, men vi referer til s. 1426 i \citep{adaptive_lasso}.
%\end{proof}
%%
%Hvis betingelse \eqref{eq:betingelse_konsistent} ikke er opfyldt, da er lasso variabeludvælgelsen ikke konsistent.
%Den nødvendige betingelse i \eqref{eq:betingelse_konsistent} er ikke triviel.
%
%\begin{cor}
%Antag \(p_0 = 2m+1 \geq 3\) og \(p=p_0+1\), således at én prædiktor er irrelevant.
%Lad \(\mathbf{C}_{11} = \del{1- \rho_1} \mathbf{I} + \rho_1 \mathbf{J}_1\), hvor \(\mathbf{J}_1\) er en matrix bestående af 1-taller og  \(\mathbf{C}_{12} =  \rho_2 \mathbf{1}\) og \(\mathbf{C}_{22}= 1\). Hvis \(-\frac{1}{p_0 - 1} < \rho_1 < -\frac{1}{p_0}\) og \(1 + \del{p_0 -1} \rho_1 < \vert \rho_2 \vert < \sqrt{\frac{1 + \del{p_0-1}\rho_1}{p_0}}\), da kan betingelse \eqref{eq:betingelse_konsistent} ikke være opfyldt.
%Derfor er lasso variabeludvælgelsen ikke konsistens.
%\end{cor}
%
%Hvis modelmatricen \(\X\) er ortogonal, da er den nødvendige betingelse \eqref{eq:betingelse_konsistent} og konsistens af lasso udvælgelsen garanteret.
%Derudover hvis \(p=2\), er den nødvendige betingelse altid opfyldt, da \(\left\vert \mathbf{C}_{21} \mathbf{C}_{11}^{-1} \text{sign} \del{\beta^*_\A} \right\vert\) reduceres til \(\vert \rho \vert\), som er korrelationen imellem prædiktorerne.
%
%Vi har vist, at lasso ikke kan opfylde orakelegenskaberne.
En metode, der konsekvent udfører konsistent variabeludvælgelse, kan dog opnås med en simpel tilføjelse til lasso estimatet. Den metode kaldes \textit{adaptive lasso} og præsenteres i afsnit \ref{ch:generalisering_lasso}.

%Lad os antage
%\begin{align*}
%y_i = \x_i \tbeta^* + \epsilon_i, \quad \epsilon_i \sim \text{idd} \del{0, \sigma^2}
%\end{align*}
%for \(i=1, \ldots, n\).
%Derudover antages det, at \(\frac{1}{n} \X^T \X \rightarrow \textbf{C}\), hvor
%\begin{align*}
%\textbf{C} = 
%\begin{bmatrix}
%\textbf{C}_{11}& \textbf{C}_{12}\\
%\textbf{C}_{21}& \textbf{C}_{22}
%\end{bmatrix},
%\end{align*}
%er en positiv definit matrix, hvor $\textbf{C}_{11}$ er en $p_0 \times p_0$ matrix. 
%Lad os betragte lasso estimatoren
%\begin{align*}
%\hat{\tbeta}^\text{lasso} = \argmin_{\tbeta} \Vert \y - \sum_{j=1}^p \x_j \beta_j \Vert_2^2 + \lambda_n \sum_{j=1}^p \vert \beta_j \vert,
%\end{align*}
%hvor \(\lambda_n\) varierer med \(n\).
%Lad \(\A_n^\text{lasso} = \cbr{j : \hat{\beta}_j^\text{lasso} \neq 0}\), da er lasso variabeludvælgelse konsistens hvis og kun hvis \(\lim_{n \rightarrow \infty} P \del{\A_n^\text{lasso} - \A} = 1\).

%\begin{lem}\label{lem:lasso_consistency1}
%Hvis $\frac{\lambda_n}{\sqrt{n}} \rightarrow \lambda_0 \geq 0$, da vil $\hat{\tbeta}^\text{lasso} \overset{p}{\rightarrow} \argmin V_1$, hvor
%\begin{align*}
%V_1 \del{\mathbf{u}} = \del{\mathbf{u} - \tbeta^*}^T \mathbf{C} \del{\mathbf{u} - \tbeta^*} + \lambda_0 \sum_{j=1}^p \vert u_j \vert.
%\end{align*}
%\end{lem}
%\begin{proof}
%Beviset undlades, men der henvises til s. 1358 i \citep{adaptive_lasso_knight}.
%\end{proof}
%%
%\begin{lem}\label{lem:lasso_consistency2}
%Hvis $\frac{\lambda_n}{\sqrt{n}} \rightarrow \lambda_0 \geq 0$, da vil \(\sqrt{n} \del{\hat{\tbeta}^\text{lasso} - \tbeta^*} \overset{d}{\rightarrow} \argmin V_2\), hvor
%\begin{align*}
%V_2 \del{\mathbf{u}} = -2 \mathbf{u}^T \mathbf{W} + \mathbf{u}^T \mathbf{C} \mathbf{u} + \lambda_0 \sum_{j=1}^p \sbr{u_j \text{sign} \del{\beta^*_j} \mathbb{1} \del{\beta_j^* \neq 0} + \vert u_j \vert \mathbb{1} \del{\beta_j^* = 0}},
%\end{align*}
%og \(\mathbf{W} \sim N\del{\mathbf{0}, \sigma^2 \mathbf{C}}\).
%\end{lem}
%%
%Af lemma \ref{lem:lasso_consistency2} har vi, at lasso estimatet er rod-n konsistent.
%
%Hvis $\lambda_0=0$, da gælder ifølge lemma \ref{lem:lasso_consistency1}, at $\hat{\boldsymbol{\beta}}^\text{lasso} \overset{p}{\rightarrow} \boldsymbol{\beta}^{*}$, da strafleddet forsvinder og $\mathbf{C}$ er en positiv definit matrix, og dermed er $\hat{\boldsymbol{\beta}}^\text{lasso}$ svagt konsistent. Men da strafleddet forsvinder, medfører det også, at lasso asymptotisk ingen variabeludvælgelse har. Hvis $\lambda_0>0$ kan det ikke udledes fra lemma \ref{lem:lasso_consistency1}, om estimatoren er konsistent. Det angives i \citep{adaptive_lasso}, at dette kun vil være tilfældet, når en given betingelse er opfyldt. 
%
%En metode, der konsekvent udfører konsistent variabeludvælgelse, kan dog opnås med en simpel tilføjelse til lasso estimatet. Denne metode kaldes adaptive lasso.
%
%\begin{thm}[Nødvendig betingelse]
%Antag at \(\lim_{n \rightarrow \infty} P \del{\A_n^\text{lasso} = \A}=1\), da eksisterer en fortegnsvektor \(\mathbf{s} = \del{s_1, \ldots, s_{p_0}}\), hvor \(s_j\) er lig \(1\) eller \(-1\), således at
%\begin{align}
%\left\vert \mathbf{C}_{21} \mathbf{C}_{11}^{-1} \mathbf{s} \right\vert \leq 1. \label{eq:betingelse_konsistent}
%\end{align}
%\end{thm}
%%
%\begin{proof}
%Beviset undlades, men vi referer til s. 1426 i \citep{adaptive_lasso}.
%\end{proof}
%%
%Hvis betingelse \eqref{eq:betingelse_konsistent} ikke er opfyldt, da er lasso variabeludvælgelsen ikke konsistent.
%Den nødvendige betingelse i \eqref{eq:betingelse_konsistent} er ikke triviel.
%
%\begin{cor}
%Antag \(p_0 = 2m+1 \geq 3\) og \(p=p_0+1\), således at én prædiktor er irrelevant.
%Lad \(\mathbf{C}_{11} = \del{1- \rho_1} \mathbf{I} + \rho_1 \mathbf{J}_1\), hvor \(\mathbf{J}_1\) er en matrix bestående af 1-taller og  \(\mathbf{C}_{12} =  \rho_2 \mathbf{1}\) og \(\mathbf{C}_{22}= 1\). Hvis \(-\frac{1}{p_0 - 1} < \rho_1 < -\frac{1}{p_0}\) og \(1 + \del{p_0 -1} \rho_1 < \vert \rho_2 \vert < \sqrt{\frac{1 + \del{p_0-1}\rho_1}{p_0}}\), da kan betingelse \eqref{eq:betingelse_konsistent} ikke være opfyldt.
%Derfor er lasso variabeludvælgelsen ikke konsistens.
%\end{cor}
%
%Hvis modelmatricen \(\X\) er ortogonal, da er den nødvendige betingelse \eqref{eq:betingelse_konsistent} og konsistens af lasso udvælgelsen garanteret.
%Derudover hvis \(p=2\), er den nødvendige betingelse altid opfyldt, da \(\left\vert \mathbf{C}_{21} \mathbf{C}_{11}^{-1} \text{sign} \del{\beta^*_\A} \right\vert\) reduceres til \(\vert \rho \vert\), som er korrelationen imellem prædiktorerne.
%
%Vi har vist, at lasso ikke kan opfylde orakelegenskaberne.

\section{Coordinate descent}
Vi anvender funktionen \texttt{glmnet} fra R-pakken af samme navn til at estimerer lasso, elastik net, ridge og adaptive lasso's koefficienter i vores modeller. 
Funktionen genererer ud fra datasættet en følge på 100 $\lambda$-værdier og tilpasser en model til hver af disse ved maksimum likelihood estimation med algoritmen coordinate descent. 
Ud fra dette anvender vi så 10-fold-krydsvalidering og BIC til at vælge $\widehat{\lambda}$, som giver den bedste model. 
Det skal lige bemærkes, at for elastisk net har vi to turning parameter vi skal estimerer nemlig $\alpha$ og $\lambda$.  Så her har vi valgt 10 værdier af $\alpha$, hvor $\alpha \in [0,1]$. 
For group lasso har vi anvendt \texttt{gglasso} fra R-pakken også med samme navn til at estimerer group lasso. 
Denne funktion generer også en følge på 100 $\lambda$-værdier men anvender i stedet algoritmen black-wise descent.  Funktionen kræver også en gruppering af de forklarende variabler. 
Vi anvender grupperne som er forslået af Michael McCracken, som ses i appendiks \ref{app:app_data}. Derudover har kvadratroden af gruppens størrelse som penality faktor. 
For adaptive lasso med lasso vægte, anvender vi kun de forklarende variable, som lasso har udvalgt til at estimerer turning parameteren $\lambda$. 

\subsection{Krydsvalidering}
I denne sektion anvender vi funktionen \texttt{cv.glmnet} og \texttt{cv.gglasso} i pakkerne \texttt{glmnet} og \texttt{gglasso} . 
Tidligere nævnt har elastik net to turning parameter. 
Vi har derfor anvendt 10-fold krydsvalidering for 10 værdier af $\alpha$. 
Derfra har vi fundet $\lambda_{\min}$ og krydsvaliderings fejlen for hver værdi af $\alpha$.  
Den mindste krydsvaliderings fejl for $\lambda_{\min}$ er når $\alpha =1$, som er den samme model som lasso. 
Vi inkluderer derfor ikke elastik net.   

\imgfigh{cv_plot.pdf}{1}{10-fold krydsvaliderings fejl plottede som en function af $ \log(\lambda)$ for vores metoder. De stiplede linjer indikerer minimum fejl, samt fejlen med en standard afvigelse af minimum}{cv_plot}
Figur \ref{fig:cv_plot} illustrer den gennemsnitlige krydsvaliderings fejl for hver værdi af $\log \lambda$ for hver af vores metoder.

\input{fig/tab/cv_tab}

For at få et bedre overblik viser tabel  \ref{tab:cv_tab} værdierne af $\log \lambda_{\min}$ og $\log \lambda_{1\text{sd}}$ , antallet af koefficienter og deres krydsvaliderings fejl.  
Vi ser for lasso,  at der sker en reducering af antallet af parameter når $\lambda_{1\text{sd}}$ anvendes i forhold til $\lambda_{\min}$, men MSE ikke er signifikant forskellig.  
Vi anvender derfor $\widehat{\lambda}_{1\text{sd}}$, som vores turning parameter. 
For ridge regression vil der ikke ske reducering af antallet af parameter, men som nævnt tidligere en reducering af værdierne af koefficienterne. 
Vi lader derfor $\widehat{\lambda}_{\min}$ være vores optimale turning parameter, da den har mindst krydsvaliderings fejl.
%
For group lasso kan vi se, at $\lambda_{1\text{sd}}$ ikke reducerer antallet af parameter meget,da den kun sætter 7 variabler til nul. 
Disse variabler stammer alle fra gruppe 5.
Det indikerer, at group lasso måske ikke er den bedste selektions model til vores data. 
Vi lader $\widehat{\lambda}_{1\text{sd}}$ være den optimale $\lambda$ for group lasso, da den stadig har færrest parameter. 

Adaptive lasso m. OLS vægte og adaptive lasso med lasso vægte vælger de færreste antal forklarende variabler. 
Den udvælger kun 2 for både  $\lambda_{\min}$ og $\lambda_{1\text{sd}}$, det er tale om variablerne \textit{CLF16OV: Civilian Labor Force} og \textit{CE16OV: Civilian Employment} begge fra gruppe to. 
Vi lader  $\widehat{\lambda}_{\min}$ være cores turning parameter for både adaptive lasso med OLS vægte og adaptive lasso med lasso vægte. 
Vi har markeret $\widehat{\lambda}$ for hver model med tykt i tabel  \ref{tab:cv_tab}. 

Figur \ref{fig:coef_kryds_coord} viser værdierne af de estimerede koefficienter for lasso og de to adaptive metoder, samt hvilken gruppe de forklarende variablerne tilhører. 
Vi kan se, at lasso hovedsageligt vælger variabler indenfor samme gruppe, som arbejdsløsheden. 
Derudover ses, det tydeligt at \texttt{CLF16OV} og \texttt{CE16OV} har de højest værdier, hvor resten af variablerne er meget tæt på nul. 
Figurerne \ref{fig:coef_ridge_kryds_coord} og \ref{fig:coef_gglasso_kryds_coord} viser det samme for hhv. ridge regression og group lasso.    

\imgfigh{coef_kryds_coord.pdf}{0.6}{Viser de estimerede koefficienters størresle for lasso, adaptive lasso med OLS vægte og adaptive lasso med lasso vægte. Farverne indikerer hvilken gruppe de forklarende variabler tilhører og y-aksen er variablerne udvalgt fra lasso.}{coef_kryds_coord}

For at undersøge tilfældigheden af de estimerede koefficienter anvender vi bootstrap. 
Figurerne \ref{fig:boxplot_lasso_coord_kryds}, \ref{fig:bootstrap_alasso} og \ref{bootstrap_gglasso} viser resultaterne af 1000 bootstrap relisationer for hhv. lasso, adaptive lasso med OLS vægte, adaptive lasso med lasso vægte og gglasso. 
Vi ser at for alle modeller, at variablerne \texttt{CLF16OV} og \texttt{CE16OV} altid bliver valgt til at være forskellige fra nul. 
For lasso ses der at de variable, som ikke er tilhørende af gruppe to ofte bliver valgt til at være nul. 
Derudover ses at variablerne  \texttt{CLF16OV},  \texttt{CE16OV} , \texttt{lag1}, \texttt{UEMPL15OV}, \texttt{UEMP5TO14} og \texttt{UEMPLT5} fra gruppe to ofte estimeret til at være forskellige fra nul, dvs at lasso ofte vælger disse variable. 
%Post-selection intervallerne vises på figur -- for disse 14 variable.
Af figur \ref{fig:fixedLassoInf} observeres at nulhypotesen afvises for \texttt{CLF16OV}, \texttt{CE16OV} samt \texttt{lag 1}.
\input{fig/tab/fixedLassoInf}


Tabel  \ref{tab:adj_r2_shrinkage_tab} viser værdierne af adjusted R$^2$, vi ser at modellerne med de højeste procent er de to adaptive funktioner, hvilket er også dem med færreste estimerede koefficienter. Ridge regression er den med den laveste adjusted R$^2$ men også den med flest koefficienter. 

Figurerne \ref{fig:resid_lasso_coord_kryds} - \ref{fig:resid_adap_ols_coord_kryds} viser en analyse af de standardiserede residualer. 
Vi ser, samme tendens for alle shrinkage metoderne. Histogrammet og QQ-plottet indikerer tungere haler end en normalfordeling og autokorrelation i første lag.
Dette bekræftes i tabel  \ref{tab:res_shrinkage_tab}, som viser skewness, kurtosis og $p$-værdierne fra JB-testen for de standardiserede residualer for alle shrinkage modeller. 
Vi ser, at alle modellerne har en negativ skewness og en kurtosis forskellige fra nul. 
Derudover afvises JB testens nul hypotese omkring normalitet for alle modellerne på nær group lasso, hvor vi kan ikke afvises normalitet af de standardiserede residualer. 
Vi ser også, at group lasso er den model, som har en kurtosis tætteste på nul, samt en meget lille skewness. 






\subsection{BIC}
I dette afsnit finder vi $\widehat{\lambda}$ med BIC. 
Vi bruger funktionen i appendiks \ref{sub:bic} til at finde $\widehat{\lambda}$. 

\input{fig/tab/bic_lambda}
Tabel \ref{tab:bic_lambda} viser $\widehat{\lambda}$ værdi, BIC værdien og antallet af parameter. 
Elastik net vælger $\alpha =1$ til at være den model med mindst BIC, hvilket er den samme model, som lasso. 
Derfor ser vi bort fra elastik net. 
Igen ser vi, at de to adaptive lasso modeller udvælger de færreste variabler, men igen er variablerne Civilian Labor Force og Civilian Employment fra samme gruppe, som responsvariablen. 
Derudover ser vi igen, at group lasso har mange forklarende variabler. 

------ 

Figurerne \ref{fig:resid_lasso_coord_bic} - \ref{fig:resid_adap_ols_coord_bic} viser en an analyse af de standardiserede residualer.
Vi ser igen lidt af den samme tendens for alle metoderne. 
Histogrammet og QQ-plottet indikerer tungere haler end en normalfordeling og autokorrelation i det første lag. 
Dog er ridge og groups lasso QQ-plot kun med få outliers. 
I tabel \ref{tab:res_shrinkage_bic_tab} ser vi også at vi ikke kan afvise LB testen omkring normalitet for group lasso. 


%\input{fig/tab/bic_ud}





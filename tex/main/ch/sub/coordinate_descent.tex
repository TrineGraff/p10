\section{Coordinate descent} \label{sec:theory_coordinatedescent}
%Ideen bag coordinat descent er, at optimere en target funktion mht én parameter mens de resterende parametre fastholdes. 
%Vi gennemløber alle parametre iterativ indtil konvergens.
%Coordinate descent er specielt attraktiv for problemer som lasso, som har en simpel lukket løsning for en dimension, men ikke for flere dimensioner.
%
Coordinate descent er en iterativ algoritme, som opdaterer fra \(\tbeta^t\) til \(\tbeta^{t+1}\) ved at vælge én koordinat som opdateres, og da udføres en univariat minimering over denne koordinat.
Hvis koordinat $k$ er valgt i iteration $t$, da er opdateringen givet ved
\begin{align}
\beta_k^{t+1} =\underset{ \beta_k}{\arg \min}  f\del{ \beta_1^t, \beta_2^t, \dots, \beta_{k-1}^t, \beta_k, \beta_{k+1}^t, \dots, \beta_p^t  }, \label{eq:5.36}
\end{align}
hvor $\beta_j^{t+1} = \beta_j^t$ for $j \neq k$. 
Typisk gennemløbes koordinaterne i en forudbestemt rækkefølge.
Dette kan generaliseres til \textit{block coordinate descent}, som anvendes for group lasso, hvor prædiktorerne er opdelt i ikke-overlappende blocks, og da udføres en minimering over en enkelt block for hvert koordinat.

For at algoritmen konvergerer til det globale minimum af en konveks funktion, skal funktionen være kontinuert differentiabel og strengt konveks i hver koordinat. 
Men som nævnt er strafleddet for lasso ikke differentiabel.

For mange optimeringsproblemer kan objektfunktionen dekomponeres
\begin{align}
f(\beta_1, \dots, \beta_p) = g(\beta_1, \dots, \beta_p) + \sum_{j = 1}^p h_j \del{\beta_j}, \label{eq:5.37}
\end{align}
hvor \(g: \R^p \rightarrow \R\) er differentiabel og konveks og \(h_j: \R \rightarrow \R\) er konveks, men ikke nødvendigvis differentiabel.
Bemærk at lasso problemet \eqref{eq:2.5} kan dekomponeres som \eqref{eq:5.37} med \(g \del{\tbeta} =\Vert \y - \X \tbeta \Vert_2^2\) og \(h_j \del{\beta_j} = \lambda \vert \beta_j \vert\).
\citep{Tseng_coordinate} viste, at for enhver konveks funktion \(f\) som kan opdeles som \eqref{eq:5.37}, vil coordinate descent algoritmen \eqref{eq:5.36} konvergere til det globale minimum. 
Nøgleegenskaben bag dette resultat er, at den ikke-differentiable komponent \(h \del{\beta} = \sum_{j=1}^p h_j \del{\beta_j}\), kan opsplittes som summen af funktioner af hver individuel parameter.
Resultatet betyder, at coordinate descent kan bruges til at løse lasso og dens generaliseringer, som beskrives senere i specialet.
Hvis den ikke-differentiable komponent \(h\) ikke kan opsplittes, da kan det ikke garanteres at coordinate descent konvergerer.

%\subsubsection{Lineær regression og lasso}
%Optimalitets betingelserne for lasso problemet \eqref{eq:2.5} er
%\begin{align*}
%-2 \sum_{i=1}^n \del{y_i - \sum_{k=1}^p x_{ik} \beta_k} x_{ij} + \lambda s_j = 0,
%\end{align*}
%hvor \(s_j =\text{sign} \del{\beta_j^*}\) hvis \(\beta_j^* \neq 0\) og \(s_j \in \sbr{-1,1}\) hvis \(\beta_j^* = 0\) for \(j=1, \ldots, p\).
%Coordinate descent algoritmen løser disse ligninger og itererer over \(j=1,2,\ldots,p,1,2, \ldots\).
%Lad os definer det partielle residual \(r_i^{(j)} = y_i - \sum_{k \neq j} x_{ik} \widehat{\beta}_k\), som fjerner nuværende fit fra alle undtaget \(j\)'te prædiktor.
%Da er opdateringen givet ved
%\begin{align*}
%\widehat{\beta}_j = S_\lambda \del{\tilde{\beta}_j},
%\end{align*}
%hvor \(\tilde{\beta}_j\) er koefficienten af en simpel lineær regression af det partial residual på variabel \(j\).
\newpage
\begin{alg} [Coordinate descent for lasso problemet]
\begin{enumerate}
\item Standardisér prædiktorerne \(\x_1, \ldots, \x_p\) og centrér responsvariablen.
Definer en følge af værdier \(\lambda_0 > \lambda_1 > \ldots > \lambda_L\), hvor \(\lambda_0\) vælges, således at \(\widehat{\tbeta}^\text{lasso} \del{\lambda_0} =\mathbf{0}\).
\item For hvert \(\lambda \in \cbr{\lambda_0, \ldots, \lambda_L}\), gentages følgende trin for \(j = 1, \ldots, p\) indtil konvergens:
\begin{itemize}
\item Opskriv lasso problemet \eqref{eq:2.5}
\begin{align*}
\sum_{i=1}^n \del{y_i - \sum_{k \neq j} x_{ik} \widehat{\beta}^\text{lasso}_k - x_{ij} \beta_j}^2 + \lambda \sum_{j = 1}^p \vert \beta_j \vert
\end{align*}
hvor \(\widehat{\beta}^\text{lasso}_k \del{\lambda}\) er det nuværende estimat for \(\beta_k\) for et given \(\lambda\), hvor \(k \neq j\).
\item Udregn de partielle residualer: \(r_{i}^{(j)} = y_i - \sum_{k \neq j} x_{ik} \widehat{\beta}^\text{lasso}_k \del{\lambda}\) for alle \(i\)
\item Udregn koefficienten af en simpel lineær regression af den partielle residual på \(j\)'te prædiktor: \(\tilde{\beta}_j = \frac{1}{n} \sum_{i=1}^n r_{i}^{(j)} x_{ij}\) 
\item Opdater det nuværende estimat \(\widehat{\beta}^\text{lasso}_j\) ud fra soft-thresholding operatoren
\begin{align}
\widehat{\beta}^\text{lasso}_j \del{\lambda}= S_{\frac{\lambda}{2n}} \del{\tilde{\beta}_j}. \label{eq:update_coordinate}
\end{align}
\end{itemize}
\end{enumerate}
\end{alg}
%
Løsningerne udregnes for en aftagende følge af værdier \(\cbr{\lambda_{\ell}}_{\ell = 0}^L\), hvor \(\lambda_0\) vælges således at \(\widehat{\tbeta} \del{\lambda_0} =\mathbf{0}\). 
Algoritmen udnytter \textit{warm start}, hvilket betyder, at \(\widehat{\tbeta} \del{\lambda_\ell}\) anvendes som begyndelsesværdi for løsningen \(\widehat{\tbeta} \del{\lambda_{\ell + 1}}\), dette fører til en mere stabil algoritme. 
Når \(\widehat{\tbeta} = \mathbf{0}\), har vi, at \(\widehat{\beta}_j\) vil forblive nul hvis \(\frac{1}{n} \left\vert \left\langle \mathbf{x}_j, \mathbf{y} \right\rangle \right\vert < \frac{\lambda}{2n}\). Derfor er \( \lambda_0 = 2 \max_j \left\vert \left\langle \mathbf{x}_j, \mathbf{y} \right\rangle \right\vert\).
Strategien er at vælge en minimum værdi \(\lambda_L = \epsilon \lambda_0\) og konstruere en følge af \(K\) værdier af \(\lambda\), som aftager fra \(\lambda_0\) til \(\lambda_L\) på logskalaen.
Typiske værdier er \(\epsilon = 0.001\) og \(K =100\).

Den beskrevne coordinate descent algoritme er implementeret i \Rlang-pakken \texttt{glmnet}.
Koefficientstierne i figur \ref{fig:diabetes_koef} er fundet ud fra denne algoritme.

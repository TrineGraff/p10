\section{Coordinate descent}
Ideen bag coordinat descent er somsagt at optimere over hver parameter separat, mens de resterende parametre fastholdes.
Coordinate descent er en første ordens iterativ algoritme, og derfor er det blot gradienten som betragtes.
Algoritmen opdaterer fra \(\beta^t\) til \(\beta^{t+1}\) ved at vælge én koordinat som skal opdateres, og da udføres en univariat minimering over dette koordinat.
Dvs hvis koordinat $k$ er valgt i iteration $t$, da er opdateringen givet ved
\begin{align}
\beta_k^{t+1} =\underset{ \beta_k}{\arg \min}  f\del{ \beta_1^t, \beta_2^t, \dots, \beta_{k-1}^t, \beta_k, \beta_{k+1}^t, \dots, \beta_p^t  }, \label{eq:5.36}
\end{align}
hvor $\beta_j^{t+1} = \beta_j^t$ for $j \neq k$. 
%For at algoritmen konvergerer til det globale minimum af en konveks funktion, skal funktionen være kontinuert differentiabel og streng konveks i hver koordinat.
%
%Der er to basale modeller for coordinate algoritmer i forhold til den måde de opdaterer parametrene. Den første er cyclic coordinate og den anden kaldes greedy coordinate. 
%Cyclic opdaterer en parameter i hvert loop og den greedy opdaterer parameteren, som giver den største ændring i funktionen. 
%Coordinate algoritmer er generelt ofte anvendt, når man skal estimere parametre af en funktion, som ikke er differentiable. 
%En sådan funktion kan se ud som følgende
%\begin{align}
%f(\beta_1, \dots, \beta_p) = g(\beta_1, \dots, \beta_p) + \sum_{j = 1}^p h_j \del{\beta_j}, \label{eq:5.37}
%\end{align}
%hvor $g: \R^p \rightarrow \R $ er differentialble og konveks, og hvor den univariate funktion $h_j : \R \rightarrow \R$ er konveks, men ikke nødvendigvis differentialble. Udfra Tseng .... kan det vises at enhver konveks funktion, som $f$ i \eqref{eq:5.37}, konvergerer coordinate descent algoritmen \eqref{eq:5.36} til det globale minimum. 

For mange optimeringsproblemer kan objektfunktionen opskrives på formen
\begin{align}
f(\beta_1, \dots, \beta_p) = g(\beta_1, \dots, \beta_p) + \sum_{j = 1}^p h_j \del{\beta_j}, \label{eq:5.37}
\end{align}
svarende til \eqref{eq:opdeling_fkt}, hvor \(g \del{\beta} = \frac{1}{2n} \Vert \y - \X \beta \Vert_2^2\) og \(h_j \del{\beta_j} = \lambda \vert \beta_j \vert\) for lasso.
Hvis \(g: \ \R^p \rightarrow \R\) konveks og differentialble og $h_j: \ \R \rightarrow \R$ er konveks, men ikke nødvendigvis differentialble, da viste TSeng==, at for enhver konveks funktion som kan opdeles som \eqref{eq:5.37}, konvergerer coordinate descent algoritmen \eqref{eq:5.36} til det globale minimum. 
%
\begin{alg} [Coordinate descent for lasso problemet]
\begin{enumerate}
%
\item Standardisere prædiktorerne \(\x_1, \ldots, \x_p\) og centre responsvariablen.
Sæt alle \(\beta_j = 0\).
Definer et gitter af værdier \(\lambda_1 > \lambda_2 > \ldots\), hvor \(\lambda_1\) vælges således at \(\hat{\beta}^\text{lasso} \del{\lambda_1} =\mathbf{0}\).
\item For hvert \(\Lambda \in \cbr{\lambda_1, \lambda_2, \ldots}\), gentages følgende steps over \(j = 1, \ldots, p\) indtil konvergens:
\begin{itemize}
\item Udregn de partielle residualer \(r_{ij} = y_i - \sum_{k \neq j} x_{ik} \beta_k\)
\item Udregn simple mindste kvadraters koefficienter af disse residualer på \(j\)'te prædiktor: \(\beta_j^* = \frac{1}{n} \sum_{i=1}^n x_{ij} r_{ij}\) 
\item Opdater \(\beta_j\) udfra soft-thresholding
\begin{align*}
\beta_j \rightarrow S_\lambda \del{\beta_j^*}.
\end{align*}
\end{itemize}
\end{enumerate}
\end{alg}
Coordinate descent algoritmen for lasso giver ikke den fulde lasso løsningsstil, men kan bruges til at udregne lasso løsningerne for \(\Gamma\).
Vi aftager \(\lambda\) og cycle igennem variablerne indtil konvergens til lasso estimatet \(\hat{\beta}^\text{lasso} \del{\lambda}\).
Derefter aftages \(\lambda\) igen og vi anvender den tidligere løsning som begyndelsesværdi for den nye værdi af \(\lambda\).
\newpage
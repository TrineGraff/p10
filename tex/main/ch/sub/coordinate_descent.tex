\section{Coordinate descent}
Coordinate descent er en første ordens iterativ algoritme, og derfor er det blot gradienten som betragtes.
Algoritmen opdaterer fra \(\beta^t\) til \(\beta^{t+1}\) ved at vælge én koordinat som skal opdateres, og da udføres en univariat minimering over dette koordinat.
Dvs hvis koordinat $k$ er valgt i iteration $t$, da er opdateringen givet ved
\begin{align}
\beta_k^{t+1} =\underset{ \beta_k}{\arg \min}  f\del{ \beta_1^t, \beta_2^t, \dots, \beta_{k-1}^t, \beta_k, \beta_{k+1}^t, \dots, \beta_p^t  }, \label{eq:5.36}
\end{align}
hvor $\beta_j^{t+1} = \beta_j^t$ for $j \neq k$. 

Der er to basale modeller for coordinate algoritmer i forhold til den måde de opdaterer parametrene. Den første er cyclic coordinate og den anden kaldes greedy coordinate. 
Cyclic opdaterer en parameter i hvert loop og den greedy opdaterer parameteren, som giver den største ændring i funktionen. 

Coordinate algoritmer er generelt ofte anvendt, når man skal estimere parametre af en funktion, som ikke er differentiable. 
En sådan funktion kan se ud som følgende
\begin{align}
f(\beta_1, \dots, \beta_p) = g(\beta_1, \dots, \beta_p) + \sum_{j = 1}^p h_j \del{\beta_j}, \label{eq:5.37}
\end{align}
hvor $g: \R^p \rightarrow \R $ er differentialble og konveks, og hvor den univariate funktion $h_j : \R \rightarrow \R$ er konveks, men ikke nødvendigvis differentialble. Udfra Tseng .... kan det vises at en hver konveks funktion, som $f$ i \eqref{eq:5.37}, konvergerer coordinate descent algoritmen \eqref{eq:5.36} til det globale minimum. 
?? bevis
 
 
\begin{alg} [Coordinate descent]
\begin{enumerate}
%
\item Standardisere prædiktorerne til at have en middelværdi 0 og $\norm{\textbf{x}_j}_2^2$ for alle $j$.
\end{enumerate}
\end{alg}

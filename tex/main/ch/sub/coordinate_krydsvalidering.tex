\subsection{Krydsvalidering}
Funktionerne \texttt{cv.glmnet} og \texttt{cv.gglasso} fra pakkerne \texttt{glmnet} og \texttt{gglasso} udfører 10-fold krydsvalidering.

Figur \ref{fig:cv_plot} illustrerer krydsvaliderings kurven samt øvre og nedre standard afvigelse for hver værdier af $\log \del{\lambda}$ for lasso og dens generaliseringer. 
De to lodrette stiplede linjer indikerer \(\lambda_{\text{min}}\) og \(\lambda_\text{1sd}\), hvor \(\lambda_{\text{min}}\) er værdien af \(\lambda\), som giver den mindste gennemsnitlige krydsvalideringsfejl og \(\lambda_\text{1sd}\) er den største værdi af \(\lambda\), således at fejlen er indenfor en standardafvigelse af minimum. 

For elastik net betragtes som sagt to tuning parametre, \(\lambda\) og \(\alpha\), hvor vi udfører krydsvalidering for hver værdi af \(\alpha\). 
Den mindste krydsvalideringsfejl er for $\alpha =1$, hvilket svarer til lasso modellen, og derfor inkluderes elastisk net ikke. 
%
\imgfigh{cv_plot.pdf}{1}{10-fold krydsvalideringsfejl som funktion af $\log \del{\lambda}$ for hver metode. 
De stiplede linjer indikerer \(\lambda\) værdien for den med mindst krydsvalideringsfejl og den største værdi af \(\lambda\) således at fejlen stadig er inden for en standardafvigelse af minimum.}{cv_plot}
%

\input{fig/tab/cv_tab}

For at give et bedre overblik giver tabel \ref{tab:cv_tab} værdierne af $\log \del{ \lambda_{\min}}$ og $\log \del{ \lambda_{1\text{sd}}}$, krydsvalideringsfejlen og antallet af parametre for hver metode.
%Den valgte tuning parameter er markeret med tykt for hver metode.   

For lasso ses en markant reducering i antallet af parametre for $\lambda_{1\text{sd}}$ i forhold til $\lambda_{\min}$, dette øger ikke krydsvalideringsfejlen betydeligt og derfor anvendes $\widehat{\lambda}_{1\text{sd}}$ som tuning parameter for lasso. 
Ridge regression shrinker blot koefficienter, og derfor vælges alle variable.
For ridge regression vælger vi, derfor $\widehat{\lambda}_{\min}$ som tuning parameter, da den har mindst krydsvalideringsfejl.
For group lasso vælges lidt overraskende alle parametre med \(\lambda_\text{min}\), mens antallet af parameter reduceres med 7 for $\lambda_{1\text{sd}}$. Disse 7 variable tilhører alle gruppe 5.
Vi lader $\widehat{\lambda}_{1\text{sd}}$ være den optimale tuning parameter for group lasso, da den har det færreste antal parametre. 

Adaptive lasso med OLS og lasso vægte vælger blot to variable for både  $\lambda_{\min}$ og $\lambda_{1\text{sd}}$. 
Variablerne er  \textcolor{blue3}{CLF16OV} og \textcolor{blue3}{CE16OV}.
Vi lader $\widehat{\lambda}_{\min}$ være tuning parameteren for adaptive lasso modellerne. 

Figur \ref{fig:coef_kryds_coord} giver et overblik over størrelsen af de 14 estimerede koefficienter for lasso og de to estimerede koefficienter for adaptive lasso.
Heraf ses, at lasso hovedsagligt vælger variable i samme gruppe som arbejdsløshedsraten.
Variablerne valgt af adaptive lasso, \textcolor{blue3}{CLF16OV} og \textcolor{blue3}{CE16OV}, har også de største estimerede koefficienter for lasso, mens resten af variablerne har koefficienter meget tæt på nul. 
Figur \ref{fig:coef_ridge_kryds_coord} og \ref{fig:coef_gglasso_kryds_coord} viser de estimerede koefficienter for henholdsvis ridge regression og group lasso.
Igen ser vi, at variablerne \textcolor{blue3}{CLF16OV} og \textcolor{blue3}{CE16OV} klart har de største estimerede koefficienter.    
%
\imgfigh{coef_kryds_coord.pdf}{1}{Estimerede koefficienter for lasso og adaptive lasso med OLS og lasso vægte,  hvor $\widehat{\lambda}$ er fundet ud fra krydsvalidering.
Farverne indikerer hvilken gruppe, variabler tilhører, og y-aksen er variablerne udvalgt af lasso. }{coef_kryds_coord}


Tabel \ref{tab:adj_r2_shrinkage_tab} viser adjusted R$^2$ for lasso og dens generaliseringer.
Adaptive lasso modellerne har størst adjusted R$^2$, mens ridge regression har mindst adjusted R\(^2\), hvilket stemmer overens med at adaptive lasso valgte det færreste antal parametre, mens ridge regression valgte alle parametre. 


Figur \ref{fig:resid_lasso_coord_kryds}-\ref{fig:resid_adap_ols_coord_kryds} viser en analyse af de standardiserede residualer for lasso og dens generaliseringer. 
Vi ser, samme tendens for lasso og dens generaliseringer. Histogrammet og QQ-plottet indikerer tungere haler end en normalfordeling og autokorrelation i første lag.
Dette bekræftes i tabel \ref{tab:res_shrinkage_tab}, som viser skewness, excess kurtosis, $p$-værdier fra JB-testen og LB testen for de standardiserede residualer, hvor $\widehat{\lambda}$ er estimerede udfra krydsvalidering.  
Vi ser, at alle modellerne har en negativ skewness og en kurtosis forskellige fra nul. 
Derudover afvises JB testens nulhypotesen om normalitet for alle modeller med undtagelse af group lasso, samt bliver hulhypotesen omkring uafhængighed også afvist i LB testen. 
Vi ser også, at group lasso er den model, som har en kurtosis tætteste på nul, samt en meget lille skewness. 

\subsubsection*{Inferens}
På figur \ref{fig:boxplot_lasso_coord_kryds} ses bootstrap resultater af variablene udvalgt af lasso.
Idet adaptive lasso er konsistent i variable udvælgels, vil de to adaptive modeller altid vælge variablerne\textcolor{blue3}{CLF16OV} og \textcolor{blue3}{CE16OV} og derfor ser vi bort fra bootstrap af de to adaptive modeller. 

For lasso ser vi, at variablerne \textcolor{blue3}{lag 1},\textcolor{blue3}{UEMPL15OV}, \textcolor{blue3}{UEMP5TO14}, \textcolor{blue3}{UEMPLT5},\textcolor{blue3}{CE16OV} og \textcolor{blue3}{CLF16OV} ofte vælges, mens variablerne som ikke tilhører gruppe 2 oftes fravælges.

%

%
%\imgfigh{bootstrap_alasso.pdf}{0.7}{Til venstre vises et boxplot af 1000 bootstrap realisationer af $\widehat{\tbeta}^{\text{AL}} \del{{\widehat{\lambda}_\text{min}}}$ med OLS vægte, mens plottet til højre illustrerer andelen af bootstrap realisationer, hvor parameter estimaterne er præcis lig nul.}{bootstrap_alasso}

Vi udfører inferens for lasso modellen for en fast værdi af tuning parameteren, hvor resultaterne er givet  i tabel \ref{tab:fixedLassoInf}.
Heraf ser vi at variablerne \textcolor{blue3}{CLF16OV}, \textcolor{blue3}{CE16OV} og \textcolor{blue3}{lag1} afviser nulhypotesen, og derfor er signifikante. ------


% \(\lambda\), med funktionen \texttt{fixedLassoInf} fra \Rlang-pakken \texttt{selectiveInference}.
%\Rlang-koden for dette er givet i appendiks \ref{subsubsec:inferens}.
%Funktionen udregner \(p\)-værdier og konfidensintervaller for lasso estimatet for en fast værdi af tuning parameteren.
%Resultaterne er givet i tabel \ref{tab:fixedLassoInf}.

%
\input{fig/tab/fixedLassoInf}
%

\newpage
%
%

 \input{fig/tab/adj_r2_shrinkage}


\newpage
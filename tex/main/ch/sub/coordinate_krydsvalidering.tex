\subsection{Krydsvalidering}
Funktionerne \texttt{cv.glmnet} og \texttt{cv.gglasso} fra pakkerne \texttt{glmnet} og \texttt{gglasso} udfører 10-fold krydsvalidering.

Figur \ref{fig:cv_plot} illustrerer den gennemsnitlige krydsvalideringsfejl samt øvre og nedre standardafvigelse for hver værdier af $\log \del{\lambda}$ for lasso og dens generaliseringer. 
De to lodrette stiplede linjer indikerer \(\lambda_{\text{min}}\) og \(\lambda_\text{1sd}\), hvor \(\lambda_{\text{min}}\) er værdien af \(\lambda\), som giver den mindste gennemsnitlige krydsvalideringsfejl og \(\lambda_\text{1sd}\) er den største værdi af \(\lambda\), således at fejlen er indenfor en standardafvigelse af minimum. 
For elastisk net finder vi, at $\alpha =1$ giver den mindste krydsvalideringsfejl, hvilket svarer til lasso modellen. Derfor betragter vi ikke elastik net i dette afsnit. 
Vi betragter adaptive lasso med OLS vægte og adaptive lasso med lasso vægte, hvor $\gamma = 0.5$, da den giver den mindste krydsvalidering fejl. 

\imgfigh{cv_plot.pdf}{1}{10-fold krydsvalideringsfejl som funktion af $\log \del{\lambda}$ for lasso og den generaliseringer. 
De stiplede linjer betegner \(\lambda_\text{min}\) og \(\lambda_\text{1sd}\).}{cv_plot}
%

\input{fig/tab/cv_tab}

For at give et bedre overblik giver tabel \ref{tab:cv_tab} værdierne af $\log \del{ \lambda_{\min}}$ og $\log \del{ \lambda_{1\text{sd}}}$, krydsvalideringsfejl, antallet af parametre, justerede R$^2$ og log-likelihood for lasso og dens generaliseringer.
%Den valgte tuning parameter er markeret med tykt for hver metode.   

For lasso ses en markant reducering i antallet af parametre for $\lambda_{1\text{sd}}$ i forhold til $\lambda_{\min}$, dette øger ikke krydsvalideringsfejlen betydeligt, og derfor anvendes $\widehat{\lambda}_{1\text{sd}}$ som tuning parameter for lasso. 
Ridge regression mindsker blot koefficienterne, og derfor vælges alle variable. For ridge regression vælger vi \(\lambda_{\min}\) som tuning parameter, da den har mindst krydsvalideringsfejl.
For group lasso vælges lidt overraskende alle parametre med \(\lambda_\text{min}\), mens antallet af parametre reduceres med 7 for $\lambda_{1\text{sd}}$. 
Disse 7 variable tilhører alle gruppe 5.
Vi lader $\widehat{\lambda}_{1\text{sd}}$ være den optimale tuning parameter for group lasso, da den har det færreste antal parametre.
Adaptive lasso med OLS vægte og lasso vægte vælger blot to variable for både \(\lambda_\text{min}\) og \(\lambda_{1\text{sd}}\).
Vi lader $\lambda_{1\text{sd}}$ være tuning parameteren for adaptive lasso modellerne.  
De valgte model---

Justerede R\(^2\) er størst for adaptive lasso modellerne og mindst for ridge regression.
%
%
%For lasso ses en markant reducering i antallet af parametre for $\lambda_{1\text{sd}}$ i forhold til $\lambda_{\min}$. Dette øger ikke krydsvalideringsfejlen eller justerede R$^2$ betydeligt, og derfor anvendes $\widehat{\lambda}_{1\text{sd}}$ som tuning parameter for lasso. 
%Vi ser også, at ridge regression har den mindste værdi af justerede R$^2$ samt den højeste værdi af log-likelihood, men modellen mindsker også blot koefficienterne. 
%For ridge regression vælger vi, derfor $\widehat{\lambda}_{\min}$ som tuning parameter, da den har mindst krydsvalideringsfejl.
%
%For group lasso vælges lidt overraskende alle parametre med \(\lambda_\text{min}\), mens antallet af parametre reduceres med 7 for $\lambda_{1\text{sd}}$. 
%Disse 7 variable tilhører alle gruppe 5.
%Vi anvender $\widehat{\lambda}_{1\text{sd}}$ som turning parameter, da den har færreste antal parametre. 
%Adaptive lasso m. OLS vægte og lasso vægte vælger blot to variable for $\lambda_{1\text{sd}}$ uden at det øger deres krydsvalideringsfejl betydeligt. 
%Derfor lader vi $\lambda_{1\text{sd}}$ være tuning parameteren for adaptive lasso modellerne. 

På figur \ref{fig:coef_kryds_coord} vises de 14 estimerede koefficienter for lasso og de 2 estimerede koefficienter for adaptive lasso.

Heraf ses, at lasso hovedsagligt vælger variable i samme gruppe som arbejdsløshedsraten.
For lasso ses at variablerne valgt af adaptive lasso, \textcolor{blue3}{CLF16OV} og \textcolor{blue3}{CE16OV}, har de største estimerede koefficienter, efterfulgt af \textcolor{blue3}{UEMPLT5}, \textcolor{blue3}{UEMP5TO14}, \textcolor{blue3}{UEMPL15OV} og \textcolor{blue3}{lag 1}, mens de øvrige er meget tæt på nul. 
Figur \ref{fig:coef_ridge_kryds_coord} og \ref{fig:coef_gglasso_kryds_coord} viser de estimerede koefficienter for henholdsvis ridge regression og group lasso.
Igen ser vi, at variablerne \textcolor{blue3}{CLF16OV} og \textcolor{blue3}{CE16OV} klart har de største estimerede koefficienter.    
%
\imgfigh{coef_kryds_coord.pdf}{1}{Estimerede koefficienter for lasso og adaptive lasso med OLS og lasso vægte,  hvor $\widehat{\lambda}$ er fundet ud fra krydsvalidering.
Farverne indikerer hvilken gruppe, variabler tilhører, og y-aksen er variablerne udvalgt af lasso. }{coef_kryds_coord}


Figur \ref{fig:resid_lasso_coord_kryds}-\ref{fig:resid_adap_ols_coord_kryds} viser en analyse af de standardiserede residualer for lasso og dens generaliseringer. 
Vi ser, samme tendens for lasso og dens generaliseringer. Histogrammet og QQ-plottet indikerer tungere haler end normalfordelingen og autokorrelation i første lag.
Dette bekræftes i tabel \ref{tab:res_shrinkage_tab}, som viser skewness, excess kurtosis, $p$-værdier fra JB-testen og LB testen for de standardiserede residualer, hvor $\widehat{\lambda}$ er estimerede udfra krydsvalidering.  
Vi ser, at alle modellerne har en negativ skewness og en kurtosis forskellige fra nul. 
Derudover afvises JB testens nulhypotesen om normalitet for alle modeller med undtagelse af group lasso, dog har den en lille skewness og kurtosis.
For LB testen afvises nulhypotesen om uafhængighed for alle modeller.

\subsubsection{Inferens}
På figur \ref{fig:boxplot_lasso_coord_kryds} ses bootstrap resultater for variablene udvalgt af lasso.
Da adaptive lasso har konsistent variabeludvælgelse, vil variablerne \textcolor{blue3}{CLF16OV} og \textcolor{blue3}{CE16OV} altid vælges, derfor laves der ikke bootstrap for disse.
Variablerne \textcolor{orange}{TB6MS}, \textcolor{blue3}{PAYEMS} og \textcolor{red3}{DPCERA3M086SBEA} fravælges over 50\% af bootstrap realisationerne, mens variablerne  \textcolor{blue3}{lag 1}, \textcolor{blue3}{UEMPL15OV}, \textcolor{blue3}{UEMP5TO14}, \textcolor{blue3}{UEMPLT5}, \textcolor{blue3}{CE16OV} og \textcolor{blue3}{CLF16OV} ofte vælges.
Generelt fravælges variablerne, som ikke tilhører gruppe 2.
I forhold til størrelsen af de estimerede koefficienter for lasso er bootstrap resultaterne ikke overraskende. 

Herefter anvendes TG testen for lasso modellen.
Resultaterne er givet i tabel \ref{tab:fixedLassoInf}.
Heraf ser vi at variablerne \textcolor{blue3}{CLF16OV}, \textcolor{blue3}{CE16OV} og \textcolor{blue3}{lag 1} afviser nulhypotesen, og derfor er signifikante.
$Z$-score er udregnet for $s_k \boldsymbol{\eta}^T \textbf{y}$, hvor $s_k$ er fortegnet for den $k$'te variable. 
Vi observerer, at $Z$-score er meget stor for variablerne \textcolor{blue3}{CLF16OV} og \textcolor{blue3}{CE16OV}, mens den er relative lav for de resterende variabler. 
Vi ser også, at  $\mathcal{V^-} \leq \boldsymbol{\eta}^T \textbf{y} \leq \mathcal{V^+}$, som stemmer overens med teorien.
%
%\imgfigh{ols_lasso_interval_kryds.pdf}{1}{ }{ }

%
\input{fig/tab/fixedLassoInf}
%

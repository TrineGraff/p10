\section{LARS}
I dette afsnit vil vi finde den optimale model udfra LARS algoritmen uden og med lasso modifikation. 
Hertil anvendes funktionen \texttt{lars} fra \Rlang-pakken af samme navn.
Mens \texttt{glmnet} fitter en model for 100 værdier af \(\lambda\), fitter \texttt{lars} en model for hvert step.
For LARS algoritmen uden lasso modifikationen tilføjes en variabel i hvert step, mens variable kan tilføjes og fjernes i hvert step for LARS algoritmen med lasso modifikationen.

For at finde den optimale model anvendes igen 10-fold krydsvalidering og BIC til at estimere tuning parameteren, som for LARS algoritmen er fraktion af \(\ell_1\) norm, der er givet ved \(f = \frac{\abs{\tbeta}}{\max \abs{\tbeta}}\), hvor \(f \in \sbr{0,1}\).


Vi anvender TG testen for LARS uden lasso modifikation og kovarians testen for LARS med lasso modifikation.   
Hertil anvender vi funktionerne \texttt{larInf} fra \Rlang-pakken \texttt{selectiveInference} og \texttt{covTest} fra \Rlang-pakken af samme navn. 
\Rlang-koden for dette er givet i appendiks \ref{subsubsec:inferens}.


\subsection{Krydsvalidering}
Funktionen \texttt{cv.lars} fra \Rlang-pakken \texttt{lars} udfører 10-fold krydsvalidering.

For LARS algoritmen uden lasso modifikationen betragtes en følge af 127 værdier af $f$, dvs at vi får en værdi af den gennemsnitlige krydsvaliderings fejl når en variable tilføjes.
For LARS algoritmen med lasso modifikation betragtes en følge af 100 værdier af $f$, dvs vi får ikke en gennemsnitlige krydsvaliderings fejl for hver gang en variable bliver tilføjet eller fjernet. 

Figur \ref{fig:lars_kryds} illustrerer den gennemsnitlige krydsvalideringsfejl samt øvre og nedre standardafvigelse for hver værdier af $f$ for LARS uden og med lasso modifikationen.
Hvis $f = 0$ er der ingen variabler tilføjet og hvis $f=1$ er alle variabler tilføjet. 
De to lodrette stiplede linjer indikerer \(f_{\text{min}}\) og \(f_\text{1sd}\), hvor \(f_{\text{min}}\) er værdien af \(f\), som giver den mindste gennemsnitlige krydsvalideringsfejl og \(f_\text{1sd}\) er den mindste værdi af \(f\), således at fejlen er indenfor en standardafvigelse af minimum. 


\imgfigh{lars_kryds.pdf}{1}{10-fold krydsvalideringsfejl som funktion af fraktion af \(\ell_1\)-norm for LARS algoritmen uden og med lasso modifikation. De stiplede linjer indikerer \(f_\text{min}\) og \(f_\text{1sd}\).}{lars_kryds}

Tabel \ref{tab:lars_lasso_tab} giver fraktion af \(\ell_1\)-normen, gennemsnitlige krydsvalideringsfejl, antallet af parametre og justerede R$^2$ for LARS algoritmen uden og med lasso modifikationen. 
For begge metoder afviger krydsvalideringsfejlen først på 5. decimal, derfor vælger vi modellerne med det færreste antal parametre. 
%
\input{fig/tab/lars_lasso_tab}
%
På figur \ref{fig:coef_lars_kryds} vises de 19 estimerede koefficienter for LARS uden lasso modifikation og de 13 estimerede koefficienter for LARS med lasso modifikation. 
De største estimerede koefficienter er givet for variablerne \textcolor{blue3}{CE16OV} og \textcolor{blue3}{CLF16OV}, efterfulgt af \textcolor{blue3}{UEMPLT5} \textcolor{blue3}{UEMP5TO14} og \textcolor{blue3}{UEMP15OV}, mens de resterende estimerede koefficienter er meget tæt på nul. 

\imgfigh{coef_lars_kryds.pdf}{1}{Estimerede koefficienter for LARS algoritmen uden og med lasso modifikationen, hvor $\widehat{f}$ er fundet ud fra krydsvalidering.
Farverne indikerer hvilken gruppe, variablerne tilhører.}{coef_lars_kryds}

Figur \ref{fig:lars_kryds_res} og \ref{fig:lars_lasso_kryds_res} viser en analyse af de standardiserede residualer, hvor vi igen ser tungere haler end normalfordelingen og autokorrelation i det første lag. 
Tabel \ref{tab:lars_kryds_res_tab} understøtter dette, hvor vi afviser normalitet og uafhængighed i lag 10. 

\newpage
\subsubsection{Inferens for LARS}
På figur \ref{fig:boxplot_lars_kryds} ses bootstrap resultater for variablene udvalgt af LARS algoritmen uden lasso modifikation. 
Vi kan se, at variablerne med de største estimerede koefficienter i figur \ref{fig:coef_lars_kryds} er også dem, som vælges oftest under bootstrap. 
Variablerne \textcolor{blue3}{UEMPL15OV}, \textcolor{blue3}{UEMP5TO14}, \textcolor{blue3}{UEMPLT5}, \textcolor{blue3}{CE16OV} og \textcolor{blue3}{CLF16OV} vælges for alle 1000 bootstrap realisationer.


Herefter anvendes TG testen for LARS.
Resultaterne er givet i tabel \ref{tab:larInf_kryds}.
Heraf ser vi, at variablen \textcolor{orange}{GS5} afviser nulhypotesen, som derfor er den eneste signifikante prædiktor, hvilket vi nok ikke helt havde forventet. 
%
\input{fig/tab/larInf}

\newpage
\subsubsection{Inferens for LARS med lasso modifikation}
På figur  \ref{fig:boxplot_lars_lasso_kryds} ses bootstrap resultater for variablerne udvalgt af LARS algoritmen med lasso modifikation. 
Variablerne \textcolor{blue3}{USGOOD} og \textcolor{blue3}{PAYMENS} fravælges over 75\% af bootstrap realisationerne, mens variablerne \textcolor{blue3}{CE16OV}, \textcolor{blue3}{CLF16OV} og \textcolor{blue3}{lag1} ofte vælges. 

Tabel \ref{tab:covTest} viser teststørrelsen samt $p$-værdier af kovarians testen for de 13 variabler der bliver udvagt af LARS med lasso modifikation. 
For 7 ud af 10 prædiktorer som tilhører gruppe 2 med undtagelse af \textcolor{blue3}{PAYEMS}, \textcolor{blue3}{lag 1} og \textcolor{blue3}{USCONS} afvises nulhypotesen, hvilket betyder, at disse prædiktorer er signifikante.
For de resterende prædiktorer kan nulhypotesen ikke afvises.
%
\input{fig/tab/covTest}
%
For de valgte variable udfører algoritmen 21 steps, hvor variablerne  \textcolor{chartreuse4}{CUMFNS}, \textcolor{blue3}{MANEMP} og \textcolor{orange}{GS1} tilføjes og fjernes igen. 
Variablen \textcolor{orange}{TB6MS} bliver tilføjet, fjernet og tilføjet igen. 


\newpage
\input{main/ch/sub/lars_bic}

%#  Function produces one fit at each new variable entry.
%# Cross-Validation for LASSO chops up the L1-norm into sequence of 100 points. 
%#  SE for CV is found from the sample SE of the squared errors.  NOT from rerunning CV multiple times.
%# Therefore CV may be inappropriate for small n.  Use Cp from Ssummary() instead.
%

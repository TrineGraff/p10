\section{LARS}
Først vil vi beskrive least angle regression (LARS) algoritmen, hvorefter vi vil introducere en simpel modifikation, som fører til lasso estimater. \\[2mm]
%
I grove træk fungerer algoritmen som følgende. 
Først sættes alle koefficienter lig nul, og vi finder prædiktoren, som er mest korreleret med responsvariablen \(\y\), denne prædiktor betegnes \(x_{j_1}\).
Der udføres så en simpel lineær regression af \(\y\) på \(x_{j_1}\), hvoraf vi finder en residualvektor.
Vi tager det størst mulige step i retningen af denne prædiktor indtil en anden prædiktor, som betegnes  \(x_{j_2}\), har samme korrelation med den nuværende residualvektor.
Istedet for at fortsætte langs retningen af \(x_{j_1}\) fortsætter LARS i en retning, som er ensvinklet mellem de to prædiktorer, indtil en tredje variabel bliver den mest korreleret variabel.
LARS algoritmen fortsætter da ensvinklet imellem \(x_{j_1}\), \(x_{j_2}\) og \(x_{j_3}\), indtil en fjerde variabel medtages, osv.
LARS algoritmen finder estimaterne \(\widehat{\tmu} = \X \widehat{\tbeta}\) ved at tilføje én prædiktor til modellen i hvert trin, således at præcis \(k\) koefficienter er forskellige fra nul efter \(k\) trin.

Figur \ref{fig:lars} illustrerer algoritmen, hvor $p = 2$ og $\X = \del{\textbf{x}_1 \ \textbf{x}_2}$.
Lad \(\mathbf{c} \del{\widehat{\tmu}}\) betegne de nuværende korrelationer
\begin{align}
\widehat{\mathbf{c}} = \mathbf{c} \del{\widehat{\tmu}} = \X^T \del{\y - \widehat{\tmu}}, \label{eq:lars_1.6}
\end{align}
således at \(\widehat{c}_j\) er proportional med korrelationen mellem prædiktor \(\x_j\) og den nuværende residualvektor.
For \(p=2\) afhænger de nuværende korrelationer kun af projektionen \(\bar{\y}_2\) af \(\y\) på det lineære underrum $\mathcal{L} \del{\X}$ udspændt af \(\x_1\) og \(\x_2\)
\begin{align*}
\textbf{c}\del{\boldsymbol{\widehat{\mu}}} =  \X^T \del{ \bar{\y}_2 - \boldsymbol{\widehat{\mu}}}.
\end{align*}
Algoritmen starter i $\widehat{\boldsymbol{\mu}}_0 = \textbf{0}$.
På figur \ref{fig:lars} ses, at vinklen mellem \(\bar{\y}_2 - \widehat{\boldsymbol{\mu}}_0\) og \(\x_1\) er mindre end vinklen mellem \(\bar{\y}_2 - \widehat{\boldsymbol{\mu}}_0\) og \(\x_2\) og dermed fås \(c_1 \del{\widehat{\boldsymbol{\mu}}_0} > c_2 \del{\widehat{\boldsymbol{\mu}}_0}\).
%\(\bar{\y}_2 - \widehat{\boldsymbol{\mu}}_0\) har en mindre vinkel med \(\x_1\) end \(\x_2\), dvs \(c_1 \del{\widehat{\boldsymbol{\mu}}_0} > c_2 \del{\widehat{\boldsymbol{\mu}}_0}\).
Derfor tilføjer LARS \(\widehat{\boldsymbol{\mu}}_0\) i retningen af \(\x_1\), og vi får
\begin{align*}
\widehat{\tmu}_1 = \widehat{\tmu}_0 + \widehat{\gamma}_1 \x_1,
\end{align*}
hvor stepstørrelsen \(\widehat{\gamma}_1\) vælges, således at korrelationen mellem \(\bar{\y}_2 - \widehat{\tmu}_1\) og \(\x_1\) er lig korrelationen mellem \(\bar{\y}_2 - \widehat{\tmu}_1\) og \(\x_2\).
%  \(\bar{\y}_2 - \widehat{\tmu}_1\) er ligeså korreleret med \(\x_1\) som med \(\x_2\).
Dermed halverer \(\bar{\y}_2 - \widehat{\boldsymbol{\mu}}_1\) vinklen mellem \(\x_1\) og \(\x_2\), således at \(c_1 \del{\widehat{\boldsymbol{\mu}}_1} = c_2 \del{\widehat{\boldsymbol{\mu}}_1}\).
%
\begin{figure}[H]
\centering
\scalebox{0.8}{\input{fig/lars1.tikz}}
\caption{LARS algoritmen for \(p=2\). \(\bar{\y}_2\) er projektionen af \(\y\) på det lineære underrum \(\mathcal{L} \del{\x_1, \x_2}\).
Algoritmen starter i \(\widehat{\tmu}_0=\mathbf{0}\), hvor residualvektoren \(\bar{\y}_2 - \widehat{\tmu}_0\) har en større korrelation med \(\x_1\) end \(\x_2\). Næste LARS estimat er \(\widehat{\tmu}_1 = \widehat{\tmu}_0 + \widehat{\gamma}_1 \x_1\), hvor \(\widehat{\gamma}_1\) vælges, således at \(\bar{\y}_2 - \widehat{\tmu}_1\) halverer vinklen mellem \(\x_1\) og \(\x_2\). Næste LARS estimat er \(\widehat{\tmu}_2 = \widehat{\tmu}_1 + \widehat{\gamma}_2 \mathbf{u}_2\), hvor \(\mathbf{u}_2\) er en enhedsvektor, som ligger langs denne halveringslinje.
Der gælder, at \(\widehat{\tmu}_2 = \bar{\y}_2\) for \(p=2\), dette er ikke tilfældet for \(p>2\), som det ses på figur \ref{fig:lars2}.
 }\label{fig:lars}
\end{figure}
%
Lad $\mathbf{u}_2$ være enhedsvektoren, som ligger langs denne halveringslinje.
Det næste LARS estimat er dermed
\begin{align*}
\widehat{\boldsymbol{\mu}}_2 = \widehat{\boldsymbol{\mu}}_1+ \widehat{\gamma}_2 \mathbf{u}_2,
\end{align*}
hvor $\widehat{\gamma}_2$ er valgt, således at $\widehat{\boldsymbol{\mu}}_2 = \bar{\textbf{y}}_2$ i dette tilfælde hvor $p = 2$. 
For \(p>2\), da vil stepstørrelsen \(\widehat{\gamma}_2\) være mindre, hvilket fører til en anden ændring af retningen, som illustreres på figur \ref{fig:lars2}.
%
\begin{figure}[H]
\centering
\scalebox{0.8}{\input{fig/lars2.tikz}}
\caption{I hvert trin nærmer LARS estimatet \(\widehat{\tmu}_k\) sig det tilhørende OLS estimat \(\bar{\y}_k\), men vil aldrig nå det.
 }\label{fig:lars2}
\end{figure}
%
Efterfølgende LARS trin, udover to prædiktorer, tages langs ensvinklet vektorer, som generaliserer vektoren \(\mathbf{u}_2\) i figur \ref{fig:lars}.
Vi antager, at prædiktorerne \(\x_1, \ldots, \x_p\) er lineært uafhængige.
Lad \(\A\) være er en delmængde af indekser \(\cbr{1,\ldots, p}\), og definer matricen
\begin{align}
\X_\A = \del{\dots s_j \x_j \dots}_{j \in \A}, \label{eq:lars_2.4}
\end{align}
hvor $s_j = \pm 1$ og \(\X_\A\) er en matrix, som består af søjlerne i \(\X\), der er inkluderet i \(\mathcal{A}\) gange med \(s_j\).
Lad 
\begin{align}
\mathbf{N}_\A = \X_\A^T \X_\A \quad \text{og} \quad A_\A = \del{\mathbf{1}_\A^T \mathbf{N}_\A^{-1} \mathbf{1}_\A}^{-1/2}, \label{eq:lars_2.5}
\end{align}
hvor \(\mathbf{1}_\A\) er en vektor af 1-taller med en længde lig antallet af elementer i \(\A\).
Da defineres en såkaldt ensvinklet vektor
\begin{align}
\mathbf{u}_\A = \X_\A \omega_\A, \quad \text{hvor } \omega_\A = A_\A \mathbf{N}_\A^{-1} \mathbf{1}_\A, \label{eq:lars_2.6}
\end{align}
som er en enhedsvektor, der gør vinkler mellem søjlerne i \(\X_\A\) lige store, dvs
%der resulterer i lige store vinkler der er mindre end \(90^0\), med søjlerne i \(\X_\A\), dvs
\begin{align}
\X_\A^T \mathbf{u}_\A = A_\A \mathbf{1}_\A \quad \text{og} \quad \Vert \mathbf{u}_\A \Vert_2^2 = 1. \label{eq:lars_2.7}
\end{align}
Antag \(\widehat{\tmu}_\A\) er det nuværende LARS estimat.
Lad \(\widehat{\mathbf{c}} = \X^T \del{\y - \widehat{\boldsymbol{\mu}}_\A}\) være en vektor af nuværende korrelationer \eqref{eq:lars_1.6}.
Den \textit{aktive mængde} \(\A\) er en mængde af indekser, som svarer til prædiktorerne med de største absolutte korrelationer
\begin{align}
\widehat{C} = \max_j \cbr{\abs{\widehat{c}_j}}  \qquad \text{og} \qquad \A= \cbr{j: \ \abs{ \widehat{c}_j} = \widehat{C}}. \label{eq:lars_2.9}
\end{align}
Lad 
\begin{align}
s_j = \text{sign} \del{\widehat{c}_j}, \quad j \in \A. \label{eq:lars_2.10}
\end{align}
%
Herefter kan vi give en fyldestgørende beskrivelse af LARS algoritmen.
%
\begin{alg} [LARS algoritmen]
\begin{enumerate}
\item Standardisér prædiktorerne og centrér responsvariablen. 
Start med \(\widehat{\boldsymbol{\mu}}_0 = \mathbf{0}\), \(\widehat{\mathbf{c}} = \X^T \y\), og \(\A = \emptyset\).
\item Find prædiktoren \(\tx_j\) med den største værdi af \(\abs{\widehat{c}_j}\) og definer den aktive mængde \(\A = \cbr{j}\).
\item 
Gentag følgende indtil alle prædiktorer er indeholdt i den aktive mængde:
\begin{itemize}
\item Udregn \(\widehat{\mathbf{c}}\), \(\widehat{C}\), \(\X_\A\), \(A_\A\) og \(\mathbf{u}_\A\) som i \eqref{eq:lars_2.4}-\ref{eq:lars_2.9} samt
\begin{align*}
\mathbf{a} = \X^T \mathbf{u}_\A.
\end{align*}
\item Opdatér \(\widehat{\boldsymbol{\mu}}_\A\) til
\begin{align}
\widehat{\boldsymbol{\mu}}_{\A_+} = \widehat{\boldsymbol{\mu}}_\A + \widehat{\gamma} \mathbf{u}_\A, \label{eq:lars_2.12}
\end{align}
hvor 
\begin{align}
\widehat{\gamma} = \min_{j \in \A^c} \cbr{ \frac{\widehat{C}- \widehat{c}_j}{A_\A - a_j} , \frac{\widehat{C} + \widehat{c}_j}{A_\A + a_j}}_+, \label{eq:lars_2.13}
\end{align}
og hvor \(\min \cbr{\cdot , \cdot}_+\) indikerer, at minimum kun tages over de positive komponenter indenfor valget af \(j\) i \eqref{eq:lars_2.13}.
\item Sæt \(\A = \A \cup \cbr{\widehat{j}}\), hvor \(\widehat{j}\) er minimeringsindekset i \eqref{eq:lars_2.13}.
\end{itemize}
\end{enumerate}
\end{alg}
%
%Formlerne \eqref{eq:lars_2.12} og \eqref{eq:lars_2.13} har følgende fortolkning.
%Definer
%\begin{align}
%\tmu \del{\gamma} = \widehat{\tmu}_\A + \gamma \mathbf{u}_\A, \label{eq:lars_2.14}
%\end{align}
%for \(\gamma > 0\), således at den nuværende korrelation er givet ved
%\begin{align}
%c_j \del{\gamma} = \x_j^T \del{\y - \tmu \del{\gamma}} = \widehat{c}_j - \gamma a_j. \label{eq:lars_2.15}
%\end{align}
%For \(j \in \A\) giver \eqref{eq:lars_2.7} og \eqref{eq:lars_2.9} at
%\begin{align}
%\abs{c_j \del{\gamma}} = \widehat{C} - \gamma A_\A,\label{eq:lars_2.16}
%\end{align}
%som viser, at alle af de maksimale absolutte nuværende korrelationer falder ligeligt ?????
%For \(j \in \A^C\), viser \eqref{eq:lars_2.15} og \eqref{eq:lars_2.16} at \(c_j \del{\gamma}\) er lig den maksimale værdi i \(\gamma = \frac{\widehat{C} - \widehat{c}_j}{A_\A - a_j}\).
%Derfor er \(\widehat{\gamma}\) i \eqref{eq:lars_2.13} den mindst positive værdi af \(\gamma\), således at et nyt indeks \(\widehat{j}\) tilføjes til den aktive mængde.
%\(\widehat{j}\) er minimeringsindekset i \eqref{eq:lars_2.13} og den nye aktive mængde \(\A_+\) er \(\A \cup \cbr{\widehat{j}}\) og den nye maksimum absolut korrelation er \(\widehat{C}_+ = \widehat{C}- \widehat{\gamma} A_\A\).

\subsection{Lasso modifikation} \label{subsec:lasso_modifikation}
I dette afsnit beskrives en simpel modifikation af LARS algoritmen, således at den giver lasso estimater.
Lad \(\widehat{\tbeta}^\text{lasso}\) være løsningen til lasso problemet \eqref{eq:2.5} med \(\widehat{\tmu}^\text{lasso} = \X \widehat{\tbeta}^\text{lasso}\).
Da kan det vises, at fortegnet af enhver ikke-nul koefficient \(\widehat{\beta}_j\) og fortegnet \(s_j\) af den nuværende korrelation \(\widehat{c}_j = \x_j^T \del{\y - \widehat{\tmu}}\) må stemme overens
\begin{align}
\text{sign} \del{\widehat{\beta}_j } = \text{sign} \del{\widehat{c}_j } = s_j, \quad j \in \A. \label{eq:lars_3.1}
\end{align}
%
%\begin{lem}
%For \(\widehat{\beta}^\text{lasso}\) må der gælder, at
%\begin{align*}
%\widehat{c}_j = \widehat{C} \cdot \text{sign} \del{\widehat{\beta}_j},
%\end{align*}
%hvor \(\widehat{c}_j = \x_j^T \del{\y - \widehat{\tmu}}= \x_j^T \del{\y - \X \widehat{\beta}}\).
%Dette medfører, at
%\begin{align}
%\text{sign} \del{\widehat{\beta}_j } = \text{sign} \del{\widehat{c}_j }, \quad j \in \A \label{eq:lars_5.29}
%\end{align}
%\end{lem}
%
Denne restriktion er ikke inkluderet i LARS algoritmen, men kan nemt modificeres hertil:
\textit{Når en ikke-nul koefficient ændrer fortegn eller bliver lig nul, da fjernes variablen fra den aktive mængde og vi beregner igen den nuværende ensvinklede retning \eqref{eq:lars_2.12}}.

For at tage denne modifikation i betragtning, defineres en \(p \times 1\) vektor
\begin{align*}
\widehat{\mathbf{d}} = \begin{cases}
s_j \omega_{\A_j}, &\text{hvis } j \in \A, \\
0, & \text{ellers},
\end{cases}
\end{align*}
hvor \(\omega_{\A_j}\) betegner elementet af vektoren \(\omega_{\A}\), som svarer til indeks \(j\).
For \(j \in \A\) opdateres
\begin{align*}
\widehat{\beta}_j \del{\gamma} = \widehat{\beta}_j^\text{prev} + \gamma \widehat{d}_j,
\end{align*}
hvor \(\widehat{\beta}_j^\text{prev}\) er lasso estimaterne fra det tidligere trin.
Lad 
\begin{align*}
\gamma_j = -\frac{\widehat{\beta}_j}{\widehat{d}_j}, \qquad \text{og} \qquad \tilde{\gamma} = \min_{\gamma_j > 0} \cbr{\gamma_j}.
\end{align*}
%
%Antag vi netop har fuldendt et LARS step, som har givet en ny aktiv mængde \(\A\) som i \eqref{eq:lars_2.9}, og at det tilhørende LARS estimat \(\widehat{\tmu}_\A\) svarer til en lasso løsning \(\widehat{\tmu}^\text{lasso} = \X \widehat{\tbeta}^\text{lasso}\).
%Lad
%\begin{align*}
%\omega_\A = A_\A \mathbf{N}_\A^{-1} \mathbf{1}_\A,
%\end{align*}
%være en vektor med længde lig antallet af elementer i \(\A\) og definer en \(p \times 1\) vektor
%\begin{align*}
%\widehat{\mathbf{d}} = \begin{cases}
%s_j \omega_{\A_j}, &\text{hvis } j \in \A, \\
%0, & \text{ellers}.
%\end{cases}
%\end{align*}
%Hvis vi bevæger os i den positive \(\gamma\) retning langs LARS linjen \eqref{eq:lars_2.14}, ser vi, at
%\begin{align*}
%\tmu \del{\gamma} = \X \tbeta \del{\gamma}, \quad \text{hvor } \beta_j \del{\gamma} = \widehat{\beta}_j + \gamma \widehat{d}_j
%\end{align*}
%for \(j \in \A\).
%Derfor vil \(\beta_j \del{\gamma}\) ændre fortegn i
%\begin{align*}
%\gamma_j = -\frac{\widehat{\beta}_j}{\widehat{d}_j},
%\end{align*}
%den første af sådan en ændring kommer i
%\begin{align*}
%\tilde{\gamma} = \min_{\gamma_j > 0} \cbr{\gamma_j},
%\end{align*}
%for prædiktor \(\x_{\tilde{j}}\).
%Hvis der ikke findes en \(\gamma_j > 0\), da er \(\tilde{\gamma}=\infty\) per definition.
%
%Hvis \(\tilde{\gamma} < \widehat{\gamma}\), da kan \(\beta_{\tilde{j}} \del{\gamma}\) ikke være lasso løsningen for \(\gamma > \tilde{\gamma}\), da restriktionen \eqref{eq:lars_3.1} ikke er opfyldt, eftersom \(\beta_{\tilde{j}} \del{\gamma}\) har ændret fortegn, mens \(c_{\tilde{j}}\) ikke har.
%Der gælder, at \(c_{\tilde{j}}\) ikke kan ændre fortegn indenfor ét LARS trin da \(\abs{c_{\tilde{j}} \del{\gamma}} = \widehat{C} - \gamma A_\A> 0\) af \eqref{eq:lars_2.16}. \\[2mm]
%%
%\textbf{Lasso modifikation} \\
Hvis \(\tilde{\gamma} < \widehat{\gamma}\), stop det igangværende LARS trin i \(\gamma = \tilde{\gamma}\) og fjern \(\tilde{j}\) fra udregningen af den næste ensvinklede retning.
Dvs
\begin{align*}
\widehat{\tmu}_{\A_+} = \widehat{\tmu}_\A + \tilde{\gamma} \mathbf{u}_\A \quad \text{og} \quad \A_+ = \A - \cbr{\tilde{j}},
\end{align*}
istedet for \eqref{eq:lars_2.12}.

En mere detaljeret gennemgang af LARS med lasso modifikation kan findes i \citep{efron}.
Da variable kan fjernes og tilføjes til den aktive mængde, er antallet af trin i LARS algoritmen med lasso modifikationen større end \(p\).

\begin{eks}
Figur \ref{fig:diabetes_lars} illustrerer koefficientstierne for LARS algoritmen uden og med lasso modifikationen, som funktion af fraktion af \(\ell_1\)-norm for diabetes data.
Hvis \(\frac{\abs{\tbeta}}{\max \abs{\tbeta}} = 0\), da er ingen variable tilføjet til den aktive mængde og hvis \(\frac{\abs{\tbeta}}{\max \abs{\tbeta}} = 1\) er alle variable inkluderet.
Af figuren kan vi aflæse rækkefølgen, hvori variablerne medtages i modellen.
For LARS algoritmen uden lasso modifikationen udføres 10 steps, mens LARS algoritmen med lasso modifikationen udfører 12 steps.
De 2 ekstra steps som LARS algoritmen med lasso modifikationen udfører, kommer af, at variablen \texttt{hdl} fjernes og tilføjes igen i henholdsvis step 11 og 12.
Heraf ses det også at stien er kontinuert og stykvis lineær.
Den mængde og fortegnene af de aktive variable er konstant mellem stepsene.

\imgfigh{diabetes_lars.pdf}{0.9}{Koefficientstierne for LARS algoritmen uden og med lasso modifikationen som funktion af fraktion af \(\ell_1\)-normen for diabetes data.}{diabetes_lars}
\end{eks}

%\subsection{Frihedsgrader og \(C_p\) estimater}

For LARS algoritmen kræves blot \(p\) steps for at finde den fulde løsning.
De beregningsmæssige omkostninger for LARS algoritmen er af samme orden som løsningen af OLS med \(p\) prædiktorer.

Coordinate descent kan være hurtigere end LARS algoritmen særligt for store problemer.
Dette skyldes, at \eqref{eq:update_coordinate} hurtigt kan opdateres som \(j\) varierer, og ofte er opdateringen at lade \(\beta_j = 0\).
Coordinate descent algoritmen for lasso giver ikke den fulde lasso løsningssti som LARS algoritmen, men kan bruges til at udregne lasso løsningerne for \(\Lambda\).





\section{Lasso estimatoren} \label{sec:lasso_estimatoren}
\textit{Dette afsnit er skrevet udfra kapitel 2 i \citep{hastie}.}

\textit{The Least Absolute Shrinkage Selection Operator}, som forkortes lasso, blev introduceret af \citep{lasso}. 
Lasso finder løsningen til optimeringsproblemet
\begin{align}
\arg \min_{\beta_0, \beta} \cbr{\frac{1}{2n} \sum_{i=1}^n \del{y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j}^2}, \ \text{underlagt at } \sum_{j=1}^p \vert \beta_j \vert \leq t. \label{eq:2.3}
\end{align} 
Betingelsen $\sum_{j=1}^p \vert \beta_j \vert \leq t$ kan skrives mere kompakt som en \(\ell_1\)-norm betingelse $\Vert \beta \Vert_1 \leq t$.

Lad \(\y=(y_1, \ldots, y_n)\) være en \(n\) dimensional vektor med responsvariable og lad \(\X\) være en $n \times p$ matrix med $x_i \in \R^p$ som den i'te række, da kan \eqref{eq:2.3} omskrives til matrix-vektor form
\begin{align*}
\arg \min_{\beta_0, \beta} \cbr{\frac{1}{2n} \Vert \y - \beta_0 \mathbf{1} - \X \beta \Vert_2^2}, \ \text{underlagt at } \Vert \beta \Vert_1 \leq t,
\end{align*}
hvor \(\mathbf{1}\) er en \(n\) dimensional vektor bestående af 1 taller og \(\Vert \cdot \Vert_2\) betegner den Euklidiske norm.
Værdi af \(t\) begrænser summen af de absolutte værdier af parameter estimaterne.
Den kontrollerer kompleksiteten af modellen. 
Større værdi af \(t\) betyder flere parametre og tillader dermed at modellen tilpasser data meget præcis.
Mindre værdi af \(t\) vil begrænse antallet af parametre, hvilket fører til mere sparse modeller, som tilpasser data mindre præcis.
Værdien af \(t\) skal specificeres ved en ekstern procedure kaldet \textit{krydsvalidering}, som vil blive diskuteret i kapitel \ref{kap:statistisk_inferens}.

Ofte standardiseres prediktorerne \(\X\) således at kolonnerne er centeret og har varians 1. Dvs \(\frac{1}{n} \sum_{i=1}^n x_{ij} = 0\) og \(\frac{1}{n} \sum_{i=1}^n x_{ij}^2=1\). Hvis ikke prediktorerne standardiseres da vil lasso estimaterne være afhængige af enhederne, som prediktorerne er målt i.
Hvis prediktorerne er målt i samme enhed, da vil vi typisk ikke standardisere.
For fuldstændighed centreres responsvariablen $y_i$ også, dvs \(\frac{1}{n} \sum_{i=1}^n y_{i} = 0\).
Hermed kan vi se bort fra skæringen $\beta_0$ i lasso optimeringen.
Givet en optimal lasso løsning \(\hat{\beta}\) på det centreret data, kan vi finde løsningen for det ikke-centreret data udfra
\begin{align*}
\hat{\beta}^{\text{ikke-centreret}} = \hat{\beta}^{\text{centreret}}, \\
\hat{\beta}_0^{\text{ikke-centreret}} = \bar{y} - \sum_{j=1}^p \bar{x}_j \hat{\beta}_j,
\end{align*}
hvor \(\bar{y}\) og \(\cbr{\bar{x}_j}_{j=1}^p\) er de originale gennemsnit.
Derfor ser vi bort fra skæringen resten af kapitlet.

Lasso problemet kan omskrives til Lagrange form
\begin{align}
\arg \min_{\beta} \cbr{\frac{1}{2n} \Vert \y - \X \beta \Vert_2^2 + \lambda \Vert \beta \Vert_1}, \label{eq:2.5}
\end{align}
hvor $\lambda \geq 0$ er en såkaldt strafparameter, som også bestemmes udfra krydsvalidering. 
Der er en en-til-en korrespondance mellem det betingede problem \eqref{eq:2.3} og Lagrange problemet \eqref{eq:2.5}. 
For hver værdi af \(t\) hvor \(\Vert \beta \Vert_1 \leq t\) er opfyldt, da findes en tilhørende værdi af $\lambda$ som giver den samme løsning for \eqref{eq:2.5}.
Omvendt gælder der, at løsningen $\hat{\beta}_\lambda$ til \eqref{eq:2.5} løser grænse problemet med $t=\Vert \hat{\beta}_\lambda \Vert_1$.

I andre beskrivelser af lasso estimatoren ses det at faktoren \(\frac{1}{2n}\) i \eqref{eq:2.3} og \eqref{eq:2.5} erstattes med \(\frac{1}{2}\) eller \(1\).
Dette gør ingen forskel i \eqref{eq:2.3} og svarer blot til en simpel reparametrisering af \(\lambda\) i \eqref{eq:2.5}.
Denne standardisering gør \(\lambda\) værdierne sammenlignelige for samples size af forskellige størrelse, som er brugbart i krydsvalidering.

\textit{Ridge regression} estimatoren findes udfra 
\begin{align} 
\arg \min_{\beta_0, \beta} \cbr{\frac{1}{2n} \sum_{i=1}^n \del{y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j}^2}, \ \text{underlagt at } \sum_{j=1}^p \beta_j^2 \leq t, \label{eq:2.7} 
\end{align} 
som kan omskrives på Lagrange form
\begin{align} 
\arg \min_{\beta_0, \beta} \cbr{\frac{1}{2n} \sum_{i=1}^n \del{y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j}^2 + \lambda \sum_{j=1}^p \beta_j^2}, 
\end{align} \label{eq:2.8} 
hvor $\lambda \geq 0$.
Efter standardisering
\begin{align*}
\arg \min_{\beta} \cbr{\frac{1}{2n} \Vert \y - \X \beta \Vert_2^2 + \lambda \Vert \beta \Vert_2^2}.
\end{align*}
Heraf kan ridge regression estimatoren findes ved at differentiere \(\del{\y - \X \beta}^T \del{\y - \X \beta} + \lambda \beta^T \beta\) mht $\beta$, sætte dette lig 0 og isolere for $\beta$. Hvoraf vi finder, at
\begin{align*} 
\hat{\beta}^\text{ridge} = (\X^T \X + \lambda I_p)^{-1} \X^T \y. 
\end{align*}  
I forhold til mindste kvadraters regression tilføjer ridge regression en positiv konstant $\lambda$ på diagonalen af $\X^T \X$, hvilket medfører, at \(\X^T \X\) er invertibel, selvom $\X$ ikke har fuld rang. 
Dermed er en entydig løsning altid garanteret. 

Lad os betragte data givet i tabel \ref{tab:crime}.
Outputtet er kriminalitetsraten per 1 million indbyggere i byer i United States.
Prediktorerne er følgende: den årlige politiske finansiering i dollars per indbygger, den procentvise andel af 25 årige eller derover med 4 års gymnasial uddannelse, den procentvise andel af 16-19 årige som ikke er på eller har en gymnasiel uddannelse, den procentvise andel af 18-24 årige på universitetet og den procentvise andel af 25 årige eller derover med mindst 4 år på universitetet.
%
\input{fig/tab/crime}
%
Løbende i rapporten referer vi til dette data, som er inkluderet for at underbygger teorien.
I dette afsnit anvendes crime data til at illustrere lasso løsningsstierne.
%
\begin{figure}[H]
\centering
\begin{minipage}{0.4\linewidth}
\scalebox{0.3}{\includegraphics{fig/crime_ridge.png}}
\end{minipage}
\hspace{0.2cm}
\begin{minipage}{0.4\linewidth}
\scalebox{0.3}{\includegraphics{fig/crime_lasso.png}}
\end{minipage}
\caption{Koefficientstierne for henholdsvis ridge regression (venstre) og lasso (højre) plottet imod \(\ell_1\)-normen.} \label{fig:crime_koef}
\end{figure}

Variabel udvægelsen for ridge regression og lasso illustreres på figur \ref{fig:LassoRig}.
\begin{figure}[H]
\centering
\begin{minipage}{0.4\linewidth}
\scalebox{0.7}{\input{fig/Ridge.tikz}}
\end{minipage}
\hspace{0.2cm}
\begin{minipage}{0.4\linewidth}
\scalebox{0.7}{\input{fig/Lasso.tikz}}
\end{minipage}
\caption{Konturer for SSR og betingelsesområderne for ridge regression (venstre) og lasso (højre). De blå arealer er betingelsesområderne $\vert \beta_1 \vert+\vert \beta_2 \vert \leq t$ og $\beta_1^2+\beta_2^2 \leq t^2$, mens de røde ellipser er konturkurver for SSR. Konturkurverne har centrum i OLS estimatoren, $\hat{\beta}^\text{OLS}$.} \label{fig:LassoRig}
\end{figure}
For $p=2$ underligges OLS betingelsen $\beta_1^2 + \beta_2^2 \leq t^2$ for ridge regression og betingelsen $\vert \beta_1 \vert + \vert \beta_2 \vert \leq t$ for lasso.
Ellipserne omkring $\hat{\beta}^{\text{OLS}}$ er konturkurverne for SSR, dvs. SSR er konstant i en given ellipse. Værdien af SSR stiger, som ellipsen udvides fra $\hat{\beta}^{\text{OLS}}$.
Løsningen for ridge regression og lasso er givet ved det første punkt, hvor konturkurverne rammer betingelsesområderne.
Da ridge regression har et cirkulært betingelsesområde, vil skæringen med konturkurverne generelt ikke forekomme direkte på en akse.
Omvendt har lasso et regulært betingelsesområde med hjørner i hver akse, hvilket betyder, at hvis løsningen forekommer i et hjørne, da vil en af parametrene $\beta_j$ være lig 0.

Hvis $t$ er tilstrækkelig stor, da vil betingelsesområderne indeholde $\hat{\beta}^{\text{OLS}}$ og derfor vil ridge regression og lasso estimatorerne være lig OLS estimatoren.

På figur \ref{fig:LassoRig} har vi blot betragtet det simple tilfælde hvor $p=2$. Når $p=3$ vil betingelsesområdet for ridge regression være en kugle, mens betingelsesområdet for lasso vil være en polydron. 

\begin{lem}
Givet data \(\del{\y, \X}\), defineres et augmented datasæt
\begin{align*}
\mathbf{X}^* = \begin{bmatrix}
\mathbf{X} \\ \sqrt{\lambda} I_p
\end{bmatrix}, \quad 
\mathbf{y}^* = \begin{bmatrix}
\mathbf{y} \\ 0_p
\end{bmatrix},
\end{align*}
hvor \(\X^* \in \mathbb{R}^{\del{n+p} \times p}\) og \(\y^* \in \mathbb{R}^{n+p}\). Da kan estimatoren for ridge regression udledes udfra mindste kvadraters metode
\begin{align*}
\del{\X^{*^T} \X^*}^{-1} \X^{*^T} \y^* &= \left( \begin{bmatrix}
\mathbf{X} & \sqrt{\lambda} I_p
\end{bmatrix}
\begin{bmatrix}
\mathbf{X} \\ \sqrt{\lambda} I_p
\end{bmatrix} \right)^{-1}
\begin{bmatrix}
\mathbf{X} & \sqrt{\lambda} I_p
\end{bmatrix}
\begin{bmatrix}
\mathbf{y} \\ 0
\end{bmatrix} \\
&= \left( \mathbf{X}^T \mathbf{X} + \lambda I_p \right)^{-1} \mathbf{X}^T \mathbf{y}.
\end{align*}
\end{lem}

\subsection{Udregning af lasso}
Da lasso strafleddet ikke er differentialbel, findes der ikke en eksplicit løsning til lasso problemet.
Vi antager, at responsvariablerne $y_i$ og prediktorerne $x_{ij}$ er standardiseret således at \(\frac{1}{n} \sum_{i=1}^n y_{i} = 0\), \(\frac{1}{n} \sum_{i=1}^n x_{ij} = 0\) og \(\frac{1}{n} \sum_{i=1}^n x_{ij}^2=1\), da kan vi ser bort fra skæringen $\beta_0$.
En simpel procedure kaldet \textit{coordinate descent} kan udfra lagrange formen udregne en numerisk løsning. 
%
\subsubsection{Single prediktor: soft tresholding}
Vi betragter blot en enkelt prediktor \(z_i\). 
Problemet, som vi skal løse, er da
\begin{align*}
\arg \min_{\beta} \cbr{\frac{1}{2n} \sum_{i=1}^n \del{y_i - z_{i} \beta}^2 + \lambda \vert \beta \vert}.
\end{align*}
Som bekendt er standard proceduren at finde den første ordens afledede mht $\beta$, sætte denne lig 0 og isolere for $\beta$. 
Men vi bemærker, at \(\vert \beta \vert \) ikke har en afledt i $\beta=0$.
Vi kan dog fortsætte ...
Vi finder, at
\begin{align*}
\frac{\partial}{\partial \beta} \del{\frac{1}{2n} \sum_{i=1}^n \del{y_i - z_{i} \beta}^2 + \lambda \vert \beta \vert}
&= -\frac{1}{n} \sum_{i=1}^n \del{y_i - z_{i} \beta} z_i + \begin{cases}
-\lambda \quad &\beta < 0 \\
[-\lambda, \lambda] & \beta = 0 \\
\lambda & \beta >0 
\end{cases}  \\
&= -\frac{1}{n} \left\langle \mathbf{z}, \mathbf{y} \right\rangle + \beta + \begin{cases}
-\lambda \quad &\beta < 0 \\
[-\lambda, \lambda] & \beta = 0 \\
\lambda & \beta >0 
\end{cases},
\end{align*}
da $\frac{1}{n} \sum_{i=1}^n z_i^2=1$. Herefter sættes disse lig 0 og $\beta$ isoleres, hvoraf vi finder, at
\begin{align*}
\hat{\beta} = \begin{cases}
\frac{1}{n} \left\langle \mathbf{z}, \mathbf{y} \right\rangle - \lambda, \quad &\frac{1}{n} \left\langle \mathbf{z}, \mathbf{y} \right\rangle > \lambda, \\
0 &\frac{1}{n} \left\langle \mathbf{z}, \mathbf{y} \right\rangle \leq \lambda, \\
\frac{1}{n} \left\langle \mathbf{z}, \mathbf{y} \right\rangle + \lambda, &\frac{1}{n} \left\langle \mathbf{z}, \mathbf{y} \right\rangle < \lambda.
\end{cases}
\end{align*}
Definer \textit{soft-threshold operatoren} $S_\lambda\del{x}=\text{sign}\del{x} \del{\vert x \vert - \lambda}_+$, som mindsker dens argument $x$ mod 0 ved $\lambda$, og sætter den lig med 0 hvis $\vert x \vert \leq \lambda$. Da kan vi omskrive opdateringen til
\begin{align*}
\hat{\beta} = S_\lambda \del{\frac{1}{n} \left\langle \mathbf{z}, \mathbf{y} \right\rangle}.
\end{align*}
%
\subsubsection{Multiple prediktorer: cyclic coordinate descent}
Herefter kan vi betragte multivariate prediktorer. 
Gentagne cycle gennem prediktorerne i en fast, men vilkårlig orden $j=1, 2, \ldots, p$, hvor koefficienten $\beta_j$ opdateres i det $j$'te step, ved at minimere objektfunktionen i dens koordinat, mens de resterende koefficienter $\cbr{\hat{\beta}_k, k \neq j}$ fastholdes deres nuværende værdier. 
Vi kan opskrive objektfunktionen i \eqref{eq:2.5} som
\begin{align*}
\frac{1}{2n} \sum_{i=1}^n \del{y_i - \sum_{k \neq j} x_{ik} \beta_k - x_{ij} \beta_j}^2 + \lambda \sum_{j = 1}^p \vert \beta_j \vert
\end{align*}
Definer den partialle residual $r_i^{(j)}=y_i - \sum_{k \neq j} x_{ik} \hat{\beta}_k$, som fjerner nuværende fit fra den $j$'te prediktor.


Da er den j'te koefficient opdateret ved
\begin{align}
\hat{\beta}_j = S_\lambda \del{\frac{1}{n} \left\langle \mathbf{x}_j, \mathbf{r}^{(j)} \right\rangle}, \label{eq:2.14}
\end{align}
hvor \(r_i = y_i - \sum_{j = 1}^p x_{ij} \hat{\beta}_j \) er de fulde residualer.
Den beskrevne algoritme svarer til metoden \textit{cyclical coordinate descent}, som minimerer en konveks objektfunktion langs hver koordinat af gangen.
Under milde regularitetsbetingelser konvergerer løsningen til et global optimum.
Fra opdateringen \eqref{eq:2.14} ser vi, at algoritmen foretager en univariat regression af den partial residual på hver prediktor, cycling gennem prediktorerne indtil konvergens. \\

Ofte er vi interesseret i, at finde lasso løsningen for en mængde af \(\lambda\) værdier og ikke blot én fast lambda.
\textit{pathwise coordinate descent} kan anvendes hertil, ved at begynde med en værdi af \(\lambda\) som præcis er høj nok således at den optimale løsning er vektor bestående af \(0\).
Denne værdi er lig \(\lambda_{\max}=\max_j \vert \frac{1}{2} \left\langle \mathbf{x}_j, \y \right\rangle \vert\).
Da kan vi aftage \(\lambda\) med en lille mængde og køre coordinate descent undtil konvergens.
Aftage \(\lambda\) igen og anvende den tidligere løsning som en warm start, da kan vi kører coordinate descent indtil konvergens.
Hermed kan vi udregne løsningen over en grid af \(\lambda\) værdier. \\

Coordinate descent er særlig hurtig til at løse lasso problemet, da koordinatvis minimering er tilgængelig \eqref{eq:2.14}, og dermed er en iterativ søgning langs hver koordinat ikke nødvendig.
Derudover udnytter coordinat descent at lasso giver sparse løsninger.
For tilstrækkelige høje \(\lambda\) værdier er de fleste koefficienter lig $0$. \\[2mm] 
%
\textit{Homotopy metoder} er en alternativ teknisk til at løse lasso problemet. Disse producerer en helt sti af løsninger i en frekventiel sekvens, ved at starte med nul.
Denne sti er faktisk piecewise lineær.
Algoritmen kaldet \textit{least angle regression} (LARS) er en homotopy metode som effektivt konstruerer piecewise lineære stier.
En mere teoretisk gennemgang af coordinate descent og LARS algoritmen er givet i kapitel \ref{kap:optimeringsmetoder}. \\[2mm]


Hvis prediktorerne er ortogonale, dvs $\frac{1}{n} \left\langle \mathbf{x}_j, \mathbf{x}_k \right\rangle = 0$ for alle $j \neq k$.
Da reduceres opdateringen \eqref{eq:2.14} til
\begin{align*}
\hat{\beta}_j = S_\lambda \del{\frac{1}{n} \left\langle \mathbf{x}_j, \mathbf{y} \right\rangle},
\end{align*}
dermed er $\hat{\beta}_j$ blot soft-thresholded version af det univariate mindste kvadraters estimat af $\mathbf{y}$ regresseret imod $\mathbf{x}_j$. Således har vi en lukket løsning og ingen iterationer er påkrævet.

\subsection{Frihedsgrader}
Antag at, vi har \(p\) prediktorer og tilpasser en lineær regressions model udfra \(k\) af disse prediktorer.
Hvis disse \(k\) prediktorer vælges uafhængigt af responsvariablen, da "anvender" fitting proceduren \(k\) frihedsgrader.
Dvs at teststørrelsen for at teste hypotesen om at alle \(k\) koefficienter er 0, har en Chi-squared fordeling med \(k\) frihedsgrader.

Hvis valget af de \(k\) prediktorer afhænger af responsvariablen, da forventes det at fitting proceduren anvender mere end \(k\) frihedsgrader. 
Vi kalder sådan en fitting procedure \textit{adaptiv}, og tydeligvis er lasso et eksempel på dette.

Ligeledes er forward-stepwise proceduren adaptiv, hvor vi sekventiel tilføjer prediktorer som mindsker fejlen mest.
Her forventes det at modellen anvender mere end \(k\) frihedsgrader efter \(k\) step.
Derfor er antallet af frihedsgrader ikke nødvendigvis lig med antallet af ikke-nul koefficienter.

Men for lasso er antallet af frihedsgrader faktisk lig antallet af ikke-nul koefficienter, som vi nu vil beskrive.

Først defineres hvad vi mener med frihedsgrader for en adaptiv fitted model. 
Antag at, vi har en additive-error model
\begin{align*}
y_i = f \del{x_i} + \epsilon_i, \quad i = 1, \ldots, n,
\end{align*}
hvor \(f\) er ukendt og \(\epsilon_i \sim iid \del{0, \sigma^2}\).
Lad \(\hat{\y}\) betegne \(n\) sample prediktorer, da defineres 
\begin{align*}
\text{df}\del{\hat{\y}} := \frac{1}{\sigma^2} \sum_{i=1}^n \text{Cov}\del{\hat{y}_i, y_i}.
\end{align*}
Antal frihedsgrader svarer da til indflydelsen hver respons mål har på dens prediktion.
Desto bedre modellen tilpasser data, desto højere antal frihedsgrader.
Det kan vises at, lasso med en fast strafparameter \(\lambda\) er antallet af ikke-nul koefficienter \(k_\lambda\) et unbiased estimat af frihedsgrader.

Som nævnt ovenfor anvender forward-stepwise regression mere end \(k\) frihedsgrader efter \(k\) step.
Lasso udvælger ikke blot prediktorer, som bekendt øger antallet af frihedsgrader, men shrinks også koefficienterne mod 0 relativ til mindste kvadraters estimaterne.
Denne skrinkage er netop tilstrækkelig til at brige antallet af frihedsgrader til \(k\).
Dette resultat er særligt nyttig, da det giver et kvalitativ mål af mængden af fitting som vi har opnået ved ethvert punkt på lasso stien.
Generelt er bevises for dette resultat meget besværligt.
Men i det ortogonale tilfælde ...

Ideen tages et skridt videre i sektion \ref{subsec:kovarians_test} hvor vi beskriver \textit{kovarians testen} til at teste significancen af prediktorerne ift lasso.


\subsubsection{Ikke-negative garrote}
\textit{Ikke-negative garrote}, introduceret af \citep{nonnegative_garrote}, er en two-stage procedure. 
Givet et initial estimat af regression koefficienterne \(\tilde{\beta} \in \mathbb{R}^p\), kan vi løse optimeringsproblemet
\begin{align}
\arg \min_{c \in \mathbb{R}^p}  \cbr{ \sum_{i=1}^n \del{y_i - \sum_{j=1}^p c_j x_{ij} \tilde{\beta}_j}^2}, \ \text{underlagt at } c_j \geq 0 \text{ og } \Vert c \Vert_1 \leq t. \label{eq:2.19}
\end{align}
Lad \(\hat{\beta}_j = \hat{c}_j \cdot \tilde{\beta}_j\), \(j = 1, \ldots, p\). \( \hat{\beta}_j^\text{garotte} = \hat{c}_j \cdot \hat{\beta}_j^\text{OLS}\)
På Lagrange form
\begin{align*}
\arg \min_{c \in \mathbb{R}^p}  \cbr{\Vert \y - \X \beta \Vert_2^2 + \lambda \Vert c \Vert_1}
\end{align*}
Der er en ækvivalent Lagrange form for denne procedure, ..

\newpage
 
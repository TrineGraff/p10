\section{Lasso estimatoren}
The Least Absolute Shrinkage Selection Operator (lasso) blev introduceret af ---. 
Effektiv for at håndtere flere prediktorer end variable.
Derudover producere den parsimonious modeller som er nemme at fortolke.
Vi anvender LASSO estimatoren for udvælgelse af modeller for tidsrække data.

%Når vi ændre strafparameteren i \eqref{eq:2.8} til $\lambda \norm{\boldsymbol\beta}_1 = \lambda \sum_{j=1}^p \abs{\beta_j}$ opnår vi endnu en shrinkage metode, som er the least absolut shrinkage selection operator (lasso)
Lasso finder løsningen til optimerings problemet
\begin{align}
\min_{\beta_0, \beta} \cbr{\frac{1}{2n} \sum_{i=1}^n \del{y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j}^2}, \ \text{underlagt at } \sum_{j=1}^p \vert \beta_j \vert \leq t \label{eq:2.3}
\end{align}
Betingelsen $\sum_{j=1}^p \vert \beta_j \vert \leq t$ kan skrives mere kompakt ved $\Vert \beta \Vert_1 \leq t$.
Dette kan udtrykkes på matrix-vektor notation.
Lad \(\y=(y_1, \ldots, y_n)\) være en \(n\) dimensional vektor med responsvariable og \(\X\) være en $n \times p$ matrix med $x_i \in \R^p$ som den i'te række, da kan \eqref{eq:2.3} omskrives til
\begin{align*}
\min_{\beta_0, \beta} \cbr{\frac{1}{2n} \Vert \y - \beta_0 \mathbf{1} - \X \beta \Vert_2^2}, \ \text{underlagt at } \Vert \beta \Vert_1 \leq t,
\end{align*}
hvor \(\mathbf{1}\) er en \(n\) dimensional vektor bestående af 1 og \(\Vert \cdot \Vert_2\) betegner den Euklidiske norm af vektorer.

Grænsen \(t\) begrænser summen af de absolutte værdier af parameter estimaterne.
Denne skal specificeres ved en ekstern procedure kaldet \textit{kryds validering}, som vil blive diskuteret i kap --.

Ofte standardiseres prediktorerne \(\X\) således at kolonnerne er centeret og har varians 1. Dvs \(\frac{1}{n} \sum_{i=1}^n x_{ij} = 0\) og \(\frac{1}{n} \sum_{i=1}^n x_{ij}^2=1\). Hvis ikke prediktorerne standardiseres da vil lasso estimaterne afhænge af enhederne.
Hvis prediktorerne er målt i samme enhed, da vil vi typisk ikke standardisere.
For fuldstændigheden, antager vi også at responsvariablen $y_i$ er centeret, dvs \(\frac{1}{n} \sum_{i=1}^n y_{i} = 0\).
Når data er centreret da kan vi se bort fra skæringen $\beta_0$ i lasso optimeringen.
Given en optimal lasso løsning \(\hat{\beta}\) på det centreret data, kan vi finde løsningen for det ikke-centreret data. Der gælder at
\begin{align*}
\hat{\beta}^{\text{ikke-centreret}} = \hat{\beta}^{\text{centreret}} \\
\hat{\beta}_0^{\text{ikke-centreret}} = \bar{y} - \sum_{j=1}^p \bar{x}_j \hat{\beta}_j
\end{align*}
Derfor ser vi bort fra skæringen resten af kapitlet.

Vi kan omskrive lasso problemet til Lagrange form
\begin{align}
\min_{\beta} \cbr{\frac{1}{2n} \Vert \y - \X \beta \Vert_2^2 + \lambda \Vert \beta \Vert_1} \label{eq:2.5}
\end{align}
for $\lambda \geq 0$.
Af Lagrange dualiteten er der en bijektion mellem \eqref{eq:2.3} og \eqref{eq:2.5}: for hver værdi af \(t\) hvor \(\Vert \beta \Vert_1 \leq t\) er opfyldt, da findes en tilhørende værdi af $\lambda$ som giver den samme løsning for \eqref{eq:2.5}.
Mens løsningen $\hat{\beta}_\lambda$ til \eqref{eq:2.5} løser grænse problemet med $t=\Vert \hat{\beta}_\lambda \Vert_1$

Variabel udvægelsen for ridge regression og lasso illustreres på figur \ref{fig:LassoRig}.
\begin{figure}[H]
\begin{minipage}{0.5\linewidth}
\scalebox{0.8}{\input{fig/Ridge.tikz}}
\end{minipage}
\hspace{0.3cm}
\begin{minipage}{0.5\linewidth}
\scalebox{0.8}{\input{fig/Lasso.tikz}}
\end{minipage}
\caption{Konturer for SSR og betingelsesområderne for ridge regression (venstre) og lasso (højre). De blå arealer er betingelsesområderne $\vert \beta_1 \vert+\vert \beta_2 \vert \leq t$ og $\beta_1^2+\beta_2^2 \leq t^2$, mens de røde ellipser er konturkurver for SSR. Konturkurverne har centrum i OLS estimatoren, $\hat{\beta}^\text{OLS}$.} \label{fig:LassoRig}
\end{figure}
For $p=2$ underligges OLS betingelsen $\beta_1^2 + \beta_2^2 \leq t^2$ for ridge regression og betingelsen $\vert \beta_1 \vert + \vert \beta_2 \vert \leq t$ for lasso.
Ellipserne omkring $\hat{\beta}^{\text{OLS}}$ er konturkurverne for SSR, dvs. SSR er konstant i en given ellipse. Værdien af SSR stiger, som ellipsen udvides fra $\hat{\beta}^{\text{OLS}}$.
Ligningerne - og - indikerer at løsningen for ridge regression og lasso er givet ved det første punkt, hvor konturkurverne rammer betingelsesområdet.
Siden ridge regression har et cirkulært betingelsesområde, vil skæringen med konturkurverne generelt ikke forekomme direkte på en akse.
Modsat ridge regression har lassos betingelses område hjørner i hver akse, hvilket betyder, at hvis løsningen forekommer i et hjørne, da vil en af parametrene $\beta_j$ være lig 0.

Hvis $t$ er tilstrækkelig stor, da vil betingelsesområderne indeholde $\hat{\beta}^{\text{OLS}}$ og derfor vil ridge regression og lasso estimatorerne være lig OLS estimatoren.

På figur \ref{fig:LassoRig} har vi blot betragtet det simple tilfælde hvor $p=2$. Når $p=3$ vil betingelsesområdet for ridge regression være en kugle, mens betingelsesområdet for lasso vil være en polydron. 

\subsubsection{Udregning af lasso}
Da lasso penalty ikke er differentialbel, findes der ikke en eksplicit løsning til lasso problemet.
Vi antager at responsvariablerne $y_i$ og prediktorerne $x_{ij}$ er standardiseret således at \(\frac{1}{n} \sum_{i=1}^n y_{i} = 0\), \(\frac{1}{n} \sum_{i=1}^n x_{ij} = 0\) og \(\frac{1}{n} \sum_{i=1}^n x_{ij}^2=1\).
Da kan vi ser bort fra skæringen $\beta_0$.
Lagrange formen er nyttig for numerisk udregning af løsningen som findes vha en simpel procedure kaldet \textit{coordinate descent}. \\
%
\textbf{Single prediktor: soft tresholding} \\
Vi betragter blot en enkelt prediktor baseret på samples $\cbr{\del{z_i, y_i}}_{1=1}^n$, hvor vi omdøber $z_i$ til blot at være én $x_{ij}$.
Problemet som vi skal løse er da
\begin{align*}
\min_{\beta} \cbr{\frac{1}{2n} \sum_{i=1}^n \del{y_i - z_{i} \beta}^2 + \lambda \vert \beta \vert}.
\end{align*}
Standard proceduren er at finde den første ordens afledede mht $\beta$, sætte denne lig 0 og isolere for $\beta$. Men vi bemærker, at \(\vert \beta \vert \) ikke har en afledt i $\beta=0$.
Vi finder, at
\begin{align*}
\frac{\partial}{\partial \beta} \del{\frac{1}{2n} \sum_{i=1}^n \del{y_i - z_{i} \beta}^2 + \lambda \vert \beta \vert}
&= -\frac{1}{n} \sum_{i=1}^n \del{y_i - z_{i} \beta} z_i + \begin{cases}
-\lambda \quad &\beta < 0 \\
[-\lambda, \lambda] & \beta = 0 \\
\lambda & \beta >0 
\end{cases}  \\
&= -\frac{1}{n} \left\langle \mathbf{z}, \mathbf{y} \right\rangle + \beta + \begin{cases}
-\lambda \quad &\beta < 0 \\
[-\lambda, \lambda] & \beta = 0 \\
\lambda & \beta >0 
\end{cases},
\end{align*}
da $\frac{1}{n} \sum_{i=1}^n z_i^2=1$. Herefter sættes disse lig 0 og $\beta$ isoleres, hvoraf vi finder, at
\begin{align*}
\hat{\beta} = \begin{cases}
\frac{1}{n} \left\langle \mathbf{z}, \mathbf{y} \right\rangle - \lambda, \quad &\frac{1}{n} \left\langle \mathbf{z}, \mathbf{y} \right\rangle > \lambda, \\
0 &\frac{1}{n} \left\langle \mathbf{z}, \mathbf{y} \right\rangle \leq \lambda, \\
\frac{1}{n} \left\langle \mathbf{z}, \mathbf{y} \right\rangle + \lambda, &\frac{1}{n} \left\langle \mathbf{z}, \mathbf{y} \right\rangle < \lambda.
\end{cases}
\end{align*}
Definer \textit{soft-threshold operatoren} $S_\lambda\del{x}=\text{sign}\del{x} \del{\vert x \vert - \lambda}_+$, som mindsker dens argument $x$ mod 0 ved $\lambda$, og sætter den lig med 0 hvis $\vert x \vert \leq \lambda$. Da kan vi omskrive opdateringen til
\begin{align*}
\hat{\beta} = S_\lambda \del{\frac{1}{n} \left\langle \mathbf{z}, \mathbf{y} \right\rangle}.
\end{align*}
%
\textbf{Multiple prediktorer: cyclic coordinate descent} \\
Herefter kan vi betragte multivariat prediktorer. 
Gentagne cycle gennem prediktorerne i en fast vilkårlig orden $j=1, 2, \ldots, p$, hvor koefficienten $\beta_j$ opdateres i det $j$'te step, ved at minimere objektfunktionen i denne koordinat, mens de resterende koefficienter $\cbr{\hat{\beta}_k, k \neq j}$ fastholdes deres nuværende værdier. 
Vi kan opskrive objektfunktionen i \eqref{eq:2.5} som
\begin{align*}
\frac{1}{2n} \sum_{i=1}^n \del{y_i - \sum_{k \neq j} x_{ik} \beta_k - x_{ij} \beta_j}^2 + \lambda \sum_{j = 1}^p \vert \beta_j \vert
\end{align*}
Definer den partialle residual $r_i^{(j)}=y_i - \sum_{k \neq j} x_{ik} \hat{\beta}_k$, som fjerner nuværende fit fra den $j$'te prediktor.


Da er den j'te koefficient opdateret ved
\begin{align}
\hat{\beta}_j = S_\lambda \del{\frac{1}{n} \left\langle \mathbf{x}_j, \mathbf{r}^{(j)} \right\rangle}, \label{eq:2.14}
\end{align}
hvor \(r_i = y_i - \sum_{j = 1}^p x_{ij} \hat{\beta}_j \) er de fulde residualer.
Den beskrevne algoritme svarer til metoden \textit{cyclical coordinate descent}, som minimerer en konveks objektfunktion langs hver koordinat af gangen.
Under milde regularitets betingelser, konvergerer løsningen til et global optimum.
Fra opdateringen \eqref{eq:2.14} ser vi at algoritmen foretager en univariat regression af den partial residual på hver prediktor, cycling gennem prediktorerne indtil konvergens.
\textit{pathwise coordinate descent}

Coordinate descent er særlig hurtig til at løse lasso problemet da ...

\textit{Homotopy metoder} er en alternativ teknisk til at løse lasso problemet. Disse producerer en helt sti af løsninger i en frekventiel sekvens, ved at starte med nul.
Denne sti er faktisk piecewise lineær.
Algoritmen kaldet \textit{least angle regression} (LARS) er en homotopy metode som effektivt konstruerer piecewise lineære stier.
En mere teoretisk gennemgang af coordinate descent og LARS algoritmen er givet i kapitel --. 


Hvis prediktorerne er ortogonale, dvs $\frac{1}{n} \left\langle \mathbf{x}_j, \mathbf{x}_k \right\rangle = 0$ for alle $j \neq k$.
Da reduceres opdateringen \eqref{eq:2.14} til
\begin{align*}
\hat{\beta}_j = S_\lambda \del{\frac{1}{n} \left\langle \mathbf{x}_j, \mathbf{y} \right\rangle},
\end{align*}
dermed er $\hat{\beta}_j$ blot soft-thresholded version af det univariate mindste kvadraters estimat af $\mathbf{y}$ regresseret imod $\mathbf{x}_j$. Således har vi en lukket løsning og ingen iterationer er påkrævet.

\subsection{Frihedsgrader}
 
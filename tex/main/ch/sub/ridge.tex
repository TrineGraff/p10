\section{Ridge regression}
I dette afsnit introduceres ridge regression som blot er OLS underlagt en restriktion på værdien af parametrene.

Ridge regression estimatoren findes udfra
\begin{align}
\min_{\beta_0, \beta} \cbr{\frac{1}{2n} \sum_{i=1}^n \del{y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j}^2}, \ \text{underlagt at } \sum_{j=1}^p \beta_j^2 \leq t, \label{eq:2.7}
\end{align}
som kan omskrives til et Lagrange problem
\begin{align}
\min_{\beta_0, \beta} \cbr{\frac{1}{2n} \sum_{i=1}^n \del{y_i - \beta_0 - \sum_{j=1}^p x_{ij} \beta_j}^2 + \lambda \sum_{j=1}^p \beta_j^2},
\end{align} \label{eq:2.8}
hvor $\lambda \geq 0$ er en såkaldt strafparameter, som bestemmes separat.
Der er en en-til-en korrespondance mellem det betingede problem \eqref{eq:2.7} og Lagrange problemet \eqref{eq:2.8}.
Første led i \eqref{eq:2.8} svarer til OLS, som finder de estimerede koefficienter ved at minimere SSR, mens sidste led mindsker de estimerede koefficienter .
På matrix-vektor form er løsningen af ridge regression givet ved
\begin{align*}
\hat{\beta}^R = (\X^T \X + \lambda I_p)^{-1} \X^T \y.
\end{align*} 
Ift til mindste kvadraters regression tilføjer ridge regression en positiv konstant $\lambda$ på diagonalen af $\X^T \X$, hvilket medfører, at \(\X^T \X\) er invertibel, selvom $\X$ ikke har fuld rang.
Dermed er en entydig løsning altid garanteret.
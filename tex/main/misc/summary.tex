\chapter{Summary}
This project deals with linear regression for macroeconomic variables in a time series context. The focus is a setting where the number of explanatory variable is large compared to the number of observations, which complicate the use of ordinary least square estimation. However the theory is still presented under the assumption that the designmatrix has full rank, so the case, where the number of explanatory variables is bigger than the number of observation, is not covered in the theory.\\
In chapter 1 the many nice properties that the OLS estimator enjoys in the right setting are presented and an explanation why complications arises in the setting that we are considering is given. Therefore alternative fitting procedures for linear regression are considered. We introduce ridge regression, lasso and the adaptive lasso  in a cross-sectional setting, and we prove that the adaptive lasso enjoys the oracle properties under certain assumptions. \\
Basic timeseries theory are introduced in chapter 2, as we move away from the cross-sectional setting and consider linear regression in a timeseries context in chapter 3. We consider the autoregressive model of order $p$, and show that the estimator estimated by ordinary least squares will be asymptotically normally distributed. Furthermore we argue for, that the assumptions made for the setting, in which the adaptive lasso enjoys the oracle properties, will be satisfied by an autoregressive model.\\
In chapter 4 we use the different fitting procedures presented in the theory to make one-step-ahead forecasts of the quarterly percentage growth in the gross domestic product. The data analysed in this chapter is quarterly data made publicly available by the Federal Reserve Bank of St. Louis. The simple autoregressive model with parameters estimated by OLS is fitted to the data at first, and used as a benchmark model throughout the chapter, in the sense that it is used as a reference for the performance of the other models. Since the presented theory covers the case with a designmatrix of full rank, a reduction of the available data is made, so that this assumption is reasonable. Linear models is then fitted to the reduced data by ridge regression, lasso and the adaptive lasso procedure, and an analysis of the residuals from the last fitted model is carried out in each case. The adaptive lasso is tried fitted to the data with weights chosen by both ordinary least squares and the lasso. \\
We also try reducing the data by the estimates given by the lasso used on the full dataset, in the sense that an appropriate penalty parameter is chosen, so that the number of parameters, the lasso estimates to be different from zero, will be less than the number of observations. A linear model containing the variables from this reduced dataset is then fitted to the data by the adaptive lasso. This procedure is not covered in the theory, it yields, however, superior results compared to the case, in which the variable selection was done manually.              
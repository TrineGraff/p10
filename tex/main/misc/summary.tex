\chapter{Summary}
%mellem min 1 side og maks 2 sider.
%indg√•r i evalueringen
%
The object of the master is to predict a macroeconomic variable, which we have chosen to be the unemployment rate, from a dataset of 122 variables.
To improve the prediction accuracy we would like to identify a smaller subset of these variables, which have the greatest impact on the unemployment rate, and also shrinks the coefficients of the variables.
The report is divided into two parts. 
At first we present the necessary theory and then we include an empirical part, in which we apply the theory.

In the theoretical part we first describe the classical factor model.
The factor model limits the number of model parameters by collecting information about variance in the explanatory variables in some few underlying factors.

Then we present the lasso estimator, which is an extension of classic linear regression, in which an \(\ell_1\)-constraint is imposed on the parameter estimates.
This constraint shrinks the coefficients, and will set some of them to zero.
As such the lasso estimator performs variable selection in linear regression.
This results in a convex optimization problem, which can be solved efficiently for large problems.

To solve the lasso problem we consider an algorithm called coordinate descent.
It is especially attractive for problems such as lasso in which a closed solution does not exists, but a coordinatewise does.
The coordinate descent algorithm choose a single coordinate to update and then performs a univariate minimization over this coordinate holding the remaining coordinates fixed, and cycle through the coordinates in some fixed order.
We also consider the least angle regression (LARS), which delivers the entire solution path as a function of a regularization parameter.
This solution path is continuous and piecewise linear.
A modification can be applied to the LARS algorithm, such that it can be used to solve the lasso problem.

Furthermore we will consider some generalizations of the lasso estimator, which all inherit the two essential features of the standard lasso, which are the shrinkage and selection of variables, or groups of variables.
A breakdown of the lasso estimator is that it tends to select only one variabel, if there is a group of variables in which the pairwise correlations are very high.
The elastic net makes a compromise between the ridge regression and the lasso penalty, and handles highly correlated variables better than lasso. 
%The elastic net has an additional tuning parameter \(\alpha\), that has to be determined. For \(\alpha = 1\), the elastic net reduces to the lasso, and for \(\alpha = 0\) it reduces to ridge regression.
Another generalization of the lasso is group lasso, which is preferred if the variables have a natural group structure.
At last we consider the adaptive lasso as a means for fitting models sparser than lasso.
The adaptive lasso satisfy the so-called oracle properties under some general conditions, which is very desirable. 

We also describe some theory for the inference of the lasso.
For the LARS algorithm with lasso modification we consider the covariance test.
The covariance test assigns \(p\)-values to the predictors as they are successively entered by the lasso.
It turns out that the selection event of LARS and lasso can be characterized as a polyhedral.
The polyhedral lemma presents an alternative representation of the polyhedral, which is used to make inference for the selected predictors.

In the empirical part the presented models are used to predict the unemployment rate one-step-ahead.
The dataset that we will consider is gathered from the Federal Reserve Bank of St. Louis and is publicly available.
It consists of 122 monthly macroeconomic variables and covers the period from January 1, 1959 to November 1, 2017, corresponding to 707 observations.

The autoregressive model is included as an argument for the number of lagged values for the unemployment rate to included in the model matrix.
As a benchmark model we will consider the factor model.

The optimal model for the lasso problem and its generalizations is found by a 10-fold crossvalidation and BIC.
The presented models will be evaluated out-of-sample by the mean absolutte error and the mean squared error.
From the Diebold Mariano test we find that the lasso based models are significantly better that the benchmark model. 
At last the model confidence set (MCS) procedure is considered, which identifies a set of models that are significantly better that the other models.
Both the 80\% and 90\% MCS includes all the lasso based models.
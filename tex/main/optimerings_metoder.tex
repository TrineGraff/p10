\chapter{Optimerings metoder}

Der er flere forskellige måder, at løse et lasso problem, hvis den er konveks. 
Vi introducerer to efficiente algoritmer for at løse lasso problemet, de er henholdvis LARS  (Least Angle Regression) metode og coordinate descent algoritmen 

\section{LARS metode}
Ideen bag algoritmen er, at sætte alle koefficienter lig nul. 
Vi starter med at finde en prædiktor, som er mest kollerede med responsens, lad os kalde den $x_{j1}$. Vi tager det størst mulige skridt i den retning, som prædiktoren indtil en anden prædiktor, $x_{j2}$ har samme korrelation med den nuværende residual. 
I stedet for at fortsætte ud af $x_{j1}$ fortsætter LARS metoden i en ensvinklet retning mellem de to prædiktor indtil en tredje variable $x_{j3}$ er den mest kollerede. LARS metoden fortsætter så ensvinklet mellem $x_{j1}, x_{j2}$ og $ x_{j3}$, dvs den "least angle direction" indtil en fjerde variable forekommer. 

Vi har at succesfuld trin
\begin{align*}
\widehat{\mu} = X \widehat{\beta}
\end{align*}
hvor hvert trin tilføjer et kovariat til modellen, så efter $k$ trin er $k$ af $\widehat{\beta}$ ikke nul. 
Eksempel ses på grafen .... , hvor vi lader $m = 2$, og kovariaterne er givet ved $X = (x_1, x_2)$. I det tilfælde har vi følgende nuværende korrelation, som kun afhænger af $y_2$
\begin{align*}
c(\widehat{\mu}) = X^T \del{y- \widehat{\mu}} = X^T(\bar{y}_2 - \widehat{\mu} )
. 
\end{align*}
Algoritmen begynder i $\widehat{\mu}_0 = 0$. Figuren har en mindre vinkel med $x_1$ end $x_2$ når $\bar{y} - \widehat{\mu}_0$, dvs at $c_1(\widehat{\mu}_0) > c_2(\widehat{\mu}_0$.  

......
......


\section{Coordinate Descent}
For at løse optimeringsproblemer kan Coordinate Descent anvendes. 
Coordinate Descent er en første ordens iterative algoritme, hvilket betyder der kun bruges gradienten af funktionen, som information. 
Hvis koordinat $k$ er valgt i iteration $t$, så får vi følgende opdaterings linje
\begin{align*}
\beta_k^{t+1} =\underset{ \beta_k}{\arg \min}  f\del{ \beta_1^t, \beta_2^t, \dots, \beta_{k-1}^t, \beta_k, \beta_{k+1}^t, \dots, \beta_p^t  },
\end{align*}
hvor $\beta_j^{t+1} = \beta_j^t$ for $j \neq k$. 
Der er to basale modeller for coordinate algoritmer i forhold til den måde de opdaterer parametrene. Den første er cyclic coordinate og den anden kaldes greedy coordinate. 
Cyclic opdaterer en parameter i hvert loop og den greedy opdaterer parameteren, som giver den største ændring i funktionen. 

Coordinate algortimer er generelt rigtig anvendt, når man skal estimerer parameter af en funktion, som ikke er differentiable. Sådan en funktion kunne se føælgende ud
\begin{align*}
f(\beta_1, \dots, \beta_p) = g(\beta_1, \dots, \beta_p) + \sum_{j = 1}^p h_j \del{\beta_j},
\end{align*}
hvor $g: \R^p \rightarrow \R $ er differentialble og konveks, og hvor den univariate funktion $h_j : \R \rightarrow \R$ er konveks (ikke nødvendigvis differentialble). Udfra Tseng .... kan det vises at en hver konveks funktion, som $f$, konvergerer coordinate descent til det globale minimum. 
?? bevis
 

\chapter{Iterative metoder}
I dette kapitel fokuserer vi på iterative metoder, som er anvendt for estimerer ukendte parameter for en givet funktion $f \del{\beta}$
--- psudokoder? 

\section{Bootstrap}
Siden at ikke alle penaliseds regressioner har en lukket løsning, anvender vi bootstrap. 
Bootstrap er en teknik, som bruger observeret data for at lave inference for parameterne af en model. Den er meget nem at implementerer og meget anvendt for big data. 
Hvis vi har data matrix, som indeholder $N$ rækker. Så et bootstrap sample er konstruerede ved at lave en anden matrix af samme størrelse. Det gøres ved at sample rækker med udskiftning fra vores start matrix. 
Så vi laver $B$ antal bootstrap samples fra vores datasæt og udregner ønsket statistik $s$ for hvert bootstrap sample. Til sidst finder vi middelværdien af vores bootstraps og estimerer vores statistik $s$.

\section{Kryds validering}
Kryds validering er en måde for at estimerer fejlene i en fittede model. Fejlene i en fitted model er givet ved $\E{y - \hat{y}},$ hvor $y$ er den observerede respons og  $\hat{y} $ er den fittede respons. Denne fejl kaldes prædiktions fejl. Så derfor fortrækker vi så lav mulige fejl. 

Ideen bag at udregne prædiktion fejl af en model er at have et ekstra data sæt, som anvendes til at udregne det fittede respons. Udfra det kan vi se hvordan vores fittede model anvendes på et nyt datasæt. %fa
Hvilket ligger til grund for kryds validering. Kryds validering deler vores data i $K$ lige store  grupper, og anvender det $i$'te gruppe, som test sættet og de andre $K-1$ grupper bruges, som trænings sæt. Vi udregner præditktions fejlene, hvor vi gentager denne proces $K$ gange. Vi finder så en middelværdi af de prædiktions fejlene. 
For et $K$ kryds validering, får vi følgende prædiktions fejl
\begin{align*}
CV = \frac{1}{K} \sum_{i =1}^k \del{y_i - y_u^{-k(i}},
\end{align*}
hvor $y_i^{-k(i)}$ er det fittede respons af en model når $k(i)$ delen er fjernet. 